{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bb3dad",
   "metadata": {},
   "source": [
    "# Epidemiology of CRRT\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "\n",
    "This script identifies the cohort using CLIF 2.1 tables\n",
    "\n",
    " \n",
    "                        ðŸš¨Code will break if the following requirements are not satisfiedðŸš¨  \n",
    "#### Requirements\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support` ,`crrt_therapy`, `clif_hospital_diagnosis`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category`, `location_type` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | sodium, potassium, chloride, bicarbonate, bun, creatinine, glucose_serum, calcium_total, lactate, magnesium, ph_arterial, ph_venous, po2_arterial |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin, dobutamine, milrinone, isoproterenol |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set`, `peak_inspiratory_pressure_set`, `tidal_volume_obs` | - |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm`, `crrt_mode_name`, `crrt_mode_category`, `device_id`, `blood_flow_rate`, `dialysate_flow_rate`, `ultrafilteration_out` | - |\n",
    "| **clif_hospital_diagnosis** | `hospitalization_id`, `diagnosis_code`, `present_on_admission` | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bc9e1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "import pyarrow\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyCLIF\n",
    "import waterfall\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "    outlier_cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize STROBE counts\n",
    "strobe_counts = {}\n",
    "# Set up output folders\n",
    "output_folder = pyCLIF.setup_output_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513c268",
   "metadata": {},
   "source": [
    "## Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ac00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Loading Core Tables ===\")\n",
    "\n",
    "# Load patient table\n",
    "patient = pyCLIF.load_data('clif_patient')\n",
    "print(f\"Loaded patient table: {len(patient)} rows\")\n",
    "\n",
    "# Load hospitalization table\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "print(f\"Loaded hospitalization table: {len(hospitalization)} rows\")\n",
    "\n",
    "# Load ADT table\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "print(f\"Loaded ADT table: {len(adt)} rows\")\n",
    "\n",
    "# Ensure ID variables are strings\n",
    "patient['patient_id'] = patient['patient_id'].astype(str)\n",
    "hospitalization['hospitalization_id'] = hospitalization['hospitalization_id'].astype(str)\n",
    "hospitalization['patient_id'] = hospitalization['patient_id'].astype(str)\n",
    "adt['hospitalization_id'] = adt['hospitalization_id'].astype(str)\n",
    "\n",
    "# Convert datetime columns\n",
    "patient = pyCLIF.convert_datetime_columns_to_site_tz(patient, pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.convert_datetime_columns_to_site_tz(hospitalization, pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.convert_datetime_columns_to_site_tz(adt, pyCLIF.helper['timezone'])\n",
    "\n",
    "# Remove duplicates\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9430ef",
   "metadata": {},
   "source": [
    "# Cohort Identification\n",
    "\n",
    "1. Adults\n",
    "2. Admitted between January 1, 2018 to December, 31, 2024\n",
    "2. Receiving CRRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd769eae",
   "metadata": {},
   "source": [
    "#### (A) Date and Age filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad204124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date filter: 2018-01-01 to 2024-12-31\n",
    "date_mask = (\n",
    "    (hospitalization['admission_dttm'] >= '2018-01-01') & \n",
    "    (hospitalization['admission_dttm'] <= '2024-12-31')\n",
    ")\n",
    "\n",
    "# Age filter: adults (>= 18)\n",
    "age_mask = (hospitalization['age_at_admission'] >= 18)\n",
    "\n",
    "# Apply filters based on site\n",
    "if pyCLIF.helper['site_name'].lower() == 'mimic':\n",
    "    print(\"skipping date filter for MIMIC\")\n",
    "    # MIMIC doesn't have date restrictions\n",
    "    hospitalization_cohort = hospitalization[age_mask].copy()\n",
    "else:\n",
    "    hospitalization_cohort = hospitalization[date_mask & age_mask].copy()\n",
    "\n",
    "strobe_counts['A_after_date_age_filter'] = hospitalization_cohort['hospitalization_id'].nunique()\n",
    "print(f\"Hospitalizations after date & age filter: {strobe_counts['A_after_date_age_filter']}\")\n",
    "\n",
    "# Also track total adult hospitalizations\n",
    "total_adult = hospitalization[age_mask]['hospitalization_id'].nunique()\n",
    "strobe_counts['A_total_adult_hospitalizations'] = total_adult\n",
    "print(f\"Total adult hospitalizations (no date filter): {total_adult}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb855f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hospitalization IDs from cohort\n",
    "cohort_ids = hospitalization_cohort['hospitalization_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f97e8",
   "metadata": {},
   "source": [
    "#### (B) Stitch hospitalizations\n",
    "\n",
    "Combine multiple `hospitalization_ids` into a single `encounter_block` for patients who transfer between hospital campuses or return soon after discharge. Hospitalizations that have a gap of **6 hours or less** between the discharge dttm and admission dttm are put in one encounter block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8620de",
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort_ids)].copy()\n",
    "\n",
    "# Check for missing values in admission and discharge dates\n",
    "print(\"\\nMissing values in admission_dttm:\", hospitalization_cohort['admission_dttm'].isna().sum())\n",
    "print(\"Missing values in discharge_dttm:\", hospitalization_cohort['discharge_dttm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: Stitch Encounters => 'encounter_block'\n",
    "# Use stitch_encounters from pyCLIF with time_interval=6\n",
    "print(\"\\n=== STEP B: Stitch encounters ===\\n\")\n",
    "stitched_cohort = pyCLIF.stitch_encounters(hospitalization_cohort, adt_cohort, time_interval=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitched_cohort now has: 'patient_id','hospitalization_id','encounter_block', discharge category and other ADT variables. This will have duplicate rows because of location category\n",
    "# We only want 1 row per unique encounter_block for the next steps.\n",
    "stitched_unique = stitched_cohort[['patient_id', 'encounter_block']].drop_duplicates()\n",
    "\n",
    "strobe_counts['B_before_stitching'] = stitched_cohort['hospitalization_id'].nunique()\n",
    "strobe_counts['B_after_stitching'] = stitched_unique['encounter_block'].nunique()\n",
    "strobe_counts['B_stitched_hosp_ids'] = strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']\n",
    "print(f\"Number of unique hospitalizations before stitching: {stitched_cohort['hospitalization_id'].nunique()}\")\n",
    "print(f\"Number of unique encounter blocks after stitching: {strobe_counts['B_after_stitching']}\")\n",
    "print(f\"Number of linked hospitalization ids: {strobe_counts['B_before_stitching']-strobe_counts['B_after_stitching']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of patient id, hospitalization id and encounter blocks\n",
    "all_ids = stitched_cohort[['patient_id', 'hospitalization_id', 'encounter_block', 'admission_dttm',\n",
    "                            'discharge_dttm', 'age_at_admission','admission_type_category','discharge_category']].drop_duplicates()\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in all_ids.columns[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(all_ids[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1471f",
   "metadata": {},
   "source": [
    "#### (C) Identify CRRT patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ce9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CRRT therapy table for these hospitalizations\n",
    "crrt_columns = [\n",
    "    'hospitalization_id', \n",
    "    'recorded_dttm',\n",
    "    'crrt_mode_name',\n",
    "    'crrt_mode_category',\n",
    "    'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "\n",
    "try:\n",
    "    crrt_df = pyCLIF.load_data(\n",
    "        'clif_crrt_therapy',\n",
    "        columns=crrt_columns,\n",
    "        filters={'hospitalization_id': cohort_ids}\n",
    "    )\n",
    "    print(f\"Loaded CRRT therapy data: {len(crrt_df)} rows\")\n",
    "    # Ensure hospitalization_id is string\n",
    "    crrt_df['hospitalization_id'] = crrt_df['hospitalization_id'].astype(str)\n",
    "    # Sort by hospitalization_id and recorded_dttm\n",
    "    crrt_df = crrt_df.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "    # Get unique hospitalizations with CRRT\n",
    "    hosp_ids_with_crrt = crrt_df['hospitalization_id'].unique()\n",
    "    strobe_counts['C_hospitalizations_with_crrt'] = len(hosp_ids_with_crrt)\n",
    "    print(f\"Hospitalizations with CRRT: {len(hosp_ids_with_crrt)}\")\n",
    "    \n",
    "    # Summary of CRRT modes\n",
    "    if 'crrt_mode_category' in crrt_df.columns:\n",
    "        mode_counts = crrt_df['crrt_mode_category'].value_counts()\n",
    "        print(\"\\nCRRT Mode Distribution (N rows):\")\n",
    "        for mode, count in mode_counts.items():\n",
    "            print(f\"  {mode}: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CRRT therapy data: {e}\")\n",
    "    raise SystemExit(\"Stopping execution due to error loading CRRT therapy data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each  crrt_mode_category\n",
    "numeric_columns = [\n",
    "    'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "# Ensure numeric columns are properly converted\n",
    "for col in numeric_columns:\n",
    "    crrt_df[col] = pd.to_numeric(crrt_df[col], errors='coerce')\n",
    "\n",
    "# Convert recorded_dttm to datetime\n",
    "crrt_df['recorded_dttm'] = pd.to_datetime(crrt_df['recorded_dttm'], errors='coerce')\n",
    "\n",
    "# Create summary for each numeric column by  crrt_mode_category\n",
    "summary_list = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    summary =crrt_df.groupby('crrt_mode_category')[col].agg([\n",
    "        ('total_N', 'size'),\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'),\n",
    "        ('first_quantile', lambda x:x.quantile(0.25)),\n",
    "        ('median', lambda x: x.quantile(0.5)),\n",
    "        ('third_quantile', lambda x: x.quantile(0.75)),\n",
    "        ('missing_values', lambda x: x.isna().sum()),\n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std')\n",
    "    ]).reset_index()\n",
    "    summary['parameter'] = col\n",
    "    summary_list.append(summary)\n",
    "\n",
    "# Combine all summaries\n",
    "summary_crrt_by_mode = pd.concat(summary_list,\n",
    "ignore_index=True)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = ['crrt_mode_category', 'parameter',\n",
    "'total_N', 'missing_values',\n",
    "                'min', 'first_quantile', 'median',\n",
    "'third_quantile', 'max',\n",
    "                'mean', 'std']\n",
    "summary_crrt_by_mode =summary_crrt_by_mode[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "summary_crrt_by_mode.to_csv('../output/final/summary_crrt_by_mode_category.csv', index=False)\n",
    "\n",
    "# Also create a simpler summary showing just the count of each mode\n",
    "mode_counts = crrt_df['crrt_mode_category'].value_counts().reset_index()\n",
    "mode_counts.columns = ['crrt_mode_category','count']\n",
    "mode_counts.to_csv('../output/final/crrt_mode_category_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbab79b",
   "metadata": {},
   "source": [
    "##### CRRT Waterfall Algorithm\n",
    "\n",
    "Numeric columns tracked:\n",
    "â€¢ blood_flow_rate\n",
    "â€¢ pre_filter_replacement_fluid_rate \n",
    "â€¢ post_filter_replacement_fluid_rate\n",
    "â€¢ dialysate_flow_rate\n",
    "â€¢ ultrafiltration_out\n",
    "\n",
    "Waterfall algorithm steps:\n",
    "\n",
    "1. Fill crrt_mode_category based on non-missing numerical columns:\n",
    "\n",
    "   * If all numeric columns are non-NA â†’ CVVHDF\n",
    "   * If only dialysate_flow_rate is NA â†’ CVVHD\n",
    "   * If pre_filter_replacement_fluid_rate and post_filter_replacement_fluid_rate are NA â†’  CVVHD\n",
    "   * If pre_filter_replacement_fluid_rate, post_filter_replacement_fluid_rate and dialysate_flow_rate are NA â†’  SCUF\n",
    "   * If SCUF sandwiched between two other modes, then check if dialysate_flow_rate == 0, blood_flow_rate>0, ultrafiltration_out>0 â†’ True SCUF otherwise, update it to the previous crrt_mode_category\n",
    "\n",
    "2. Fill forward the crrt_mode_category column- only if the time gap is 3h or less across the missing rows\n",
    "\n",
    "3. Create episode IDs:\n",
    "\n",
    "   * Episode definition: One continuous run of the same CRRT setup on same patient\n",
    "   * Gap threshold: Maximum allowed gap within an episode (e.g. \"2h\", \"3h\", \"90min\")\n",
    "     before starting new episode. default = \"3h\"\n",
    "   * a new `crrt_episode_id` starts whenever  \n",
    "       â€¢ `crrt_mode_category` changes **OR**  \n",
    "       â€¢ the gap between successive rows exceeds *gap_thresh* (default 2 h).\n",
    "\n",
    "4. Fill forward numeric variables within each episode ID\n",
    "\n",
    "5. Replace invalid parameters with NA based on CRRT mode. Assuming someone inputted wrong modality. Infer modality\n",
    "   (Note: This step needs verification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(waterfall)\n",
    "import waterfall\n",
    "processed_crrt_df = waterfall.process_crrt_waterfall(crrt_df, \n",
    "                                                     id_col = \"hospitalization_id\",\n",
    "                                                     gap_thresh=\"3h\", \n",
    "                                                     fix_islands = True,\n",
    "                                                     wipe_unused=True,\n",
    "                                                     verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each  crrt_mode_category after applying waterfall\n",
    "# First, identify the numeric columns to summarize\n",
    "numeric_columns = [\n",
    "    'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "\n",
    "# Create summary for each numeric column by  crrt_mode_category\n",
    "summary_list = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    summary =processed_crrt_df.groupby('crrt_mode_category')[col].agg([\n",
    "        ('total_N', 'size'),\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'),\n",
    "        ('first_quantile', lambda x:x.quantile(0.25)),\n",
    "        ('median', lambda x: x.quantile(0.5)),\n",
    "        ('third_quantile', lambda x: x.quantile(0.75)),\n",
    "        ('missing_values', lambda x: x.isna().sum()),\n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std')\n",
    "    ]).reset_index()\n",
    "    summary['parameter'] = col\n",
    "    summary_list.append(summary)\n",
    "\n",
    "# Combine all summaries\n",
    "summary_crrt_by_mode = pd.concat(summary_list, ignore_index=True)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = ['crrt_mode_category', 'parameter',\n",
    "'total_N', 'missing_values',\n",
    "                'min', 'first_quantile', 'median',\n",
    "'third_quantile', 'max',\n",
    "                'mean', 'std']\n",
    "summary_crrt_by_mode =summary_crrt_by_mode[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "summary_crrt_by_mode.to_csv('../output/final/summary_crrt_by_mode_category_post_waterfall.csv', index=False)\n",
    "\n",
    "# Also create a simpler summary showing just the count of each mode\n",
    "mode_counts = crrt_df['crrt_mode_category'].value_counts().reset_index()\n",
    "mode_counts.columns = ['crrt_mode_category','count']\n",
    "mode_counts.to_csv('../output/final/crrt_mode_category_counts_post_waterfall.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_crrt_df.drop(columns=[#'crrt_episode_id', \n",
    "#                                'blood_flow_missing_after_ffill',\n",
    "#                                'post_filter_replacement_fluid_rate_unexpected',\n",
    "#                                'pre_filter_replacement_fluid_rate_unexpected', \n",
    "#                                'dialysate_flow_rate_unexpected'\n",
    "#                                ],\n",
    "#                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CRRT units check\n",
    "unit_expectations = {\n",
    "    \"blood_flow_rate\":                   {\"unit\": \"mL/hr\", \"min\": 1000, \"max\": 20000},\n",
    "    \"pre_filter_replacement_fluid_rate\": {\"unit\": \"mL/hr\", \"min\":    0, \"max\": 10000},\n",
    "    \"post_filter_replacement_fluid_rate\":{\"unit\": \"mL/hr\", \"min\":    0, \"max\": 10000},\n",
    "    \"dialysate_flow_rate\":               {\"unit\": \"mL/hr\", \"min\":  500, \"max\": 10000},\n",
    "    \"ultrafiltration_out\":               {\"unit\": \"mL/hr\", \"min\":    0, \"max\":  5000},\n",
    "}\n",
    "\n",
    "def check_crrt_units(df, expectations, out_folder):\n",
    "    os.makedirs(os.path.join(out_folder, \"final\"), exist_ok=True)\n",
    "    log_path = os.path.join(out_folder, \"final\", \"crrt_unit_warnings.txt\")\n",
    "    # clear previous log\n",
    "    open(log_path, \"w\").close()\n",
    "\n",
    "    for var, spec in expectations.items():\n",
    "        if var not in df:\n",
    "            msg = f\"{var!r} not found in CRRT dataframe, skipping.\"\n",
    "        else:\n",
    "            ser = df[var].dropna()\n",
    "            if ser.empty:\n",
    "                msg = f\"{var!r} has no non-null values, skipping.\"\n",
    "            else:\n",
    "                med = ser.median()\n",
    "                lo, hi = spec[\"min\"], spec[\"max\"]\n",
    "                if lo <= med <= hi:\n",
    "                    msg = f\"{var!r} median={med:.1f} within expected [{lo}-{hi}] {spec['unit']}.\"\n",
    "                else:\n",
    "                    msg = (\n",
    "                        f\"{var!r} median={med:.1f} outside expected \"\n",
    "                        f\"[{lo}-{hi}] {spec['unit']}â€”please inspect units.\"\n",
    "                    )\n",
    "        print(msg)\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Usage: right after crrt_df is loaded\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "check_crrt_units(processed_crrt_df, unit_expectations, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc485c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_crrt_df = pyCLIF.convert_datetime_columns_to_site_tz(processed_crrt_df, pyCLIF.helper['timezone'])\n",
    "crrt_stitched = processed_crrt_df.merge(all_ids[['hospitalization_id',\n",
    "                                           'encounter_block']],\n",
    "                                  on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only hospitalization IDs that are in crrt_stitched\n",
    "all_ids = all_ids[all_ids['hospitalization_id'].isin(crrt_stitched['hospitalization_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"blocks: \", pyCLIF.count_unique_encounters(all_ids, 'encounter_block'))\n",
    "print(\"hosps: \", pyCLIF.count_unique_encounters(all_ids, 'hospitalization_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59381ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts['C_blocks_with_crrt'] = len(crrt_stitched['encounter_block'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e062dd",
   "metadata": {},
   "source": [
    "#### (D) Exclude patients on ESRD \n",
    "\n",
    "Prior to admission ICD codes for ESRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73069c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diagnoses data for our cohort\n",
    "# diagnoses_columns = ['hospitalization_id', 'diagnosis_code', 'diagnosis_code_type'] \n",
    "hospital_diagnosis = pyCLIF.load_data(\n",
    "    'clif_hospital_diagnosis',\n",
    "    filters={'hospitalization_id': list(all_ids['hospitalization_id'])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf419fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_diagnosis_stitched = hospital_diagnosis.merge(all_ids[['hospitalization_id',\n",
    "                                           'encounter_block']],\n",
    "                                  on='hospitalization_id', how='left')\n",
    "hospital_diagnosis_stitched['diagnosis_code'] = hospital_diagnosis_stitched['diagnosis_code'].str.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check present_on_admission column type and standardize to int8\n",
    "if 'present_on_admission' in hospital_diagnosis_stitched.columns:\n",
    "    # Convert to string first to handle any data type\n",
    "    hospital_diagnosis_stitched['present_on_admission'] = hospital_diagnosis_stitched['present_on_admission'].astype(str)\n",
    "    \n",
    "    # Map various possible values to 1/0\n",
    "    hospital_diagnosis_stitched['present_on_admission'] = (\n",
    "        hospital_diagnosis_stitched['present_on_admission']\n",
    "        .str.lower()\n",
    "        .map({'yes': 1, 'y': 1, 'true': 1, '1': 1, 'no': 0, 'n': 0, 'false': 0, '0': 0})\n",
    "        .fillna(0)  # Fill any unmapped values with 0\n",
    "        .astype(np.int8)\n",
    "    )\n",
    "hospital_diagnosis_stitched.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8aec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ESRD diagnosis codes\n",
    "# Let's debug why we're not finding ESRD codes\n",
    "esrd_codes = [\n",
    "    'Z992',    # Dependence on renal dialysis\n",
    "    'Z9115',   # Patient's noncompliance with renal dialysis\n",
    "    'I120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD\n",
    "    'N186',    # End stage renal disease\n",
    "    'I132',    # Hypertensive heart and chronic kidney disease with heart failure and ESRD\n",
    "    'Z992',    # Dependence on renal dialysis (alternate code)\n",
    "    'N186',    # End stage renal disease (alternate code)\n",
    "    'I120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD (alternate code)\n",
    "    'Z91158',  # Patient's noncompliance with renal dialysis (alternate code)\n",
    "    'I1311',   # Hypertensive heart and chronic kidney disease with heart failure and stage 5 CKD\n",
    "    'I132',    # Hypertensive heart and chronic kidney disease with ESRD (alternate code)\n",
    "    '5856',     #ICD9 :End stage renal disease\n",
    "    '40391',    #ICD9: Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease\n",
    "    '40311',     #ICD9: Hypertensive chronic kidney disease, benign, with chronic kidney disease stage V or end stage renal disease\n",
    "    'V4511',     #ICD9: Renal dialysis status\n",
    "    'V4512'     #ICD9: Noncompliance with renal dialysis\n",
    "]\n",
    "\n",
    "# Get hospitalization IDs with ESRD diagnoses and print debug info\n",
    "# print(\"Unique diagnosis codes in data:\", hospital_diagnosis_stitched['diagnosis_code'].unique()[:20], \"...\")\n",
    "# print(\"\\nNumber of rows matching ESRD codes:\", hospital_diagnosis_stitched['diagnosis_code'].isin(esrd_codes).sum())\n",
    "# print(\"\\nSample of matching rows:\")\n",
    "# print(hospital_diagnosis_stitched[hospital_diagnosis_stitched['diagnosis_code'].isin(esrd_codes)].head())\n",
    "\n",
    "# Check if present_on_admission column exists and has valid values\n",
    "\n",
    "# Check if present_on_admission exists and has valid values\n",
    "if ('present_on_admission' in hospital_diagnosis_stitched.columns and \n",
    "    not hospital_diagnosis_stitched['present_on_admission'].isna().all()):\n",
    "    # Get hospitalizations where ESRD was present on admission\n",
    "    esrd_mask = (\n",
    "        hospital_diagnosis_stitched['diagnosis_code'].isin(esrd_codes) & \n",
    "        (hospital_diagnosis_stitched['present_on_admission'] == 1)\n",
    "    )\n",
    "    hosp_ids_with_esrd = hospital_diagnosis_stitched[esrd_mask]['hospitalization_id'].unique()\n",
    "    blocks_with_esrd = hospital_diagnosis_stitched[esrd_mask]['encounter_block'].unique()\n",
    "else:\n",
    "    # If no present_on_admission info or all NAs, use all ESRD diagnoses\n",
    "    hosp_ids_with_esrd = hospital_diagnosis_stitched[hospital_diagnosis_stitched['diagnosis_code'].isin(esrd_codes)]['hospitalization_id'].unique()\n",
    "    blocks_with_esrd = hospital_diagnosis_stitched[hospital_diagnosis_stitched['diagnosis_code'].isin(esrd_codes)]['encounter_block'].unique()\n",
    "\n",
    "\n",
    "\n",
    "strobe_counts['D_hospitalizations_with_esrd'] = len(hosp_ids_with_esrd)\n",
    "strobe_counts['D_encounter_blocks_with_esrd'] = len(hosp_ids_with_esrd)\n",
    "\n",
    "# Create cohort subset excluding hospitalizations with ESRD\n",
    "all_ids = all_ids[~all_ids['hospitalization_id'].isin(hosp_ids_with_esrd)]\n",
    "crrt_stitched = crrt_stitched[~crrt_stitched['hospitalization_id'].isin(hosp_ids_with_esrd)]\n",
    "strobe_counts['D_encounter_blocks_without_esrd'] = len(all_ids['encounter_block'].unique())  # Count blocks without ESRD\n",
    "strobe_counts['D_hospitalizations_without_esrd'] = len(all_ids['hospitalization_id'].unique())  # Count hospitalizations without ESRD\n",
    "\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AKI Codes Sanity check\n",
    "\n",
    "# Define AKI ICD-10 codes\n",
    "aki_codes = [\n",
    "    # ICD-10 codes for acute kidney injury\n",
    "    'N170', 'N171', 'N172', 'N178', 'N179',  # Acute kidney failure codes\n",
    "    'R34',   # Anuria and oliguria\n",
    "    'N990', # Post-procedural kidney failure\n",
    "    'T795',  # Traumatic anuria\n",
    "    '5845',  # ICD9 Acute kidney failure with lesion of tubular necrosis\n",
    "    '5849',  # ICD9- Acute kidney failure, unspecified\n",
    "    \"5848\"    # ICD9 - Acute kidney failure with other specified pathological lesion in kidney\n",
    "]\n",
    "\n",
    "# Create mask for AKI diagnoses and filter to non-ESRD encounters\n",
    "aki_mask = hospital_diagnosis_stitched['diagnosis_code'].isin(aki_codes)\n",
    "non_esrd_encounters = hospital_diagnosis_stitched[hospital_diagnosis_stitched['encounter_block'].isin(all_ids['encounter_block'])]\n",
    "\n",
    "# Get encounter blocks with AKI diagnoses\n",
    "blocks_with_aki = non_esrd_encounters[aki_mask]['encounter_block'].unique()\n",
    "total_non_esrd_blocks = all_ids['encounter_block'].nunique()\n",
    "strobe_counts['D_encounter_blocks_with_AKI_no_esrd'] = len(blocks_with_aki) \n",
    "# Calculate percentage\n",
    "aki_percentage = (len(blocks_with_aki) / total_non_esrd_blocks) * 100\n",
    "\n",
    "print(f\"\\nPercentage of non-ESRD encounter blocks with AKI codes: {aki_percentage:.1f}%\")\n",
    "print(f\"({len(blocks_with_aki)} out of {total_non_esrd_blocks} blocks)\")\n",
    "\n",
    "# Show sample of AKI diagnoses\n",
    "aki_diagnoses = non_esrd_encounters[aki_mask][['hospitalization_id', 'diagnosis_code','diagnosis_code_format', 'diagnosis_name', 'present_on_admission']].drop_duplicates()\n",
    "print(\"\\nSample of AKI-related diagnoses found: check aki_diagnoses.head()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad375d",
   "metadata": {},
   "source": [
    "#### Sanity Check: ADT~  ICU admissions\n",
    "\n",
    "CRRT is typically administered in the ICU setting due to the need for continuous monitoring, specialized nursing care, and close medical supervision. This section validates that our cohort consists of ICU admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT data to only include hospitalizations in all_ids\n",
    "adt_final = adt[adt['hospitalization_id'].isin(all_ids['hospitalization_id'])].copy()\n",
    "print(\"unique encounters in adt_final\", pyCLIF.count_unique_encounters(adt_final))\n",
    "adt_final['hospitalization_id'] = adt_final['hospitalization_id'].astype(str)\n",
    "adt_final = pyCLIF.convert_datetime_columns_to_site_tz(adt_final, pyCLIF.helper['timezone'])\n",
    "adt_final_stitched = adt_final.merge(all_ids[['hospitalization_id', 'encounter_block']], \n",
    "                                     on='hospitalization_id', how='left')\n",
    "adt_final_stitched = adt_final_stitched.sort_values(by=['encounter_block', 'in_dttm'])\n",
    "desired_order = ['hospitalization_id', 'encounter_block', 'hospital_id', 'in_dttm', 'out_dttm']\n",
    "remaining_cols = [col for col in adt_final_stitched.columns if col not in desired_order]\n",
    "adt_final_stitched = adt_final_stitched[desired_order + remaining_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"blocks: \", pyCLIF.count_unique_encounters(adt_final_stitched, 'encounter_block'))\n",
    "print(\"hosps: \", pyCLIF.count_unique_encounters(adt_final_stitched, 'hospitalization_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3139ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Validating ICU Administration ===\")\n",
    "\n",
    "adt_final_stitched['is_icu'] = adt_final_stitched['location_category'] == 'icu'\n",
    "\n",
    "# Check if each hospitalization had at least one ICU stay\n",
    "hosp_icu_status = adt_final_stitched.groupby('encounter_block')['is_icu'].any()\n",
    "non_icu_hosps = hosp_icu_status[~hosp_icu_status].index.tolist()\n",
    "strobe_counts[\"D1_number_hosp_without_ICU_stay\"] = len(non_icu_hosps)\n",
    "print(f\"\\nNumber of CRRT hospitalizations without any ICU stay: {len(non_icu_hosps)}\")\n",
    "if len(non_icu_hosps) > 0:\n",
    "    print(\"WARNING: Found CRRT hospitalizations without ICU stays\")\n",
    "    print(\"First few hospitalization IDs without ICU stays:\", non_icu_hosps[:5])\n",
    "else:\n",
    "    print(\"All CRRT hospitalizations had at least one ICU stay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter patient data to required columns and join with all_ids\n",
    "patient_filtered = patient[['patient_id', 'race_category', 'ethnicity_category', 'sex_category', 'death_dttm', 'language_category']]\n",
    "all_ids = all_ids.merge(patient_filtered, on='patient_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfdbee6",
   "metadata": {},
   "source": [
    "## (E) Exclude encounters who died close to CRRT start\n",
    "\n",
    "Exclude patients who died 6 hours or less after starting CRRT as data for these patients is likely to be skewed towards the extremely sick who were unlikely to ever recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819318f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mortality column based on discharge category\n",
    "all_ids['discharge_category'] = all_ids['discharge_category'].str.lower()\n",
    "all_ids['mortality'] = (all_ids['discharge_category'].isin(['expired', 'hospice'])).astype(np.int8)\n",
    "\n",
    "# Create death_dttm_proxy\n",
    "all_ids['death_dttm_proxy'] = all_ids['death_dttm']\n",
    "\n",
    "# If death_dttm is less than discharge_dttm, use discharge_dttm\n",
    "mask = (all_ids['death_dttm'] < all_ids['discharge_dttm'])\n",
    "all_ids.loc[mask, 'death_dttm_proxy'] = all_ids.loc[mask, 'discharge_dttm']\n",
    "\n",
    "# If death_dttm is missing and mortality=1, use discharge_dttm\n",
    "mask = (all_ids['death_dttm'].isna() & (all_ids['mortality'] == 1))\n",
    "all_ids.loc[mask, 'death_dttm_proxy'] = all_ids.loc[mask, 'discharge_dttm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d513dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first CRRT time for each encounter block\n",
    "first_crrt = (\n",
    "    crrt_stitched\n",
    "    .groupby(\"encounter_block\", as_index=False)[\"recorded_dttm\"]\n",
    "    .min()\n",
    "    .rename(columns={\"recorded_dttm\": \"first_crrt_time\"})\n",
    ")\n",
    "\n",
    "# Merge first CRRT time with all_ids to get death times\n",
    "early_death_df = (\n",
    "    first_crrt.merge(\n",
    "        all_ids[['encounter_block', 'death_dttm_proxy', 'mortality']], \n",
    "        on='encounter_block',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate time from CRRT start to death\n",
    "early_death_df['time_to_death'] = early_death_df['death_dttm_proxy'] - early_death_df['first_crrt_time']\n",
    "\n",
    "# Identify encounters where death occurred within 6 hours of CRRT start\n",
    "early_death_encounters = early_death_df[\n",
    "    (early_death_df['mortality'] == 1) & \n",
    "    (early_death_df['time_to_death'] <= pd.Timedelta(hours=6))\n",
    "]['encounter_block'].tolist()\n",
    "\n",
    "# Update metrics dictionary\n",
    "strobe_counts['E_encounters_early_death'] = len(early_death_encounters)\n",
    "\n",
    "# Remove early death encounters from cohort\n",
    "all_ids = all_ids[~all_ids['encounter_block'].isin(early_death_encounters)]\n",
    "strobe_counts['E_encounters_after_early_death_exclusion'] = len(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cdb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a460d38",
   "metadata": {},
   "source": [
    "# Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f517dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labs\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "\n",
    "labs_of_interest = [\n",
    "    'sodium',\n",
    "    'potassium', \n",
    "    'chloride',\n",
    "    'bicarbonate',\n",
    "    'bun',\n",
    "    'creatinine',\n",
    "    'glucose_serum',\n",
    "    'calcium_total',\n",
    "    'lactate',\n",
    "    'magnesium',\n",
    "    'ph_arterial',\n",
    "    'ph_venous',\n",
    "    'po2_arterial'\n",
    "]\n",
    "\n",
    "\n",
    "labs_filters = {\n",
    "    'hospitalization_id': crrt_df['hospitalization_id'].unique().tolist(),\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)\n",
    "labs = labs.merge(all_ids[['hospitalization_id', 'encounter_block']], \n",
    "                  on='hospitalization_id', how='inner')\n",
    "labs = pyCLIF.convert_datetime_columns_to_site_tz(labs, pyCLIF.helper['timezone'])\n",
    "labs['lab_value_numeric'] = pd.to_numeric(labs['lab_value_numeric'], errors='coerce')\n",
    "labs = labs.sort_values(by=['encounter_block', 'lab_result_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot labs data to get lab values as columns\n",
    "labs_pivoted = labs.pivot_table(\n",
    "    index=['encounter_block', 'lab_result_dttm'],\n",
    "    columns='lab_category',\n",
    "    values='lab_value_numeric'\n",
    ").reset_index().rename(columns={'lab_result_dttm': 'recorded_dttm'})\n",
    "\n",
    "print(\"\\nShape of pivoted labs data:\", labs_pivoted.shape)\n",
    "print(\"\\nColumns in pivoted labs data:\")\n",
    "print(labs_pivoted.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3fc1a",
   "metadata": {},
   "source": [
    "# Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['weight_kg', 'height_cm', \n",
    "                      'heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2']\n",
    "\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(), \n",
    "             'vital_category': vitals_of_interest}\n",
    ")\n",
    "vitals_cohort = pyCLIF.convert_datetime_columns_to_site_tz(vitals_cohort, pyCLIF.helper['timezone'])\n",
    "vitals_cohort['vital_value'] = pd.to_numeric(vitals_cohort['vital_value'], errors='coerce')\n",
    "# sort vitals cohort by hospitalization_id and recorded_dttm\n",
    "vitals_cohort = vitals_cohort.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "# Replace outliers with NAs in the vitals table \n",
    "# Extract min/max values from config for each vital\n",
    "min_hr, max_hr = outlier_cfg['heart_rate']\n",
    "min_rr, max_rr = outlier_cfg['respiratory_rate'] \n",
    "min_sbp, max_sbp = outlier_cfg['sbp']\n",
    "min_dbp, max_dbp = outlier_cfg['dbp']\n",
    "min_map, max_map = outlier_cfg['map']\n",
    "min_spo2, max_spo2 = outlier_cfg['spo2']\n",
    "min_weight, max_weight = outlier_cfg['weight_kg']\n",
    "min_height, max_height = outlier_cfg['height_cm']\n",
    "\n",
    "# For each vital category, set out-of-range values to NaN\n",
    "is_hr = vitals_cohort['vital_category'] == 'heart_rate'\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] < min_hr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_hr & (vitals_cohort['vital_value'] > max_hr), 'vital_value'] = np.nan\n",
    "\n",
    "is_rr = vitals_cohort['vital_category'] == 'respiratory_rate'\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] < min_rr), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_rr & (vitals_cohort['vital_value'] > max_rr), 'vital_value'] = np.nan\n",
    "\n",
    "is_sbp = vitals_cohort['vital_category'] == 'sbp'\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] < min_sbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_sbp & (vitals_cohort['vital_value'] > max_sbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_dbp = vitals_cohort['vital_category'] == 'dbp'\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] < min_dbp), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_dbp & (vitals_cohort['vital_value'] > max_dbp), 'vital_value'] = np.nan\n",
    "\n",
    "is_map = vitals_cohort['vital_category'] == 'map'\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] < min_map), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_map & (vitals_cohort['vital_value'] > max_map), 'vital_value'] = np.nan\n",
    "\n",
    "is_spo2 = vitals_cohort['vital_category'] == 'spo2'\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] < min_spo2), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_spo2 & (vitals_cohort['vital_value'] > max_spo2), 'vital_value'] = np.nan\n",
    "\n",
    "is_weight = vitals_cohort['vital_category'] == 'weight_kg'\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] < min_weight), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_weight & (vitals_cohort['vital_value'] > max_weight), 'vital_value'] = np.nan\n",
    "\n",
    "is_height = vitals_cohort['vital_category'] == 'height_cm'\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] < min_height), 'vital_value'] = np.nan\n",
    "vitals_cohort.loc[is_height & (vitals_cohort['vital_value'] > max_height), 'vital_value'] = np.nan\n",
    "\n",
    "vitals_cohort = vitals_cohort.merge(all_ids[['hospitalization_id', 'encounter_block']], on='hospitalization_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04190519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the index columns\n",
    "duplicates = vitals_cohort.groupby(['encounter_block', 'recorded_dttm', 'vital_category']).size().reset_index(name='count')\n",
    "print(\"Number of duplicate rows before deduplication:\", len(duplicates[duplicates['count'] > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the last value for any duplicates\n",
    "vitals_cohort = vitals_cohort.sort_values(['encounter_block', \n",
    "                                           'recorded_dttm', 'vital_category']).drop_duplicates(\n",
    "    ['encounter_block', 'recorded_dttm', 'vital_category'], \n",
    "    keep='last'\n",
    ")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "duplicates = vitals_cohort.groupby(['encounter_block', 'recorded_dttm', 'vital_category']).size().reset_index(name='count')\n",
    "print(\"Number of duplicate rows after deduplication:\", len(duplicates[duplicates['count'] > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first and last vital recorded times for each encounter block\n",
    "vital_times = vitals_cohort.groupby('encounter_block')['recorded_dttm'].agg(['first', 'last']).reset_index()\n",
    "vital_times = vital_times.rename(columns={\n",
    "    'first': 'first_vital_dttm',\n",
    "    'last': 'last_vital_dttm'\n",
    "})\n",
    "# Join vital times with all_ids to get vital times for all encounter blocks\n",
    "all_ids = all_ids.merge(vital_times, on='encounter_block', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4eff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot \n",
    "vitals_filtered = vitals_cohort[vitals_cohort['vital_category'].isin(vitals_of_interest)]\n",
    "\n",
    "# Pivot the dataframe\n",
    "vitals_pivoted = vitals_filtered.pivot(\n",
    "    index=['encounter_block', 'recorded_dttm'],\n",
    "    columns='vital_category',\n",
    "    values='vital_value'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63763ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join vitals_pivoted with labs_pivoted\n",
    "clif_wide = vitals_pivoted.merge(\n",
    "    labs_pivoted,\n",
    "    on=['encounter_block', 'recorded_dttm'],\n",
    "    how='outer'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first CRRT time for each encounter_block\n",
    "vitals_bmi = crrt_stitched.groupby('encounter_block')['recorded_dttm'].first().reset_index()\n",
    "\n",
    "# Create separate dataframes for weight and height\n",
    "weight_df = vitals_cohort[vitals_cohort['vital_category'] == 'weight_kg'].copy()\n",
    "height_df = vitals_cohort[vitals_cohort['vital_category'] == 'height_cm'].copy()\n",
    "\n",
    "# Function to find closest vital measurement to CRRT start\n",
    "def get_closest_vital(vital_df, crrt_time_df):\n",
    "    vital_values = []\n",
    "    for _, crrt_row in crrt_time_df.iterrows():\n",
    "        hosp_id = crrt_row['encounter_block']\n",
    "        crrt_time = crrt_row['recorded_dttm']\n",
    "        \n",
    "        # Get vitals for this hospitalization\n",
    "        hosp_vitals = vital_df[vital_df['encounter_block'] == hosp_id]\n",
    "        \n",
    "        if len(hosp_vitals) == 0:\n",
    "            vital_values.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        # Calculate time difference and get closest\n",
    "        hosp_vitals['time_diff'] = abs(hosp_vitals['recorded_dttm'] - crrt_time)\n",
    "        closest_vital = hosp_vitals.loc[hosp_vitals['time_diff'].idxmin()]\n",
    "        vital_values.append(closest_vital['vital_value'])\n",
    "        \n",
    "    return vital_values\n",
    "\n",
    "# Get closest weight and height measurements\n",
    "vitals_bmi['weight_kg'] = get_closest_vital(weight_df, vitals_bmi)\n",
    "vitals_bmi['height_cm'] = get_closest_vital(height_df, vitals_bmi)\n",
    "\n",
    "print(\"Summary of measurements at CRRT start:\")\n",
    "print(\"\\nWeight (kg):\")\n",
    "print(vitals_bmi['weight_kg'].describe())\n",
    "print(\"\\nHeight (cm):\")\n",
    "print(vitals_bmi['height_cm'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc873f90",
   "metadata": {},
   "source": [
    "# Vasopressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clif continuous meds for the cohort on vent during the required time period\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol']\n",
    "\n",
    "meds_filters = {\n",
    "    'hospitalization_id': all_ids['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', columns=meds_required_columns, filters=meds_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975783ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.convert_datetime_columns_to_site_tz(meds,  pyCLIF.helper['timezone'])\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "meds = meds.merge(all_ids[['hospitalization_id', 'encounter_block']], on='hospitalization_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9812dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for each med_category and med_dose_unit combination\n",
    "summary_meds_cat_dose= meds.groupby(['med_category', 'med_dose_unit']).agg(\n",
    "    total_N=('med_category', 'size'),\n",
    "    min=('med_dose', 'min'),\n",
    "    max=('med_dose', 'max'),\n",
    "    first_quantile=('med_dose', lambda x: x.quantile(0.25)),\n",
    "    second_quantile=('med_dose', lambda x: x.quantile(0.5)),\n",
    "    third_quantile=('med_dose', lambda x: x.quantile(0.75)),\n",
    "    missing_values=('med_dose', lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "summary_meds_cat_dose.to_csv('../output/final/summary_meds_by_category_dose_units.csv', index=False)\n",
    "## check the distrbituon of required continuous meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check which groups have all NaN values\n",
    "print(\"Groups with all NaN med_dose values:\")\n",
    "for (med_category, med_dose_unit), group in meds.groupby(['med_category', 'med_dose_unit']):\n",
    "    if group['med_dose'].isna().all():\n",
    "        print(f\"  {med_category} - {med_dose_unit}: {len(group)} rows, all NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ca34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS- Check the med_dose_unit for each med_category in the meds table\n",
    "med_dose_unit_check = meds.groupby(['med_category', 'med_dose_unit']).size().reset_index(name='count')\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "med_dose_unit_check['unit_validity'] = med_dose_unit_check.apply(pyCLIF.check_dose_unit, axis=1)\n",
    "\n",
    "# # Optional: Filter for invalid units\n",
    "invalid_units = med_dose_unit_check[med_dose_unit_check['unit_validity'] == 'Not an acceptable unit']\n",
    "print(\"Invalid units. These will be dropped:\\n\")\n",
    "print(invalid_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter meds to include only rows with '/hr' or '/min' in 'med_dose_unit'\n",
    "meds_filtered = meds[~meds['med_dose'].isnull()].copy()\n",
    "meds_filtered = meds_filtered[meds_filtered['med_dose_unit'].apply(pyCLIF.has_per_hour_or_min)].copy()\n",
    "meds_filtered = meds_filtered.merge(vitals_bmi[['encounter_block', 'weight_kg']], on='encounter_block', how='left')\n",
    "meds_filtered[\"med_dose_converted\"] = meds_filtered.apply(pyCLIF.convert_dose, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter doses within acceptable ranges\n",
    "meds_final = meds_filtered[meds_filtered.apply(pyCLIF.is_dose_within_range, axis=1, args=(outlier_cfg,))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69806c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_final.value_counts('med_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f1e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot meds_final to get med categories as columns with converted doses as values\n",
    "# Check for duplicates in the index columns\n",
    "duplicates = meds_final.groupby(['encounter_block', 'admin_dttm', 'med_category']).size().reset_index(name='count')\n",
    "duplicates = duplicates[duplicates['count'] > 1]\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Found duplicate entries for these combinations:\")\n",
    "    print(duplicates)\n",
    "    # Keep first occurrence of each combination\n",
    "    meds_final = meds_final.drop_duplicates(['encounter_block', 'admin_dttm', 'med_category'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_final_pivoted = meds_final.pivot(\n",
    "    index=['encounter_block', 'admin_dttm'],\n",
    "    columns='med_category',\n",
    "    values='med_dose_converted'\n",
    ").reset_index()\n",
    "meds_final_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename admin_dttm to med_admin_dttm before joining\n",
    "meds_final_pivoted = meds_final_pivoted.rename(columns={'admin_dttm': 'recorded_dttm'})\n",
    "\n",
    "# Join meds_final_pivoted with labs_pivoted\n",
    "clif_wide = meds_final_pivoted.merge(\n",
    "    clif_wide,\n",
    "    on=['encounter_block', 'recorded_dttm'],\n",
    "    how='outer'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be9dfc",
   "metadata": {},
   "source": [
    "# Respiratory support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== STEP C: Load & process respiratory support => Apply Waterfall ===\\n\")\n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set'\n",
    "\n",
    "]\n",
    "\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': all_ids['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "resp_support['tracheostomy'] = pd.to_numeric(resp_support['tracheostomy'], errors='coerce').astype('Int8')\n",
    "resp_support = resp_support.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "\n",
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_support['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")\n",
    "\n",
    "pyCLIF.apply_outlier_thresholds(resp_support, 'fio2_set', *outlier_cfg['fio2_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_support, 'peep_set', *outlier_cfg['peep_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_support, 'lpm_set',  *outlier_cfg['lpm_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_support, 'resp_rate_set', *outlier_cfg['resp_rate_set'])\n",
    "pyCLIF.apply_outlier_thresholds(resp_support, 'resp_rate_obs', *outlier_cfg['resp_rate_obs'])\n",
    "\n",
    "del resp_support_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f61657",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(waterfall)\n",
    "processed_resp_support = waterfall.process_resp_support_waterfall(resp_support, \n",
    "                                                        id_col = \"hospitalization_id\",\n",
    "                                                        verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_resp_support = pyCLIF.convert_datetime_columns_to_site_tz(processed_resp_support, pyCLIF.helper['timezone'])\n",
    "processed_resp_support = processed_resp_support.merge(all_ids[['hospitalization_id', 'encounter_block']], on='hospitalization_id', how='inner')\n",
    "# Pivot meds_final to get med categories as columns with converted doses as values\n",
    "# Check for duplicates in the index columns\n",
    "duplicates = processed_resp_support.groupby(['encounter_block', 'recorded_dttm']).size().reset_index(name='count')\n",
    "duplicates = duplicates[duplicates['count'] > 1]\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Found duplicate entries for these combinations:\")\n",
    "    print(duplicates)\n",
    "    # Keep first occurrence of each combination\n",
    "    processed_resp_support = processed_resp_support.drop_duplicates(['encounter_block', 'recorded_dttm'], keep='first')\n",
    "else:\n",
    "    print(\"No duplicates in respiratory support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9938356",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_resp_support.drop(columns=['is_scaffold', 'device_cat_id', 'device_id', 'mode_cat_id', 'mode_name_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611faa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clif_wide = clif_wide.merge(\n",
    "    processed_resp_support,\n",
    "    on=['encounter_block', 'recorded_dttm'],\n",
    "    how='outer'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab02426",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4a323",
   "metadata": {},
   "source": [
    "# Save final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final datasets as parquet files\n",
    "all_ids.to_parquet('../output/intermediate/all_ids.parquet')\n",
    "adt_final_stitched.to_parquet('../output/intermediate/adt_final.parquet') \n",
    "clif_wide.to_parquet('../output/intermediate/clif_wide.parquet')\n",
    "crrt_stitched.to_parquet('../output/intermediate/crrt_df.parquet')\n",
    "\n",
    "print(\"Saved final datasets as parquet files to output/intermediate folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes and print their schemas\n",
    "print(\"\\nall_ids schema:\")\n",
    "print(all_ids.dtypes)\n",
    "all_ids.to_parquet('../output/intermediate/all_ids.parquet')\n",
    "\n",
    "print(\"\\nadt_final_stitched schema:\")\n",
    "print(adt_final_stitched.dtypes) \n",
    "adt_final_stitched.to_parquet('../output/intermediate/adt_final.parquet')\n",
    "\n",
    "print(\"\\nclif_wide schema:\")\n",
    "print(clif_wide.dtypes)\n",
    "clif_wide.to_parquet('../output/intermediate/clif_wide.parquet')\n",
    "\n",
    "print(\"\\ncrrt_stitched schema:\")\n",
    "print(crrt_stitched.dtypes)\n",
    "crrt_stitched.to_parquet('../output/intermediate/crrt_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_consort_diagram(\n",
    "        strobe_counts: dict,\n",
    "        site_name: str ,\n",
    "        output_folder: str = \"./outputs\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    CONSORT / STROBE-style cohort diagram for the CRRT study.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strobe_counts : dict\n",
    "        Dictionary with keys produced by the pipeline.\n",
    "    site_name : str\n",
    "        Label to append to the saved PNG.\n",
    "    output_folder : str\n",
    "        Root folder where   â€¦/final/graphs/consort_diagram_<site>.png   is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ numbers we need\n",
    "    start_n        = strobe_counts[\"A_total_adult_hospitalizations\"]\n",
    "    stitched_n     = strobe_counts[\"B_after_stitching\"]\n",
    "    stitched_drop  = strobe_counts[\"B_before_stitching\"] - stitched_n\n",
    "    crrt_n         = strobe_counts[\"C_hospitalizations_with_crrt\"]\n",
    "    no_crrt_drop   = stitched_n - crrt_n\n",
    "    esrd_drop      = strobe_counts[\"D_hospitalizations_with_esrd\"]\n",
    "    final_n        = strobe_counts[\"D_hospitalizations_without_esrd\"]\n",
    "    early_death_n  = strobe_counts[\"E_encounters_early_death\"]\n",
    "    final_cohort_n = strobe_counts[\"E_encounters_after_early_death_exclusion\"]\n",
    "\n",
    "    aki_blocks     = strobe_counts.get(\"D_encounter_blocks_with_AKI_no_esrd\", 0)\n",
    "    aki_pct        = aki_blocks / final_n if final_n else 0\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ plotting scaffold\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Cohort Selection for\\nCRRT Study for {site_name}\",\n",
    "                 fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    # main boxes top-to-bottom\n",
    "    main_keys  = [\"start\", \"stitched\", \"crrt\", \"final\", \"final_e\"]\n",
    "    y_levels   = [0.85, 0.65, 0.45, 0.25, 0.05]\n",
    "    x_main     = 0.35\n",
    "    main_boxes = {\n",
    "        \"start\":   (f\"Adult hospitalizations\\n(date + age filters)\\n(n={start_n:,})\",\n",
    "                    (x_main, y_levels[0])),\n",
    "        \"stitched\":(f\"After encounter stitching\\n(n={stitched_n:,})\",\n",
    "                    (x_main, y_levels[1])),\n",
    "        \"crrt\":    (f\"Hospitalizations with CRRT\\n(n={crrt_n:,})\",\n",
    "                    (x_main, y_levels[2])),\n",
    "        \"final\":   (f\"Hospitalizations without ESRD\\n(n={final_n:,})\"\n",
    "                    f\"\\nAKI in {aki_pct:.1%} of blocks\",\n",
    "                    (x_main, y_levels[3])),\n",
    "        \"final_e\": (f\"Final analytic cohort\\n(n={final_cohort_n:,})\",\n",
    "                    (x_main, y_levels[4]))\n",
    "    }\n",
    "\n",
    "    # exclusion boxes (one between each pair of mains)\n",
    "    excl_info = [\n",
    "        (f\"Excluded:\\nno stitched data\\n(n={stitched_drop:,})\", 0),   # above stitching box\n",
    "        (f\"Excluded:\\nno CRRT\\n(n={no_crrt_drop:,})\",         1),   # above CRRT box\n",
    "        (f\"Excluded:\\nESRD\\n(n={esrd_drop:,})\",               2),    # above final box\n",
    "        (f\"Excluded:\\nearly death (n={early_death_n:,})\", 3)  # above final_e box\n",
    "    ]\n",
    "\n",
    "    box_props   = dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\",\n",
    "                       edgecolor=\"black\", linewidth=1.2)\n",
    "    arrow_props = dict(arrowstyle=\"->\", color=\"black\", lw=1.3)\n",
    "\n",
    "    # â”€â”€ draw main boxes + vertical arrows\n",
    "    for k, (txt, (x, y)) in main_boxes.items():\n",
    "        ax.text(x, y, txt, ha=\"center\", va=\"center\", bbox=box_props, fontsize=11)\n",
    "\n",
    "    for i in range(len(main_keys)-1):\n",
    "        y1, y2 = y_levels[i] - 0.05, y_levels[i+1] + 0.05\n",
    "        ax.annotate(\"\", xy=(x_main, y2), xytext=(x_main, y1),\n",
    "                    arrowprops=arrow_props)\n",
    "\n",
    "    # â”€â”€ draw exclusion boxes + side arrows\n",
    "    x_excl = 0.75\n",
    "    for txt, idx in excl_info:\n",
    "        y_mid = (y_levels[idx] + y_levels[idx+1]) / 2\n",
    "        ax.text(x_excl, y_mid, txt, ha=\"left\", va=\"center\",\n",
    "                bbox=box_props, fontsize=9)\n",
    "        ax.annotate(\"\", xy=(x_excl-0.02, y_mid),\n",
    "                    xytext=(x_main+0.05, y_mid),\n",
    "                    arrowprops=arrow_props)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # â”€â”€ save\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    save_path = os.path.join(output_folder, f\"consort_diagram_{site_name}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved CONSORT diagram to: {save_path}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396fe95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame and save it as a CSV file\n",
    "pd.DataFrame(list(strobe_counts.items()), columns=['Metric', 'Value']).to_csv('../output/final/strobe_counts.csv', index=False)\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462845c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ usage example\n",
    "draw_consort_diagram(strobe_counts, site_name=pyCLIF.helper['site_name'], output_folder=\"../output/final/graphs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".crrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06e2b67",
   "metadata": {},
   "source": [
    "# Epidemiology of CRRT- Analysis\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "\n",
    "Run this script after 01_cohort_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6219f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "import pyarrow\n",
    "from tableone import TableOne\n",
    "import seaborn as sns\n",
    "import sofa_score  \n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyCLIF\n",
    "output_folder = '../output'\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "    outlier_cfg = json.load(f)\n",
    "\n",
    "import os\n",
    "output_dir = \"../output/final\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27607f",
   "metadata": {},
   "source": [
    "# Load CLIF wide and other cohort datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65075c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids_df = pd.read_parquet(f'{output_folder}/intermediate/all_ids.parquet')\n",
    "adt_final_df = pd.read_parquet(f'{output_folder}/intermediate/adt_final.parquet')\n",
    "clif_wide_df = pd.read_parquet(f'{output_folder}/intermediate/clif_wide.parquet')\n",
    "crrt_df = pd.read_parquet(f'{output_folder}/intermediate/crrt_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1b989",
   "metadata": {},
   "source": [
    "# Combine CLIF wide with ADT\n",
    "\n",
    "Combine CLIF wide with ADT and forward fill location category and location type columns to get patient location info at each time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb867033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) ANNOTATE CLIF-WIDE RECORDS WITH ADT LOCATION (FORWARD-FILL ONLY)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "adt_int = adt_final_df[[\n",
    "    \"encounter_block\",\n",
    "    \"in_dttm\", \"out_dttm\",\n",
    "    \"location_category\", \"location_type\"\n",
    "]]\n",
    "\n",
    "# 1a) Every flowsheet row matched to all ADT intervals for that encounter\n",
    "merged = clif_wide_df.merge(adt_int, on=\"encounter_block\", how=\"left\")\n",
    "\n",
    "# 1b) Keep only rows where recorded_dttm ∈ [in_dttm, out_dttm]\n",
    "mask = (\n",
    "    (merged[\"recorded_dttm\"] >= merged[\"in_dttm\"]) &\n",
    "    (merged[\"recorded_dttm\"] <= merged[\"out_dttm\"])\n",
    ")\n",
    "annot = merged.loc[mask, \n",
    "    [\"encounter_block\", \"recorded_dttm\", \"location_category\", \"location_type\"]\n",
    "].copy()\n",
    "\n",
    "# 1c) Forward-fill per encounter_block\n",
    "annot = (\n",
    "    annot\n",
    "    .sort_values([\"encounter_block\",\"recorded_dttm\"])\n",
    "    .groupby(\"encounter_block\", as_index=False)\n",
    "    .apply(lambda df: df.ffill())\n",
    "    .dropna(subset=[\"location_category\",\"location_type\"], how=\"all\")\n",
    "    # now has a flat index: (encounter_block, recorded_dttm, ...)\n",
    ")\n",
    "\n",
    "# 1d) Re-index `annot` on the two keys\n",
    "annot_indexed = annot.set_index([\"encounter_block\",\"recorded_dttm\"])[\n",
    "    [\"location_category\",\"location_type\"]\n",
    "]\n",
    "\n",
    "# 1e) Join back onto `clif_wide_df`\n",
    "clif_wide_df = (\n",
    "    clif_wide_df\n",
    "      .set_index([\"encounter_block\",\"recorded_dttm\"])\n",
    "      .join(annot_indexed, how=\"left\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Now clif_wide_annot has the ADT location forward-filled from the last interval that\n",
    "# covered each timestamp, with no peeking into future ADT rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== all_ids_df dtypes ===\")\n",
    "print(all_ids_df.dtypes)\n",
    "print(\"\\n=== clif_wide_df dtypes ===\")\n",
    "print(clif_wide_df.dtypes)\n",
    "print(\"\\n=== crrt_df dtypes ===\")\n",
    "print(crrt_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a116e",
   "metadata": {},
   "source": [
    "# (A) Summary by Time from CRRT Start\n",
    "\n",
    "* Pre- 24 hr CRRT start\n",
    "* Post-24 hr CRRT start\n",
    "* Post-72 hr CRRT start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0) CONSTANTS & INPUTS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "os.makedirs(os.path.join(output_folder, \"final\"), exist_ok=True)\n",
    "\n",
    "# path & mapping for SOFA\n",
    "tables_path = pyCLIF.helper['tables_path']\n",
    "id_mappings = all_ids_df[['encounter_block','hospitalization_id']].drop_duplicates()\n",
    "\n",
    "# lists of variables\n",
    "demog_cols    = [\"age_at_admission\",\"sex_category\",\"race_category\",\"ethnicity_category\"]\n",
    "adt_cols      = [\"location_category\",\"location_type\"]\n",
    "device_col    = \"device_category\"\n",
    "\n",
    "continuous_vars = [\n",
    "    # vasoactives\n",
    "    \"angiotensin\",\"dobutamine\",\"dopamine\",\"epinephrine\",\n",
    "    \"norepinephrine\",\"phenylephrine\",\"vasopressin\",\n",
    "    # labs\n",
    "    \"bicarbonate\",\"bun\",\"calcium_total\",\"chloride\",\"creatinine\",\"magnesium\",\n",
    "    \"glucose_serum\",\"lactate\",\"potassium\",\"sodium\",\"ph_arterial\",\"po2_arterial\",\n",
    "    # vents\n",
    "    \"fio2_set\",\"peep_set\",\"resp_rate_set\",\"tidal_volume_set\",\n",
    "    \"pressure_control_set\",\"pressure_support_set\",\"peak_inspiratory_pressure_set\",\n",
    "]\n",
    "\n",
    "crrt_vars = [\n",
    "    \"blood_flow_rate\",\n",
    "    \"pre_filter_replacement_fluid_rate\",\n",
    "    \"post_filter_replacement_fluid_rate\",\n",
    "    \"dialysate_flow_rate\",\n",
    "    \"ultrafiltration_out\",\n",
    "]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) FIRST CRRT START PER ENCOUNTER\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "first_crrt = (\n",
    "    crrt_df\n",
    "    .groupby(\"encounter_block\", as_index=False)[\"recorded_dttm\"]\n",
    "    .min()\n",
    "    .rename(columns={\"recorded_dttm\":\"first_crrt_time\"})\n",
    ")\n",
    "\n",
    "# pull demographics + death\n",
    "demog = (\n",
    "    all_ids_df\n",
    "    .set_index(\"encounter_block\")[demog_cols + [\"death_dttm_proxy\"]]\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) helper: summarize any window\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def summarize_window(start_offset_h: int, end_offset_h: int, include_crrt: bool):\n",
    "    \"\"\"\n",
    "    Summarize for each encounter_block in [first_crrt_time+start_offset_h, first_crrt_time+end_offset_h]:\n",
    "     - demographics + died_within_window\n",
    "     - median [Q1,Q3] of continuous_vars\n",
    "     - last non-null ADT location + type\n",
    "     - last non-null device_category\n",
    "     - SOFA components + total\n",
    "     - if include_crrt: last CRRT mode + median [Q1,Q3] of crrt_vars\n",
    "    Returns DataFrame indexed by encounter_block.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2a) window definitions\n",
    "    start_off = pd.Timedelta(hours=start_offset_h)\n",
    "    end_off   = pd.Timedelta(hours=end_offset_h)\n",
    "    bounds    = first_crrt.copy()\n",
    "    bounds[\"win_start\"] = bounds[\"first_crrt_time\"] + start_off\n",
    "    bounds[\"win_end\"]   = bounds[\"first_crrt_time\"] + end_off\n",
    "    bnd        = bounds.set_index(\"encounter_block\")\n",
    "\n",
    "    # 2b) CLIF-wide flowsheet slice\n",
    "    cw = (\n",
    "        clif_wide_df\n",
    "        .merge(bounds[[\"encounter_block\",\"win_start\",\"win_end\"]], on=\"encounter_block\", how=\"inner\")\n",
    "    )\n",
    "    cw = cw[(cw.recorded_dttm >= cw.win_start) & (cw.recorded_dttm <= cw.win_end)]\n",
    "\n",
    "    # continuous vars: median & IQR\n",
    "    med = cw.groupby(\"encounter_block\")[continuous_vars].median()\n",
    "    q1  = cw.groupby(\"encounter_block\")[continuous_vars].quantile(0.25)\n",
    "    q3  = cw.groupby(\"encounter_block\")[continuous_vars].quantile(0.75)\n",
    "    cont = (\n",
    "        med.add_suffix(\"_median\")\n",
    "           .join(q1.add_suffix(\"_q1\"))\n",
    "           .join(q3.add_suffix(\"_q3\"))\n",
    "    )\n",
    "\n",
    "    # ADT location/type: last non-null in window\n",
    "    loc_win = cw.dropna(subset=adt_cols, how=\"all\")\n",
    "    last_loc = (\n",
    "        loc_win.sort_values(\"recorded_dttm\")\n",
    "               .groupby(\"encounter_block\")[adt_cols]\n",
    "               .last()\n",
    "    )\n",
    "\n",
    "    # device_category: last non-null in window\n",
    "    dev_win = cw.dropna(subset=[device_col])\n",
    "    last_dev = (\n",
    "        dev_win.sort_values(\"recorded_dttm\")\n",
    "               .groupby(\"encounter_block\")[[device_col]]\n",
    "               .last()\n",
    "    )\n",
    "    # merge death times into  window bounds\n",
    "    win = bnd.join(demog[[\"death_dttm_proxy\"]], how=\"left\")\n",
    "\n",
    "    # mortality flag\n",
    "    death_flag = (\n",
    "        win[\"death_dttm_proxy\"]\n",
    "        .between(win[\"win_start\"], win[\"win_end\"])\n",
    "        .rename(\"died_within_window\")\n",
    "    )\n",
    "\n",
    "    # assemble core\n",
    "    df = (\n",
    "        bnd\n",
    "        .join(demog.drop(columns=\"death_dttm_proxy\"))\n",
    "        .join(death_flag)\n",
    "        .join(cont)\n",
    "        .join(last_loc)\n",
    "        .join(last_dev)\n",
    "    )\n",
    "\n",
    "    # 2c) SOFA\n",
    "    sofa_in = bounds[[\"encounter_block\"]].copy()\n",
    "    sofa_in[\"start_dttm\"] = bounds[\"win_start\"]\n",
    "    sofa_in[\"stop_dttm\"]  = bounds[\"win_end\"]\n",
    "    sofa_out = sofa_score.compute_sofa(\n",
    "        ids_w_dttm            = sofa_in,\n",
    "        tables_path           = tables_path,\n",
    "        use_hospitalization_id= False,\n",
    "        id_mapping            = id_mappings,\n",
    "        helper_module         = pyCLIF,\n",
    "        output_filepath       = None\n",
    "    )\n",
    "    sofa_cols = [\n",
    "      \"sofa_cv_97\",\"sofa_coag\",\"sofa_renal\",\n",
    "      \"sofa_liver\",\"sofa_resp\",\"sofa_cns\",\"sofa_total\"\n",
    "    ]\n",
    "    sofa_df = sofa_out.set_index(\"encounter_block\")[sofa_cols]\n",
    "    df = df.join(sofa_df)\n",
    "\n",
    "    # 2d) CRRT-specific (optional)\n",
    "    if include_crrt:\n",
    "        cr = crrt_df.merge(bounds.reset_index(), on=\"encounter_block\", how=\"inner\")\n",
    "        cr_win = cr[(cr.recorded_dttm >= cr.win_start) & (cr.recorded_dttm <= cr.win_end)]\n",
    "\n",
    "        # last CRRT mode\n",
    "        mode_win = cr_win.dropna(subset=[\"crrt_mode_category\"])\n",
    "        last_mode = (\n",
    "            mode_win.sort_values(\"recorded_dttm\")\n",
    "                    .groupby(\"encounter_block\")[\"crrt_mode_category\"]\n",
    "                    .last()\n",
    "                    .rename(\"last_crrt_mode\")\n",
    "        )\n",
    "\n",
    "        # CRRT cont. vars\n",
    "        cm = cr_win.groupby(\"encounter_block\")[crrt_vars].median()\n",
    "        c1 = cr_win.groupby(\"encounter_block\")[crrt_vars].quantile(0.25)\n",
    "        c3 = cr_win.groupby(\"encounter_block\")[crrt_vars].quantile(0.75)\n",
    "        crrt_cont = (\n",
    "            cm.add_suffix(\"_median\")\n",
    "              .join(c1.add_suffix(\"_q1\"))\n",
    "              .join(c3.add_suffix(\"_q3\"))\n",
    "        )\n",
    "\n",
    "        df = df.join(last_mode).join(crrt_cont)\n",
    "\n",
    "    return df.drop(columns=[\"first_crrt_time\",\"win_start\",\"win_end\"])\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) BUILD THE THREE WINDOWS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "pre24  = summarize_window(-24,  0, include_crrt=False)\n",
    "post24 = summarize_window(  0, 24, include_crrt=True)\n",
    "post72 = summarize_window(  0, 72, include_crrt=True)\n",
    "\n",
    "# tag & stack\n",
    "for df, lbl in [(pre24,\"Pre-24h\"),(post24,\"Post-24h\"),(post72,\"Post-72h\")]:\n",
    "    df[\"window\"] = lbl\n",
    "\n",
    "combined = pd.concat([pre24,post24,post72], axis=0).reset_index()\n",
    "combined.to_csv(os.path.join(output_folder,\"intermediate\",\"combined_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9aa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) MAKE TABLEONE FOR EACH WINDOW (with SOFA)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def make_tableone(summary_df, window_label):\n",
    "    df = summary_df.reset_index()\n",
    "\n",
    "    # base set of categorical & continuous variables\n",
    "    cat_vars = [\n",
    "      \"sex_category\",\"race_category\",\"ethnicity_category\",\n",
    "      \"location_category\",\"location_type\",\"device_category\",\n",
    "      \"died_within_window\"\n",
    "    ]\n",
    "\n",
    "    cont_vars = (\n",
    "      [\"age_at_admission\"]\n",
    "      + continuous_vars\n",
    "      # add SOFA components here:\n",
    "      + [\"sofa_cv_97\",\"sofa_coag\",\"sofa_renal\",\n",
    "         \"sofa_liver\",\"sofa_resp\",\"sofa_cns\",\"sofa_total\"]\n",
    "    )\n",
    "\n",
    "    # for post-windows also include CRRT mode + CRRT vars\n",
    "    if \"last_crrt_mode\" in df.columns:\n",
    "        cat_vars.append(\"last_crrt_mode\")\n",
    "        cont_vars += crrt_vars\n",
    "\n",
    "    # flatten any *_median → base var and drop *_q1/_q3\n",
    "    for v in continuous_vars + crrt_vars:\n",
    "        m = f\"{v}_median\"\n",
    "        if m in df:\n",
    "            df[v] = df.pop(m)\n",
    "        for suf in (\"_q1\",\"_q3\"):\n",
    "            col = v + suf\n",
    "            if col in df:\n",
    "                df.pop(col)\n",
    "\n",
    "    # Round vasopressors to 4 decimal places\n",
    "    vasopressors = [\"angiotensin\", \"dobutamine\", \"dopamine\", \"epinephrine\",\n",
    "                    \"norepinephrine\", \"phenylephrine\", \"vasopressin\"]\n",
    "    for v in vasopressors:\n",
    "        if v in df.columns:\n",
    "            df[v] = df[v].round(4)\n",
    "\n",
    "    # build the TableOne; for all cont_vars it will automatically compute\n",
    "    # median [IQR] because we pass them in `nonnormal=`\n",
    "    tbl = TableOne(\n",
    "      df,\n",
    "      columns     = cat_vars + cont_vars,\n",
    "      categorical = cat_vars,\n",
    "      nonnormal   = cont_vars,\n",
    "      groupby     = None\n",
    "    )\n",
    "\n",
    "    # save & return\n",
    "    fname = f\"table1_{window_label.replace(' ','_')}.csv\"\n",
    "    tbl.to_csv(os.path.join(output_folder,\"final\",fname))\n",
    "    print(\"Table saved to:\", f\"{output_folder}/final/{fname}\")\n",
    "    return tbl\n",
    "\n",
    "# regenerate your tables:\n",
    "table_pre24  = make_tableone(pre24,  \"Pre-24h\")\n",
    "table_post24 = make_tableone(post24, \"Post-24h\")\n",
    "table_post72 = make_tableone(post72, \"Post-72h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500572ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Extract the underlying pandas DataFrames from each TableOne\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "df_pre24  = pd.DataFrame(table_pre24.tableone).rename(columns={0: \"Pre-24h\",   \"Overall\": \"pre24_summary\", \"Missing\": \"pre24_missing\"})\n",
    "df_post24 = pd.DataFrame(table_post24.tableone).rename(columns={0: \"Post-24h\", \"Overall\": \"post24_summary\", \"Missing\": \"post24_missing\"})\n",
    "df_post72 = pd.DataFrame(table_post72.tableone).rename(columns={0: \"Post-72h\", \"Overall\": \"post72_summary\", \"Missing\": \"post72_missing\"})\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Join them on their index (the \"characteristic\" labels) using post24/72 order\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Get reference order from post24 or post72 if they exist\n",
    "ref_order = None\n",
    "if not df_post24.empty:\n",
    "    ref_order = df_post24.index.tolist()\n",
    "elif not df_post72.empty:\n",
    "    ref_order = df_post72.index.tolist()\n",
    "\n",
    "combined = (\n",
    "    df_pre24\n",
    "      .join(df_post24, how=\"outer\")\n",
    "      .join(df_post72, how=\"outer\")\n",
    ")\n",
    "\n",
    "# Reorder rows if we have a reference order\n",
    "if ref_order:\n",
    "    # Only use existing indices from reference order\n",
    "    valid_indices = [idx for idx in ref_order if idx in combined.index]\n",
    "    # Add any remaining indices that weren't in reference\n",
    "    remaining = combined.index.difference(valid_indices).tolist()\n",
    "    combined = combined.reindex(valid_indices + remaining)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Clean up the index name and display\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "combined.index.name = \"Characteristic\"\n",
    "combined.reset_index(inplace=True)\n",
    "\n",
    "# Drop all *_missing columns\n",
    "missing_cols = [col for col in combined.columns if col.endswith('_missing')]\n",
    "combined = combined.drop(columns=missing_cols)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Save to CSV if you like\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "filename = f\"table1_all_windows_{pyCLIF.helper['site_name']}.csv\"\n",
    "combined.to_csv(os.path.join(output_folder, \"final\", filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887f21e",
   "metadata": {},
   "source": [
    "# (B) Summary by Time, Mortality and CRRT status\n",
    "\n",
    "\n",
    "* Look at summary of characteristics in eight subcohorts \n",
    "     * Survivors - On CRRT - Post-24hr\n",
    "     * Survivors - Off CRRT - Post-24hr\n",
    "     * Survivors - On CRRT - Post-72hr\n",
    "     * Survivors - Off CRRT- Post-72hr\n",
    "     * Non-survivors - On CRRT - Post-24hr\n",
    "     * Non-survivors - Off CRRT - Post-24hr\n",
    "     * Non-survivors - On CRRT - Post-72hr\n",
    "     * Non-survivors - Off CRRT - Post-72hr\n",
    "\n",
    "* Derive per-encounter flags\n",
    "    * Survivor vs non-survivor: from all_ids_df.mortality + discharge_dttm as a proxy for death_dttm. Mortality is defined as discharge category == \"Expired\" or \"Hospice\"\n",
    "    * CRRT end time: for each encounter_block, take the max recorded_dttm in crrt_df as the end of CRRT\n",
    "    * “Still on CRRT” at 24 h (and 72 h): compare first_crrt_time + N h to that end time. If CRRT end ≥ window‐end ⇒ “still on CRRT”. Else ⇒ “off CRRT”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(output_folder, \"final\", \"table_one.log\")\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "# 1) Get (or create) our logger\n",
    "logger = logging.getLogger(\"table_one\")\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 2) Remove any handlers already registered on this logger\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# 3) Create and configure our console handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# 4) Create and configure our file handler\n",
    "fh = logging.FileHandler(log_path, mode=\"a\")\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "# 5) Create a shared formatter\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 6) Attach handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_crrt = (\n",
    "    crrt_df\n",
    "    .groupby(\"encounter_block\", as_index=False)[\"recorded_dttm\"]\n",
    "    .min()\n",
    "    .rename(columns={\"recorded_dttm\": \"first_crrt_time\"})\n",
    ")\n",
    "\n",
    "end_crrt = (\n",
    "    crrt_df\n",
    "    .groupby(\"encounter_block\", as_index=False)[\"recorded_dttm\"]\n",
    "    .max()\n",
    "    .rename(columns={\"recorded_dttm\": \"end_crrt_time\"})\n",
    ")\n",
    "\n",
    "enc = (\n",
    "    all_ids_df[[\"encounter_block\", \"mortality\", \"discharge_dttm\", \"death_dttm_proxy\"]]\n",
    "    .merge(first_crrt, on=\"encounter_block\")\n",
    "    .merge(end_crrt,   on=\"encounter_block\"))\n",
    "\n",
    "# flags: still on CRRT at 24 h and 72 h\n",
    "enc[\"on_crrt_at_24h\"] = (\n",
    "    enc[\"end_crrt_time\"] >= enc[\"first_crrt_time\"] + pd.Timedelta(hours=24)\n",
    ")\n",
    "enc[\"on_crrt_at_72h\"] = (\n",
    "    enc[\"end_crrt_time\"] >= enc[\"first_crrt_time\"] + pd.Timedelta(hours=72)\n",
    ")\n",
    "\n",
    "# death before or at 24 h post‐start\n",
    "enc[\"died_within_24h\"] = (\n",
    "    enc[\"death_dttm_proxy\"].notna() &\n",
    "    (enc[\"death_dttm_proxy\"] <= enc[\"first_crrt_time\"] + pd.Timedelta(hours=24))\n",
    ")\n",
    "# death before or at 72 h post‐start\n",
    "enc[\"died_within_72h\"] = (\n",
    "    enc[\"death_dttm_proxy\"].notna() &\n",
    "    (enc[\"death_dttm_proxy\"] <= enc[\"first_crrt_time\"] + pd.Timedelta(hours=72))\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "# ───────────────────────────────────────────────\n",
    "# Sanity checks with logging\n",
    "# ───────────────────────────────────────────────\n",
    "if not (enc[\"first_crrt_time\"] <= enc[\"end_crrt_time\"]).all():\n",
    "    logger.error(\"Some first_crrt_time > end_crrt_time!!! That's not cool!\")\n",
    "else:\n",
    "    logger.info(\"All first_crrt_time ≤ end_crrt_time.\")\n",
    "\n",
    "logger.info(f\"Total encounters: {len(enc)}\")\n",
    "mort_counts = enc[\"mortality\"].value_counts().to_dict()\n",
    "logger.info(f\"Mortality distribution: {mort_counts}\")\n",
    "p24 = enc[\"on_crrt_at_24h\"].mean()\n",
    "p72 = enc[\"on_crrt_at_72h\"].mean()\n",
    "logger.info(f\"Still on CRRT at 24 h: {p24:.1%}\")\n",
    "logger.info(f\"Still on CRRT at 72 h: {p72:.1%}\")\n",
    "d24 = enc[\"died_within_24h\"].mean()\n",
    "d72 = enc[\"died_within_72h\"].mean()\n",
    "logger.info(f\"Died within 24 h of CRRT start: {d24:.1%}\")\n",
    "logger.info(f\"Died within 72 h of CRRT start: {d72:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# died in the 24 h before CRRT start?\n",
    "enc[\"died_pre24h\"] = (\n",
    "    enc[\"death_dttm_proxy\"].notna() &\n",
    "    (enc[\"death_dttm_proxy\"] >= enc[\"first_crrt_time\"] - pd.Timedelta(hours=24)) &\n",
    "    (enc[\"death_dttm_proxy\"]  < enc[\"first_crrt_time\"])\n",
    ")\n",
    "\n",
    "# survivors at CRRT start = nobody died before or at start:\n",
    "enc[\"survived_to_start\"] = ~enc[\"died_pre24h\"]\n",
    "\n",
    "# survivors still on CRRT at 24 h:\n",
    "enc[\"surv_oncrrt_post24\"] = (\n",
    "    enc[\"survived_to_start\"] &\n",
    "    enc[\"on_crrt_at_24h\"] &\n",
    "    ~enc[\"died_within_24h\"]\n",
    ")\n",
    "\n",
    "# survivors who have come off CRRT by 24 h:\n",
    "enc[\"surv_offcrrt_post24\"] = (\n",
    "    enc[\"survived_to_start\"] &\n",
    "    ~enc[\"on_crrt_at_24h\"] &\n",
    "    ~enc[\"died_within_24h\"]\n",
    ")\n",
    "\n",
    "# died during the first 24 h on/off CRRT\n",
    "enc[\"non_surv_oncrrt_post24\"]  = enc[\"died_within_24h\"] & enc[\"on_crrt_at_24h\"]\n",
    "enc[\"non_surv_offcrrt_post24\"] = enc[\"died_within_24h\"] & ~enc[\"on_crrt_at_24h\"]\n",
    "# died between 24 h and 72 h (exclusive of the first 24 h)\n",
    "enc[\"died_24_72h\"] = enc[\"died_within_72h\"] & ~enc[\"died_within_24h\"]\n",
    "\n",
    "\n",
    "# Survivors at 72h still on CRRT\n",
    "enc[\"surv_oncrrt_post72\"] = (\n",
    "    ~enc[\"died_within_24h\"] &      # Survived first 24h\n",
    "    ~enc[\"died_within_72h\"] &      # Still alive at 72h\n",
    "    enc[\"on_crrt_at_72h\"]          # On CRRT at 72h\n",
    ")\n",
    "\n",
    "# Survivors at 72h off CRRT\n",
    "enc[\"surv_offcrrt_post72\"] = (\n",
    "    ~enc[\"died_within_24h\"] &      # Survived first 24h\n",
    "    ~enc[\"died_within_72h\"] &      # Still alive at 72h\n",
    "    ~enc[\"on_crrt_at_72h\"]         # Off CRRT at 72h\n",
    ")\n",
    "\n",
    "# Non-survivors (died 24-72h) still on CRRT\n",
    "enc[\"non_surv_oncrrt_post72\"] = (\n",
    "    ~enc[\"died_within_24h\"] &      # Survived first 24h\n",
    "    enc[\"died_within_72h\"] &       # Died by 72h\n",
    "    enc[\"on_crrt_at_72h\"]          # On CRRT at time of death/72h\n",
    ")\n",
    "\n",
    "# Non-survivors (died 24-72h) off CRRT\n",
    "enc[\"non_surv_offcrrt_post72\"] = (\n",
    "    ~enc[\"died_within_24h\"] &      # Survived first 24h\n",
    "    enc[\"died_within_72h\"] &       # Died by 72h\n",
    "    ~enc[\"on_crrt_at_72h\"]         # Off CRRT at time of death/72h\n",
    ")\n",
    "\n",
    "# nobody died before CRRT start\n",
    "if enc[\"died_pre24h\"].any():\n",
    "    logger.error(f\"{enc['died_pre24h'].sum()} patients died pre-CRRT!\")\n",
    "    bad_pre = enc[enc[\"death_dttm_proxy\"] < enc[\"first_crrt_time\"]].copy()\n",
    "    bad_pre[\"delta\"] = bad_pre[\"first_crrt_time\"] - bad_pre[\"death_dttm_proxy\"]\n",
    "    print(bad_pre[[\"encounter_block\",\"death_dttm_proxy\",\"first_crrt_time\",\"delta\"]].head(20))\n",
    "else:\n",
    "    logger.info(\"No patients died in the 24 h before CRRT start.\")\n",
    "\n",
    "# ensure post-24 h decedents aren’t in the post-72 h cohorts\n",
    "bad = enc.loc[enc[\"died_within_24h\"], \"surv_oncrrt_post72\"].any() or \\\n",
    "      enc.loc[enc[\"died_within_24h\"], \"surv_offcrrt_post72\"].any()\n",
    "if bad:\n",
    "    logger.error(\"Some 24h decedents slipped into 72h survivor groups!\")\n",
    "else:\n",
    "    logger.info(\"Post-72h survivor groups correctly exclude all 24h decedents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive sanity checks for 72h cohorts\n",
    "\n",
    "logger.info(\"=== SANITY CHECKS FOR 72H COHORTS ===\")\n",
    "\n",
    "# Basic counts\n",
    "n_total = len(enc)\n",
    "n_died_before_24h = enc[\"died_within_24h\"].sum()\n",
    "n_survived_24h = (~enc[\"died_within_24h\"]).sum()\n",
    "n_died_24_72h = ((~enc[\"died_within_24h\"]) & enc[\"died_within_72h\"]).sum()\n",
    "n_survived_72h = ((~enc[\"died_within_24h\"]) & (~enc[\"died_within_72h\"])).sum()\n",
    "\n",
    "logger.info(\"\\n1. OVERALL POPULATION\")\n",
    "logger.info(f\"   Total patients: {n_total}\")\n",
    "logger.info(f\"   Died within 24h: {n_died_before_24h}\")\n",
    "logger.info(f\"   Survived to 24h: {n_survived_24h}\")\n",
    "logger.info(f\"   - Died between 24-72h: {n_died_24_72h}\")\n",
    "logger.info(f\"   - Survived to 72h: {n_survived_72h}\")\n",
    "logger.info(f\"   Check: {n_died_24_72h + n_survived_72h} should equal {n_survived_24h}\")\n",
    "\n",
    "# 72h cohort counts\n",
    "n_surv_on_72 = enc[\"surv_oncrrt_post72\"].sum()\n",
    "n_surv_off_72 = enc[\"surv_offcrrt_post72\"].sum()\n",
    "n_non_surv_on_72 = enc[\"non_surv_oncrrt_post72\"].sum()\n",
    "n_non_surv_off_72 = enc[\"non_surv_offcrrt_post72\"].sum()\n",
    "n_total_72h_cohorts = n_surv_on_72 + n_surv_off_72 + n_non_surv_on_72 + n_non_surv_off_72\n",
    "\n",
    "logger.info(\"\\n2. 72H COHORT BREAKDOWN\")\n",
    "logger.info(f\"   Survivors at 72h:\")\n",
    "logger.info(f\"   - Still on CRRT: {n_surv_on_72}\")\n",
    "logger.info(f\"   - Off CRRT: {n_surv_off_72}\")\n",
    "logger.info(f\"   - Total survivors: {n_surv_on_72 + n_surv_off_72}\")\n",
    "logger.info(f\"   Non-survivors (died 24-72h):\")\n",
    "logger.info(f\"   - On CRRT at death: {n_non_surv_on_72}\")\n",
    "logger.info(f\"   - Off CRRT at death: {n_non_surv_off_72}\")\n",
    "logger.info(f\"   - Total non-survivors: {n_non_surv_on_72 + n_non_surv_off_72}\")\n",
    "logger.info(f\"   TOTAL in 72h cohorts: {n_total_72h_cohorts}\")\n",
    "\n",
    "# Key check: 72h cohorts should equal all 24h survivors\n",
    "logger.info(\"\\n3. PRIMARY CHECK\")\n",
    "logger.info(f\"   Survived to 24h: {n_survived_24h}\")\n",
    "logger.info(f\"   Total in 72h cohorts: {n_total_72h_cohorts}\")\n",
    "if n_survived_24h == n_total_72h_cohorts:\n",
    "    logger.info(\"   ✓ PASS: All 24h survivors accounted for in 72h cohorts\")\n",
    "else:\n",
    "    logger.warning(f\"   ✗ FAIL: Missing {n_survived_24h - n_total_72h_cohorts} patients\")\n",
    "\n",
    "# Check survivors at 72h partition correctly\n",
    "logger.info(\"\\n4. SURVIVOR PARTITION CHECK\")\n",
    "logger.info(f\"   Expected survivors at 72h: {n_survived_72h}\")\n",
    "logger.info(f\"   Actual (on + off CRRT): {n_surv_on_72 + n_surv_off_72}\")\n",
    "if n_survived_72h == n_surv_on_72 + n_surv_off_72:\n",
    "    logger.info(\"   ✓ PASS: Survivors correctly partitioned by CRRT status\")\n",
    "else:\n",
    "    logger.warning(\"   ✗ FAIL: Survivor counts don't match\")\n",
    "\n",
    "# Check non-survivors partition correctly\n",
    "logger.info(\"\\n5. NON-SURVIVOR PARTITION CHECK\")\n",
    "logger.info(f\"   Expected deaths 24-72h: {n_died_24_72h}\")\n",
    "logger.info(f\"   Actual (on + off CRRT): {n_non_surv_on_72 + n_non_surv_off_72}\")\n",
    "if n_died_24_72h == n_non_surv_on_72 + n_non_surv_off_72:\n",
    "    logger.info(\"   ✓ PASS: Non-survivors correctly partitioned by CRRT status\")\n",
    "else:\n",
    "    logger.warning(\"   ✗ FAIL: Non-survivor counts don't match\")\n",
    "\n",
    "# Check for overlaps (no patient should be in multiple cohorts)\n",
    "logger.info(\"\\n6. MUTUAL EXCLUSIVITY CHECKS\")\n",
    "overlap_survivor = enc[\"surv_oncrrt_post72\"] & enc[\"surv_offcrrt_post72\"]\n",
    "overlap_non_surv = enc[\"non_surv_oncrrt_post72\"] & enc[\"non_surv_offcrrt_post72\"]\n",
    "overlap_surv_non_surv = (enc[\"surv_oncrrt_post72\"] | enc[\"surv_offcrrt_post72\"]) & \\\n",
    "                        (enc[\"non_surv_oncrrt_post72\"] | enc[\"non_surv_offcrrt_post72\"])\n",
    "\n",
    "logger.info(f\"   Overlap between survivor cohorts: {overlap_survivor.sum()}\")\n",
    "logger.info(f\"   Overlap between non-survivor cohorts: {overlap_non_surv.sum()}\")\n",
    "logger.info(f\"   Overlap between survivors and non-survivors: {overlap_surv_non_surv.sum()}\")\n",
    "if not any([overlap_survivor.any(), overlap_non_surv.any(), overlap_surv_non_surv.any()]):\n",
    "    logger.info(\"   ✓ PASS: All cohorts are mutually exclusive\")\n",
    "else:\n",
    "    logger.warning(\"   ✗ FAIL: Found overlapping patients\")\n",
    "\n",
    "# Verify CRRT status logic\n",
    "logger.info(\"\\n7. CRRT STATUS CONSISTENCY\")\n",
    "# For survivors on CRRT at 72h\n",
    "on_crrt_72h_survivors = enc[enc[\"surv_oncrrt_post72\"]]\n",
    "logger.info(f\"   Survivors 'on CRRT' at 72h: {len(on_crrt_72h_survivors)}\")\n",
    "logger.info(f\"   - Actually on CRRT: {on_crrt_72h_survivors['on_crrt_at_72h'].sum()}\")\n",
    "logger.info(f\"   - Actually off CRRT: {(~on_crrt_72h_survivors['on_crrt_at_72h']).sum()}\")\n",
    "\n",
    "# For survivors off CRRT at 72h\n",
    "off_crrt_72h_survivors = enc[enc[\"surv_offcrrt_post72\"]]\n",
    "logger.info(f\"   Survivors 'off CRRT' at 72h: {len(off_crrt_72h_survivors)}\")\n",
    "logger.info(f\"   - Actually on CRRT: {off_crrt_72h_survivors['on_crrt_at_72h'].sum()}\")\n",
    "logger.info(f\"   - Actually off CRRT: {(~off_crrt_72h_survivors['on_crrt_at_72h']).sum()}\")\n",
    "\n",
    "# Summary\n",
    "logger.info(\"\\n8. FINAL SUMMARY\")\n",
    "logger.info(f\"   24h analysis includes: {n_total} patients\")\n",
    "logger.info(f\"   72h analysis includes: {n_survived_24h} patients (excluded {n_died_before_24h} early deaths)\")\n",
    "logger.info(f\"   72h breakdown:\")\n",
    "logger.info(f\"   - {n_survived_72h} survived to 72h ({n_surv_on_72} on CRRT, {n_surv_off_72} off)\")\n",
    "logger.info(f\"   - {n_died_24_72h} died 24-72h ({n_non_surv_on_72} on CRRT, {n_non_surv_off_72} off)\")\n",
    "\n",
    "# If there are discrepancies, identify missing patients\n",
    "if n_survived_24h != n_total_72h_cohorts:\n",
    "    logger.warning(\"\\n9. MISSING PATIENT ANALYSIS\")\n",
    "    in_72h = enc[\"surv_oncrrt_post72\"] | enc[\"surv_offcrrt_post72\"] | \\\n",
    "            enc[\"non_surv_oncrrt_post72\"] | enc[\"non_surv_offcrrt_post72\"]\n",
    "    survived_24h_mask = ~enc[\"died_within_24h\"]\n",
    "    missing_mask = survived_24h_mask & ~in_72h\n",
    "    missing_patients = enc[missing_mask]\n",
    "\n",
    "    logger.warning(f\"   Missing patients: {len(missing_patients)}\")\n",
    "    logger.warning(f\"   - Died within 72h: {missing_patients['died_within_72h'].sum()}\")\n",
    "    logger.warning(f\"   - Still alive at 72h: {(~missing_patients['died_within_72h']).sum()}\")\n",
    "    logger.warning(f\"   - On CRRT at 72h: {missing_patients['on_crrt_at_72h'].sum()}\")\n",
    "    logger.warning(f\"   - Off CRRT at 72h: {(~missing_patients['on_crrt_at_72h']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to create subcohort tables\n",
    "def create_subcohort_table(window_df, enc_df, subcohort_flag, window_name, subcohort_name):\n",
    "    \"\"\"\n",
    "    Filter a window summary to only include encounters in a specific subcohort\n",
    "    \"\"\"\n",
    "    # Get encounters in this subcohort\n",
    "    subcohort_encounters = enc_df[enc_df[subcohort_flag]]['encounter_block'].tolist()\n",
    "\n",
    "    # Get unique patients (since encounter_block is already unique per admission)\n",
    "    n_admissions = len(subcohort_encounters)\n",
    "\n",
    "    # Filter window summary to only these encounters\n",
    "    subcohort_data = window_df[window_df.index.isin(subcohort_encounters)]\n",
    "\n",
    "    # Create TableOne using the existing function\n",
    "    table = make_tableone(subcohort_data, f\"{window_name}_{subcohort_name}\")\n",
    "\n",
    "    return table, n_admissions\n",
    "\n",
    "# Step 2: Define all subcohorts\n",
    "subcohorts = [\n",
    "    # Pre-24h (only one subcohort - all alive at CRRT start)\n",
    "    {'window_df': pre24, 'flag': 'survived_to_start', 'window': 'Pre-24h', 'name': 'All'},\n",
    "\n",
    "    # Post-24h subcohorts\n",
    "    {'window_df': post24, 'flag': 'surv_oncrrt_post24', 'window': 'Post-24h', 'name': 'Survivors_on_CRRT'},\n",
    "    {'window_df': post24, 'flag': 'surv_offcrrt_post24', 'window': 'Post-24h', 'name': 'Survivors_off_CRRT'},\n",
    "    {'window_df': post24, 'flag': 'non_surv_oncrrt_post24', 'window': 'Post-24h', 'name': 'Non-survivors_on_CRRT'},\n",
    "    {'window_df': post24, 'flag': 'non_surv_offcrrt_post24', 'window': 'Post-24h', 'name': 'Non-survivors_off_CRRT'},\n",
    "\n",
    "    # Post-72h subcohorts\n",
    "    {'window_df': post72, 'flag': 'surv_oncrrt_post72', 'window': 'Post-72h', 'name': 'Survivors_on_CRRT'},\n",
    "    {'window_df': post72, 'flag': 'surv_offcrrt_post72', 'window': 'Post-72h', 'name': 'Survivors_off_CRRT'},\n",
    "    {'window_df': post72, 'flag': 'non_surv_oncrrt_post72', 'window': 'Post-72h', 'name': 'Non-survivors_on_CRRT'},\n",
    "    {'window_df': post72, 'flag': 'non_surv_offcrrt_post72', 'window': 'Post-72h', 'name': 'Non-survivors_off_CRRT'},\n",
    "]\n",
    "\n",
    "# Step 3: Generate tables for each subcohort\n",
    "subcohort_tables = {}\n",
    "admission_counts = {}\n",
    "\n",
    "for sc in subcohorts:\n",
    "    table, n_admissions = create_subcohort_table(\n",
    "        sc['window_df'],\n",
    "        enc,\n",
    "        sc['flag'],\n",
    "        sc['window'],\n",
    "        sc['name']\n",
    "    )\n",
    "    key = f\"{sc['window']}_{sc['name']}\"\n",
    "    subcohort_tables[key] = table\n",
    "    admission_counts[key] = n_admissions\n",
    "    print(f\"Created table for {key}: {n_admissions} admissions\")\n",
    "\n",
    "# Step 4: Extract and combine all tables\n",
    "combined_dfs = []\n",
    "column_names = []\n",
    "\n",
    "for name, table in subcohort_tables.items():\n",
    "    if table is not None:\n",
    "        # Extract the TableOne dataframe and keep index as is\n",
    "        df = pd.DataFrame(table.tableone)\n",
    "        \n",
    "        # Get the \"Overall\" column (usually the second column after \"Missing\")\n",
    "        if \"Overall\" in df.columns:\n",
    "            summary_col = df[[\"Overall\"]]\n",
    "        else:\n",
    "            # If no \"Overall\" column, take the second column (index 1)\n",
    "            summary_col = df.iloc[:, [1]]\n",
    "            \n",
    "        # Rename the column to the subcohort name\n",
    "        col_name = name.replace(\"_\", \" \")\n",
    "        summary_col.columns = [col_name]\n",
    "        \n",
    "        combined_dfs.append(summary_col)\n",
    "        column_names.append(col_name)\n",
    "\n",
    "# Combine all dataframes\n",
    "if combined_dfs:\n",
    "    # Start with the first dataframe\n",
    "    combined_summary = combined_dfs[0]\n",
    "    \n",
    "    # Join the rest\n",
    "    for df in combined_dfs[1:]:\n",
    "        combined_summary = combined_summary.join(df, how='outer')\n",
    "\n",
    "    # Add admission counts as a new row\n",
    "    admission_row = pd.DataFrame(\n",
    "        [[admission_counts.get(col.replace(\" \", \"_\"), \"\") for col in combined_summary.columns]],\n",
    "        columns=combined_summary.columns,\n",
    "        index=[\"Admissions n\"]\n",
    "    )\n",
    "    \n",
    "    # Add patient counts row\n",
    "    patient_counts_row = []\n",
    "    for sc in subcohorts:\n",
    "        key = f\"{sc['window']}_{sc['name']}\"\n",
    "        col_name = key.replace(\"_\", \" \")\n",
    "        if col_name in combined_summary.columns:\n",
    "            subcohort_encounters = enc[enc[sc['flag']]]['encounter_block'].tolist()\n",
    "            unique_patients = all_ids_df[all_ids_df['encounter_block'].isin(subcohort_encounters)]['patient_id'].nunique()\n",
    "            patient_counts_row.append(str(unique_patients))\n",
    "\n",
    "    patient_row = pd.DataFrame(\n",
    "        [patient_counts_row],\n",
    "        columns=combined_summary.columns,\n",
    "        index=[\"Patients n\"]\n",
    "    )\n",
    "\n",
    "    # Combine with main table\n",
    "    combined_summary = pd.concat([admission_row, patient_row, combined_summary])\n",
    "\n",
    "    # Save the final combined table\n",
    "    output_file = f\"{output_folder}/intermediate/table1_all_subcohorts_{pyCLIF.helper['site_name'].lower()}.csv\"\n",
    "    combined_summary.to_csv(output_file)\n",
    "    print(f\"\\nFinal table saved to: {output_file}\")\n",
    "\n",
    "    # Display summary\n",
    "    print(f\"\\nTotal rows in combined table: {len(combined_summary)}\")\n",
    "\n",
    "    # Save individual subcohort tables\n",
    "    for name, table in subcohort_tables.items():\n",
    "        if table is not None:\n",
    "            individual_file = f\"{output_folder}/intermediate/table1_{name}.csv\"\n",
    "            table.to_csv(individual_file)\n",
    "else:\n",
    "    print(\"No tables were created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00fbc65",
   "metadata": {},
   "source": [
    "We will have to combine non-survivors post 24 into one group, and not divide by crrt status, to avoid the possibility of sharing cohort n < 5. Same for non surv post-72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790eb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE SIMPLIFIED FLAGS FOR COMBINED NON-SURVIVORS\n",
    "# ============================================================================\n",
    "\n",
    "# Create simplified flags that combine non-survivors regardless of CRRT status\n",
    "enc['non_surv_post24'] = enc['died_within_24h']  # All 24h non-survivors (on/off CRRT combined)\n",
    "enc['non_surv_post72'] = enc['died_24_72h']      # All 72h non-survivors (on/off CRRT combined)\n",
    "\n",
    "print(\"Created simplified flags:\")\n",
    "print(f\"  non_surv_post24: {enc['non_surv_post24'].sum()} patients\")\n",
    "print(f\"  non_surv_post72: {enc['non_surv_post72'].sum()} patients\")\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLIFIED TABLE 1 GENERATION WITH NEW COHORT STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "# Step 2: Define simplified subcohorts using the new flags\n",
    "simplified_subcohorts = [\n",
    "    # Pre-24h (only one subcohort - all alive at CRRT start)\n",
    "    {'window_df': pre24, 'flag': 'survived_to_start', 'window': 'Pre-24h', 'name': 'All'},\n",
    "\n",
    "    # Post-24h subcohorts (simplified)\n",
    "    {'window_df': post24, 'flag': 'surv_oncrrt_post24', 'window': 'Post-24h', 'name': 'Survivors_on_CRRT'},\n",
    "    {'window_df': post24, 'flag': 'surv_offcrrt_post24', 'window': 'Post-24h', 'name': 'Survivors_off_CRRT'},\n",
    "    {'window_df': post24, 'flag': 'non_surv_post24', 'window': 'Post-24h', 'name': 'Non_survivors_combined'},\n",
    "\n",
    "    # Post-72h subcohorts (simplified)  \n",
    "    {'window_df': post72, 'flag': 'surv_oncrrt_post72', 'window': 'Post-72h', 'name': 'Survivors_on_CRRT'},\n",
    "    {'window_df': post72, 'flag': 'surv_offcrrt_post72', 'window': 'Post-72h', 'name': 'Survivors_off_CRRT'},\n",
    "    {'window_df': post72, 'flag': 'non_surv_post72', 'window': 'Post-72h', 'name': 'Non_survivors_combined'},\n",
    "]\n",
    "\n",
    "# Step 3: Generate tables for each simplified subcohort using existing function\n",
    "simplified_subcohort_tables = {}\n",
    "simplified_admission_counts = {}\n",
    "\n",
    "print(\"\\nGenerating simplified subcohort tables:\")\n",
    "for sc in simplified_subcohorts:\n",
    "    table, n_admissions = create_subcohort_table(\n",
    "        sc['window_df'],\n",
    "        enc,\n",
    "        sc['flag'],\n",
    "        sc['window'],\n",
    "        sc['name']\n",
    "    )\n",
    "    key = f\"{sc['window']}_{sc['name']}\"\n",
    "    simplified_subcohort_tables[key] = table\n",
    "    simplified_admission_counts[key] = n_admissions\n",
    "    print(f\"  Created table for {key}: {n_admissions} admissions\")\n",
    "\n",
    "# Step 4: Extract and combine simplified tables using existing logic\n",
    "simplified_combined_dfs = []\n",
    "\n",
    "for name, table in simplified_subcohort_tables.items():\n",
    "    if table is not None:\n",
    "        # Extract the TableOne dataframe\n",
    "        df = pd.DataFrame(table.tableone)\n",
    "\n",
    "        # Get the \"Overall\" column\n",
    "        if \"Overall\" in df.columns:\n",
    "            summary_col = df[[\"Overall\"]]\n",
    "        else:\n",
    "            summary_col = df.iloc[:, [1]]\n",
    "\n",
    "        # Rename the column to the subcohort name\n",
    "        col_name = name.replace(\"_\", \" \")\n",
    "        summary_col.columns = [col_name]\n",
    "\n",
    "        simplified_combined_dfs.append(summary_col)\n",
    "\n",
    "# Combine all simplified dataframes\n",
    "if simplified_combined_dfs:\n",
    "    # Start with the first dataframe\n",
    "    simplified_combined_summary = simplified_combined_dfs[0]\n",
    "\n",
    "    # Join the rest\n",
    "    for df in simplified_combined_dfs[1:]:\n",
    "        simplified_combined_summary = simplified_combined_summary.join(df, how='outer')\n",
    "\n",
    "    # Add admission counts as a new row\n",
    "    simplified_admission_row = pd.DataFrame(\n",
    "        [[simplified_admission_counts.get(col.replace(\" \", \"_\"), \"\") for col in simplified_combined_summary.columns]],\n",
    "        columns=simplified_combined_summary.columns,\n",
    "        index=[\"Admissions n\"]\n",
    "    )\n",
    "\n",
    "    # Add patient counts row\n",
    "    simplified_patient_counts_row = []\n",
    "    for sc in simplified_subcohorts:\n",
    "        key = f\"{sc['window']}_{sc['name']}\"\n",
    "        col_name = key.replace(\"_\", \" \")\n",
    "        if col_name in simplified_combined_summary.columns:\n",
    "            subcohort_encounters = enc[enc[sc['flag']]]['encounter_block'].tolist()\n",
    "            unique_patients = all_ids_df[all_ids_df['encounter_block'].isin(subcohort_encounters)]['patient_id'].nunique()\n",
    "            simplified_patient_counts_row.append(str(unique_patients))\n",
    "\n",
    "    simplified_patient_row = pd.DataFrame(\n",
    "        [simplified_patient_counts_row],\n",
    "        columns=simplified_combined_summary.columns,\n",
    "        index=[\"Patients n\"]\n",
    "    )\n",
    "\n",
    "    # Combine with main table\n",
    "    final_simplified_table = pd.concat([simplified_admission_row, simplified_patient_row, simplified_combined_summary])\n",
    "\n",
    "    # Save the simplified table\n",
    "    simplified_output_file = f\"{output_folder}/final/table1_subgroups2_{pyCLIF.helper['site_name'].lower()}.csv\"\n",
    "    final_simplified_table.to_csv(simplified_output_file)\n",
    "    print(f\"\\n✓ Simplified Table 1 saved to: {simplified_output_file}\")\n",
    "\n",
    "    # Also save as generic filename for multi-site compatibility\n",
    "    generic_simplified_file = f\"{output_folder}/final/table1_subgroups2.csv\"\n",
    "    final_simplified_table.to_csv(generic_simplified_file)\n",
    "    print(f\"✓ Also saved as: {generic_simplified_file}\")\n",
    "\n",
    "    # Display the new simplified cohort structure\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SIMPLIFIED TABLE 1 COHORT STRUCTURE\")\n",
    "    print(\"=\"*60)\n",
    "    for sc in simplified_subcohorts:\n",
    "        flag_count = enc[sc['flag']].sum()\n",
    "        print(f\"{sc['window']} {sc['name'].replace('_', ' ')}: n={flag_count}\")\n",
    "\n",
    "    print(f\"\\nTotal rows in simplified table: {len(final_simplified_table)}\")\n",
    "    print(\"Table columns:\", list(final_simplified_table.columns))\n",
    "\n",
    "else:\n",
    "    print(\"No simplified tables were created!\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SIMPLIFIED TABLE 1 GENERATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c3d1f",
   "metadata": {},
   "source": [
    "# (C) Summary by location type\n",
    "\n",
    "This summarization looks at the first 72 hours of CRRT therapy for all patients in the cohort, including those didn't survive the first 24 hours of CRRT therapy. \n",
    "\n",
    "Questions:\n",
    "\n",
    "1. How to handle ICU transfers? If someone started in Medical ICU but moved to Surgical ICU, which ICU do they represent?\n",
    "    * Current code uses the \"last\" location during the 72h window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE 2: Characteristics by Location Type at 72 Hours Post-CRRT\n",
    "# ============================================================================\n",
    "\n",
    "def create_table2_by_location():\n",
    "    \"\"\"Create Table 2 stratified by ICU location type at 72 hours\"\"\"\n",
    "\n",
    "    print(\"Creating Table 2: Characteristics by Location Type at 72h post-CRRT...\")\n",
    "\n",
    "    # Use the existing post72 window summary which already has location_type\n",
    "    # This represents the location during the first 72 hours of CRRT\n",
    "    location_data = post72.reset_index()\n",
    "\n",
    "    # Get unique location types\n",
    "    location_types = location_data['location_type'].dropna().unique()\n",
    "    print(f\"Location types found: {location_types}\")\n",
    "\n",
    "    # Create TableOne for each location type\n",
    "    location_tables = {}\n",
    "    location_counts = {}\n",
    "\n",
    "    for loc_type in location_types:\n",
    "        # Filter to this location type\n",
    "        subset_data = location_data[location_data['location_type'] == loc_type]\n",
    "\n",
    "        if len(subset_data) > 5:  # Minimum sample size\n",
    "            # Set index back for TableOne\n",
    "            subset_data = subset_data.set_index('encounter_block')\n",
    "\n",
    "            # Create TableOne using existing function\n",
    "            table = make_tableone(subset_data, f\"Location_{loc_type}\")\n",
    "            location_tables[loc_type] = table\n",
    "            location_counts[loc_type] = len(subset_data)\n",
    "            print(f\"  {loc_type}: {len(subset_data)} patients\")\n",
    "        else:\n",
    "            print(f\"  {loc_type}: {len(subset_data)} patients (too few - excluded)\")\n",
    "\n",
    "    return location_tables, location_counts\n",
    "\n",
    "# ============================================================================\n",
    "# COMBINE INTO TEMPLATE FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def create_table2_combined():\n",
    "    \"\"\"Combine location-stratified tables into the template format\"\"\"\n",
    "\n",
    "    location_tables, location_counts = create_table2_by_location()\n",
    "\n",
    "    if not location_tables:\n",
    "        print(\"No location tables created!\")\n",
    "        return None\n",
    "\n",
    "    # Extract data from each TableOne\n",
    "    combined_dfs = []\n",
    "\n",
    "    # Define the column order based on template\n",
    "    template_columns = [\n",
    "        'cardiac_icu', 'cvicu_icu', 'general_icu',\n",
    "        'medical_icu', 'mixed_neuro_icu', 'surgical_icu'\n",
    "    ]\n",
    "\n",
    "    for loc_type in template_columns:\n",
    "        if loc_type in location_tables:\n",
    "            table = location_tables[loc_type]\n",
    "            df = pd.DataFrame(table.tableone)\n",
    "\n",
    "            # Get the \"Overall\" column (second column)\n",
    "            if \"Overall\" in df.columns:\n",
    "                summary_col = df[[\"Overall\"]]\n",
    "            else:\n",
    "                summary_col = df.iloc[:, [1]]\n",
    "\n",
    "            summary_col.columns = [loc_type]\n",
    "            combined_dfs.append(summary_col)\n",
    "        else:\n",
    "            # Create empty column for missing location types\n",
    "            if combined_dfs:  # Use existing structure\n",
    "                empty_col = pd.DataFrame(\n",
    "                    index=combined_dfs[0].index,\n",
    "                    columns=[loc_type],\n",
    "                    data=\"\"\n",
    "                )\n",
    "                combined_dfs.append(empty_col)\n",
    "\n",
    "    # Combine all location columns\n",
    "    if combined_dfs:\n",
    "        combined_table = combined_dfs[0]\n",
    "        for df in combined_dfs[1:]:\n",
    "            combined_table = combined_table.join(df, how='outer')\n",
    "\n",
    "        # Add header row for template format\n",
    "        header_row = pd.DataFrame(\n",
    "            [[\"location_type at 72 hours post CRRT\"] + [\"\"] * (len(combined_table.columns) - 1)],\n",
    "            columns=combined_table.columns,\n",
    "            index=[\"Header\"]\n",
    "        )\n",
    "\n",
    "        # Add count row\n",
    "        count_data = []\n",
    "        for col in combined_table.columns:\n",
    "            if col in location_counts:\n",
    "                count_data.append(str(location_counts[col]))\n",
    "            else:\n",
    "                count_data.append(\"\")\n",
    "\n",
    "        count_row = pd.DataFrame(\n",
    "            [count_data],\n",
    "            columns=combined_table.columns,\n",
    "            index=[\"n\"]\n",
    "        )\n",
    "\n",
    "        # Combine all parts\n",
    "        final_table = pd.concat([header_row, combined_table, count_row])\n",
    "\n",
    "        # Save the table\n",
    "        output_file = f\"{output_folder}/final/table2_by_location_type.csv\"\n",
    "        final_table.to_csv(output_file)\n",
    "        print(f\"Table 2 saved to: {output_file}\")\n",
    "\n",
    "        # Also create a cleaner version matching the exact template\n",
    "        create_template_formatted_table2(combined_table, location_counts)\n",
    "\n",
    "        return final_table\n",
    "\n",
    "    return None\n",
    "\n",
    "def create_template_formatted_table2(data_table, counts):\n",
    "    \"\"\"Create exact template format matching the reference\"\"\"\n",
    "\n",
    "    # Create empty template structure\n",
    "    template_cols = ['cardiac_icu', 'cvicu_icu', 'general_icu',\n",
    "                    'medical_icu', 'mixed_neuro_icu', 'surgical_icu']\n",
    "\n",
    "    # Initialize with level_0, level_1 columns like template\n",
    "    template_df = pd.DataFrame(columns=['level_0', 'level_1'] + template_cols)\n",
    "\n",
    "    # Add header row\n",
    "    header_row = {\n",
    "        'level_0': '',\n",
    "        'level_1': '',\n",
    "        **{col: col for col in template_cols}\n",
    "    }\n",
    "    template_df = pd.concat([template_df, pd.DataFrame([header_row])], ignore_index=True)\n",
    "\n",
    "    # Add count row\n",
    "    count_row = {\n",
    "        'level_0': 'n',\n",
    "        'level_1': '',\n",
    "        **{col: counts.get(col, '') for col in template_cols}\n",
    "    }\n",
    "    template_df = pd.concat([template_df, pd.DataFrame([count_row])], ignore_index=True)\n",
    "\n",
    "    # Process each row from the data table\n",
    "    for idx in data_table.index:\n",
    "        if isinstance(idx, tuple) and len(idx) >= 2:\n",
    "            level_0, level_1 = idx[0], idx[1]\n",
    "        else:\n",
    "            level_0, level_1 = str(idx), ''\n",
    "\n",
    "        # Create row data\n",
    "        row_data = {\n",
    "            'level_0': level_0,\n",
    "            'level_1': level_1\n",
    "        }\n",
    "\n",
    "        # Add data for each location type\n",
    "        for col in template_cols:\n",
    "            if col in data_table.columns:\n",
    "                row_data[col] = data_table.loc[idx, col]\n",
    "            else:\n",
    "                row_data[col] = ''\n",
    "\n",
    "        template_df = pd.concat([template_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "    # Save template-formatted version\n",
    "    template_file = f\"{output_folder}/final/table2_final.csv\"\n",
    "    template_df.to_csv(template_file, index=False)\n",
    "    print(f\"Template-formatted Table 2 saved to: {template_file}\")\n",
    "\n",
    "    return template_df\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING TABLE 2: CHARACTERISTICS BY LOCATION TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "table2_result = create_table2_combined()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TABLE 2 GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files created:\")\n",
    "print(\"  - table2_by_location_type.csv\")\n",
    "print(\"  - table2_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b035db7",
   "metadata": {},
   "source": [
    "### C.1 Summary by location type by stats\n",
    "\n",
    "1. Uses ANOVA (f_oneway) for continuous variables to test if means differ across ICU types\n",
    "2. Uses Chi-square test for categorical variables to test if distributions differ across ICU types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLE 2: Characteristics by Location Type with Statistical Tests\n",
    "# ============================================================================\n",
    "\n",
    "def create_table2_with_stats():\n",
    "    \"\"\"Create Table 2 with p-values comparing across ICU locations\"\"\"\n",
    "\n",
    "    print(\"Creating Table 2 with statistical comparisons...\")\n",
    "\n",
    "    # Use the existing post72 window summary\n",
    "    location_data = post72.reset_index()\n",
    "\n",
    "    # Remove rows with missing location_type\n",
    "    location_data = location_data.dropna(subset=['location_type'])\n",
    "\n",
    "    # Get unique location types\n",
    "    location_types = sorted(location_data['location_type'].unique())\n",
    "    print(f\"Location types found: {location_types}\")\n",
    "\n",
    "    # Define continuous and categorical variables - USE THE _median COLUMNS\n",
    "    continuous_vars = [\n",
    "        'age_at_admission',\n",
    "        'bicarbonate_median', 'bun_median', 'calcium_total_median',\n",
    "        'chloride_median', 'creatinine_median', 'glucose_serum_median', 'lactate_median',\n",
    "        'magnesium_median', 'sodium_median', 'potassium_median', 'ph_arterial_median',\n",
    "        'po2_arterial_median', 'fio2_set_median', 'peep_set_median',\n",
    "        'blood_flow_rate_median', 'dialysate_flow_rate_median', 'ultrafiltration_out_median',\n",
    "        'pre_filter_replacement_fluid_rate_median', 'post_filter_replacement_fluid_rate_median',\n",
    "        'angiotensin_median', 'epinephrine_median', 'dopamine_median', 'vasopressin_median',\n",
    "        'dobutamine_median', 'norepinephrine_median', 'phenylephrine_median',\n",
    "        'sofa_total', 'sofa_cv_97', 'sofa_resp', 'sofa_renal', 'sofa_liver', 'sofa_coag', 'sofa_cns'\n",
    "    ]\n",
    "\n",
    "    categorical_vars = [\n",
    "        'sex_category', 'race_category', 'ethnicity_category',\n",
    "        'device_category', 'died_within_window', 'last_crrt_mode',\n",
    "        'location_category'\n",
    "    ]\n",
    "\n",
    "    # Filter to available columns\n",
    "    continuous_vars = [v for v in continuous_vars if v in location_data.columns]\n",
    "    categorical_vars = [v for v in categorical_vars if v in location_data.columns]\n",
    "\n",
    "    print(f\"DEBUG: Found {len(continuous_vars)} continuous variables in data\")\n",
    "    print(f\"DEBUG: Found {len(categorical_vars)} categorical variables in data\")\n",
    "\n",
    "    # Calculate p-values\n",
    "    p_values = {}\n",
    "\n",
    "    # ANOVA for continuous variables\n",
    "    from scipy import stats\n",
    "    print(f\"\\nDEBUG: Testing ANOVA for continuous variables...\")\n",
    "    for var in continuous_vars:\n",
    "        groups = []\n",
    "        group_sizes = []\n",
    "        for loc_type in location_types:\n",
    "            group_data = location_data[location_data['location_type'] == loc_type][var].dropna()\n",
    "            if len(group_data) > 0:\n",
    "                groups.append(group_data)\n",
    "                group_sizes.append(len(group_data))\n",
    "\n",
    "        if len(groups) >= 2:  # Need at least 2 groups for comparison\n",
    "            try:\n",
    "                _, p_val = stats.f_oneway(*groups)\n",
    "                # Store p-value with base name (remove _median suffix) for matching with TableOne\n",
    "                base_var_name = var.replace('_median', '')\n",
    "                p_values[base_var_name] = p_val\n",
    "                print(f\"  {var} -> {base_var_name}: p = {p_val:.6f}\")\n",
    "            except Exception as e:\n",
    "                base_var_name = var.replace('_median', '')\n",
    "                p_values[base_var_name] = np.nan\n",
    "                print(f\"  {var} -> ERROR: {e}\")\n",
    "        else:\n",
    "            base_var_name = var.replace('_median', '')\n",
    "            p_values[base_var_name] = np.nan\n",
    "\n",
    "    # Chi-square for categorical variables (unchanged)\n",
    "    for var in categorical_vars:\n",
    "        try:\n",
    "            crosstab = pd.crosstab(location_data[var], location_data['location_type'])\n",
    "            if crosstab.shape[0] > 1 and crosstab.shape[1] > 1:\n",
    "                chi2, p_val, dof, expected = stats.chi2_contingency(crosstab)\n",
    "                p_values[var] = p_val\n",
    "                print(f\"  {var}: p = {p_val:.6f}\")\n",
    "            else:\n",
    "                p_values[var] = np.nan\n",
    "        except Exception as e:\n",
    "            p_values[var] = np.nan\n",
    "            print(f\"  {var}: ERROR: {e}\")\n",
    "\n",
    "    print(f\"\\nDEBUG: Total p-values calculated: {len([p for p in p_values.values() if pd.notna(p)])}\")\n",
    "\n",
    "    # Rest of the function remains the same...\n",
    "    # Create TableOne for each location type\n",
    "    location_tables = {}\n",
    "    location_counts = {}\n",
    "\n",
    "    for loc_type in location_types:\n",
    "        subset_data = location_data[location_data['location_type'] == loc_type]\n",
    "\n",
    "        if len(subset_data) > 5:  # Minimum sample size\n",
    "            subset_data = subset_data.set_index('encounter_block')\n",
    "            table = make_tableone(subset_data, f\"Location_{loc_type}\")\n",
    "            location_tables[loc_type] = table\n",
    "            location_counts[loc_type] = len(subset_data)\n",
    "            print(f\"  {loc_type}: {len(subset_data)} patients\")\n",
    "\n",
    "    # Combine tables with p-values\n",
    "    combined_table = combine_location_tables_with_pvalues(\n",
    "        location_tables, location_counts, p_values, continuous_vars, categorical_vars\n",
    "    )\n",
    "\n",
    "    return combined_table, p_values\n",
    "\n",
    "def combine_location_tables_with_pvalues(location_tables, location_counts, p_values,\n",
    "                                       continuous_vars, categorical_vars):\n",
    "    \"\"\"Combine location tables and add p-value column\"\"\"\n",
    "\n",
    "    if not location_tables:\n",
    "        return None\n",
    "\n",
    "    # Extract data from each TableOne\n",
    "    combined_dfs = []\n",
    "\n",
    "    # Define the column order based on template\n",
    "    template_columns = [\n",
    "        'cardiac_icu', 'cvicu_icu', 'general_icu',\n",
    "        'medical_icu', 'mixed_neuro_icu', 'surgical_icu'\n",
    "    ]\n",
    "\n",
    "    # First, collect all data\n",
    "    all_data = {}\n",
    "    for loc_type in template_columns:\n",
    "        if loc_type in location_tables:\n",
    "            table = location_tables[loc_type]\n",
    "            df = pd.DataFrame(table.tableone)\n",
    "\n",
    "            if \"Overall\" in df.columns:\n",
    "                all_data[loc_type] = df[\"Overall\"]\n",
    "            else:\n",
    "                all_data[loc_type] = df.iloc[:, 1]\n",
    "\n",
    "    if not all_data:\n",
    "        return None\n",
    "\n",
    "    # Create combined dataframe\n",
    "    combined_df = pd.DataFrame(all_data)\n",
    "\n",
    "    # DEBUG: Print what we have\n",
    "    print(\"\\nDEBUG: P-values available for:\")\n",
    "    for var, p in p_values.items():\n",
    "        if pd.notna(p):\n",
    "            print(f\"  {var}: {p:.4f}\")\n",
    "\n",
    "    print(\"\\nDEBUG: Table indices (first 20):\")\n",
    "    for i, idx in enumerate(combined_df.index[:20]):\n",
    "        print(f\"  {i}: {idx}\")\n",
    "\n",
    "    # Add p-values column with better matching\n",
    "    p_value_col = []\n",
    "    matched_vars = []\n",
    "    unmatched_vars = []\n",
    "\n",
    "    for idx in combined_df.index:\n",
    "        # Extract variable name from index\n",
    "        if isinstance(idx, tuple):\n",
    "            var_name = idx[0]\n",
    "        else:\n",
    "            var_name = str(idx)\n",
    "\n",
    "        # Clean variable name (remove suffixes like ', median [Q1,Q3]')\n",
    "        base_var = var_name.split(',')[0].strip()\n",
    "\n",
    "        # Find p-value\n",
    "        if base_var in p_values:\n",
    "            p_val = p_values[base_var]\n",
    "            matched_vars.append(base_var)\n",
    "            if pd.notna(p_val):\n",
    "                if p_val < 0.001:\n",
    "                    p_value_col.append(\"<0.001\")\n",
    "                else:\n",
    "                    p_value_col.append(f\"{p_val:.3f}\")\n",
    "            else:\n",
    "                p_value_col.append(\"\")\n",
    "        else:\n",
    "            p_value_col.append(\"\")\n",
    "            if base_var != 'n':  # Don't report 'n' as unmatched\n",
    "                unmatched_vars.append((base_var, var_name))\n",
    "\n",
    "    # DEBUG: Report matching results\n",
    "    print(f\"\\nDEBUG: Successfully matched {len(set(matched_vars))} variables\")\n",
    "    print(f\"DEBUG: Failed to match {len(unmatched_vars)} variables:\")\n",
    "    for base, full in unmatched_vars[:10]:  # Show first 10\n",
    "        print(f\"  Base: '{base}' from Full: '{full}'\")\n",
    "\n",
    "    combined_df['p_value'] = p_value_col\n",
    "\n",
    "    # Add count row\n",
    "    count_data = [location_counts.get(col, \"\") for col in template_columns if col in combined_df.columns]\n",
    "    count_data.append(\"\")  # Empty p-value for count row\n",
    "\n",
    "    count_row = pd.DataFrame(\n",
    "        [count_data],\n",
    "        columns=list(combined_df.columns),\n",
    "        index=[\"n\"]\n",
    "    )\n",
    "\n",
    "    # Combine all parts\n",
    "    final_table = pd.concat([combined_df, count_row])\n",
    "\n",
    "    # Save the table\n",
    "    output_file = f\"{output_folder}/final/table2_by_location_with_stats_{pyCLIF.helper['site_name'].lower()}.csv\"\n",
    "    final_table.to_csv(output_file)\n",
    "    print(f\"Table 2 with p-values saved to: {output_file}\")\n",
    "\n",
    "    # Also save p-values separately for reference\n",
    "    p_values_df = pd.DataFrame(list(p_values.items()), columns=['Variable', 'p_value'])\n",
    "    p_values_df = p_values_df.sort_values('p_value')\n",
    "    p_values_df.to_csv(f\"{output_folder}/final/table2_p_values.csv\", index=False)\n",
    "\n",
    "    return final_table\n",
    "\n",
    "# Run the analysis\n",
    "table2_with_stats, p_values_dict = create_table2_with_stats()\n",
    "\n",
    "# Display significant findings\n",
    "print(\"\\nStatistically significant differences (p < 0.05):\")\n",
    "for var, p_val in sorted(p_values_dict.items(), key=lambda x: x[1] if pd.notna(x[1]) else 1):\n",
    "    if pd.notna(p_val) and p_val < 0.05:\n",
    "        print(f\"  {var}: p = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31275d8c",
   "metadata": {},
   "source": [
    "# (D) CRRT summary by modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa273891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first settings for each patient\n",
    "first_settings = crrt_df.sort_values(['encounter_block', 'recorded_dttm']).groupby(['encounter_block', 'crrt_mode_category']).first()\n",
    "\n",
    "# Get numeric columns for analysis\n",
    "numeric_cols = [\n",
    "    'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate', \n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "\n",
    "# Calculate median and IQR of first settings by mode\n",
    "first_settings_summary = first_settings[numeric_cols].groupby('crrt_mode_category').agg([\n",
    "    'median',\n",
    "    lambda x: x.quantile(0.25),\n",
    "    lambda x: x.quantile(0.75)\n",
    "])\n",
    "\n",
    "first_settings_summary.columns = first_settings_summary.columns.map(\n",
    "    lambda x: f\"{x[0]}_{x[1]}\" if x[1] == 'median' \n",
    "    else f\"{x[0]}_q1\" if x[1] == '<lambda_0>'\n",
    "    else f\"{x[0]}_q3\"\n",
    ")\n",
    "\n",
    "# Calculate mean and std directly from all measurements\n",
    "mode_avg_settings = crrt_df.groupby('crrt_mode_category')[numeric_cols].agg(['mean', 'std'])\n",
    "mode_avg_settings.columns = mode_avg_settings.columns.map(lambda x: f\"{x[0]}_{x[1]}\")\n",
    "\n",
    "# Calculate patient-level summaries\n",
    "# First calculate patient averages\n",
    "patient_avg_settings = crrt_df.groupby(['encounter_block', 'crrt_mode_category'])[numeric_cols].mean()\n",
    "\n",
    "# Then get median and IQR of patient averages\n",
    "patient_summary = patient_avg_settings.groupby('crrt_mode_category').agg([\n",
    "    'median',\n",
    "    lambda x: x.quantile(0.25),\n",
    "    lambda x: x.quantile(0.75)\n",
    "])\n",
    "\n",
    "patient_summary.columns = patient_summary.columns.map(\n",
    "    lambda x: f\"{x[0]}_patient_{x[1]}\" if x[1] == 'median'\n",
    "    else f\"{x[0]}_patient_q1\" if x[1] == '<lambda_0>'\n",
    "    else f\"{x[0]}_patient_q3\"\n",
    ")\n",
    "\n",
    "# Calculate duration for each mode\n",
    "crrt_df['next_time'] = crrt_df.groupby('encounter_block')['recorded_dttm'].shift(-1)\n",
    "crrt_df['duration_hrs'] = (crrt_df['next_time'] - crrt_df['recorded_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "mode_duration = crrt_df.groupby(['encounter_block', 'crrt_mode_category'])['duration_hrs'].sum()\n",
    "duration_summary = mode_duration.groupby('crrt_mode_category').agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'median',\n",
    "    lambda x: x.quantile(0.25),\n",
    "    lambda x: x.quantile(0.75)\n",
    "])\n",
    "\n",
    "duration_summary.columns = [\n",
    "    'duration_mean_hrs',\n",
    "    'duration_std_hrs',\n",
    "    'duration_median_hrs',\n",
    "    'duration_q1_hrs',\n",
    "    'duration_q3_hrs'\n",
    "]\n",
    "\n",
    "# Save results\n",
    "first_settings_summary.to_csv(f'{output_folder}/final/crrt_first_settings.csv')\n",
    "mode_avg_settings.to_csv(f'{output_folder}/final/crrt_average_settings.csv')\n",
    "patient_summary.to_csv(f'{output_folder}/final/crrt_patient_averages.csv')\n",
    "duration_summary.to_csv(f'{output_folder}/final/crrt_duration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466190dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Build the display DataFrame of formatted strings\n",
    "params = [\n",
    "    \"blood_flow_rate\",\n",
    "    \"pre_filter_replacement_fluid_rate\", \n",
    "    \"post_filter_replacement_fluid_rate\",\n",
    "    \"dialysate_flow_rate\",\n",
    "    \"ultrafiltration_out\"\n",
    "]\n",
    "\n",
    "# This will be our new table: rows = modes, cols = params\n",
    "display_df = pd.DataFrame(index=mode_avg_settings.index)\n",
    "\n",
    "for p in params:\n",
    "    mean = mode_avg_settings[f\"{p}_mean\"]\n",
    "    std = mode_avg_settings[f\"{p}_std\"]\n",
    "    \n",
    "    # format each row as \"mean ± std\" \n",
    "    display_df[p] = [\n",
    "        f\"{int(m):,}  ({int(s):,})\" if pd.notna(m) and pd.notna(s) else \"NA\"\n",
    "        if pd.notna(m)\n",
    "        else \"NA\"\n",
    "        for m, s in zip(mean, std)\n",
    "    ]\n",
    "\n",
    "# optional: rename columns to more human-friendly labels\n",
    "display_df.columns = [\n",
    "    \"Blood flow (mL/hr)\",\n",
    "    \"Pre-filter repl (mL/hr)\", \n",
    "    \"Post-filter repl (mL/hr)\",\n",
    "    \"Dialysate flow (mL/hr)\",\n",
    "    \"Ultrafiltration (mL/hr)\"\n",
    "]\n",
    "\n",
    "# Step 2) Draw with matplotlib.table\n",
    "fig, ax = plt.subplots(figsize=(10, 2 + 0.5 * len(display_df)))  # height scales with rows\n",
    "ax.axis(\"off\")\n",
    "\n",
    "tbl = ax.table(\n",
    "    cellText=display_df.values,\n",
    "    rowLabels=display_df.index,\n",
    "    colLabels=display_df.columns,\n",
    "    cellLoc=\"center\",\n",
    "    rowLoc=\"center\",\n",
    "    loc=\"center\"\n",
    ")\n",
    "\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(10)\n",
    "tbl.scale(1, 1.5)  # stretch rows a bit\n",
    "\n",
    "plt.title(f\"Average CRRT Settings by CRRT Mode in {pyCLIF.helper['site_name']}: Mean (SD)\", pad=20)\n",
    "plt.tight_layout()\n",
    "# Save the figure\n",
    "plt.savefig(f\"../output/final/graphs/avg_crrt_settings.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index as column for melt\n",
    "duration_summary_reset = duration_summary.reset_index()\n",
    "\n",
    "# Plot Duration on each CRRT mode\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=duration_summary_reset, \n",
    "            x='crrt_mode_category', y='duration_median_hrs', \n",
    "            color='skyblue')\n",
    "plt.errorbar(x=np.arange(len(duration_summary_reset)), \n",
    "             y=duration_summary_reset['duration_median_hrs'], \n",
    "             yerr=[duration_summary_reset['duration_median_hrs'] - duration_summary_reset['duration_q1_hrs'], \n",
    "                   duration_summary_reset['duration_q3_hrs'] - duration_summary_reset['duration_median_hrs']], \n",
    "             fmt='none', c='black', capsize=5)\n",
    "plt.title(\"Duration on CRRT Mode (Median and IQR)\")\n",
    "plt.ylabel(\"Hours\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "# Save the figure before showing and closing\n",
    "plt.savefig(f\"{output_dir}/graphs/crrt_mode_duration.png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177feceb",
   "metadata": {},
   "source": [
    "# (E) CRRT Mode Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze mode switches by looking at consecutive rows with different modes\n",
    "crrt_df['prev_mode'] = crrt_df.groupby('encounter_block')['crrt_mode_category'].shift(1)\n",
    "crrt_df['mode_switch'] = (\n",
    "    (crrt_df['crrt_mode_category'] != crrt_df['prev_mode']) &\n",
    "    (~crrt_df['prev_mode'].isna()) &\n",
    "    (~crrt_df['crrt_mode_category'].isna())  # ← exclude transitions TO NaN\n",
    ")\n",
    "\n",
    "# Count switches between each mode pair\n",
    "mode_switches = crrt_df[crrt_df['mode_switch']].groupby(['prev_mode', 'crrt_mode_category']).size()\n",
    "mode_switches = mode_switches.reset_index()\n",
    "mode_switches.columns = ['from_mode', 'to_mode', 'count']\n",
    "\n",
    "# Create pivot table for display\n",
    "mode_switches_pivot = mode_switches.pivot(index='from_mode', columns='to_mode', values='count').fillna(0)\n",
    "\n",
    "print(\"\\nCRRT Mode Switches:\")\n",
    "print(mode_switches_pivot)\n",
    "\n",
    "# Visualize mode switches as a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(mode_switches_pivot, \n",
    "            annot=True, \n",
    "            fmt='.0f',\n",
    "            cmap='Blues',\n",
    "            cbar_kws={'label': f\"Number of Mode Switches {pyCLIF.helper['site_name']}\"})\n",
    "plt.title('CRRT Mode Mode Switch Matrix')\n",
    "plt.xlabel('To Mode')\n",
    "plt.ylabel('From Mode')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/graphs/crrt_mode_transitions_heatmap.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Calculate average number of mode switches per hospitalization\n",
    "switches_per_hosp = crrt_df.groupby('encounter_block')['mode_switch'].sum()\n",
    "avg_switches = switches_per_hosp.mean()\n",
    "median_switches = switches_per_hosp.median()\n",
    "\n",
    "print(f\"\\nAverage mode switches per hospitalization: {avg_switches:.2f}\")\n",
    "print(f\"Median mode switches per hospitalization: {median_switches:.2f}\")\n",
    "\n",
    "# Save mode switches matrix\n",
    "mode_switches_pivot.to_csv(f\"{output_dir}/crrt_mode_switches.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "with open(f\"{output_dir}/crrt_mode_switches_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Average mode switches per hospitalization: {avg_switches:.2f}\\n\")\n",
    "    f.write(f\"Median mode switches per hospitalization: {median_switches:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure mode switches are correctly flagged\n",
    "crrt_df['prev_mode'] = crrt_df.groupby('encounter_block')['crrt_mode_category'].shift(1)\n",
    "crrt_df['mode_switch'] = (\n",
    "    crrt_df['crrt_mode_category'] != crrt_df['prev_mode']\n",
    ") & (~crrt_df['prev_mode'].isna())\n",
    "\n",
    "# Count unique encounters where a switch occurred between each mode pair\n",
    "mode_switch_encounters = (\n",
    "    crrt_df[crrt_df['mode_switch']]\n",
    "    .groupby(['prev_mode', 'crrt_mode_category'])['encounter_block']\n",
    "    .nunique()\n",
    "    .reset_index(name='encounter_count')\n",
    ")\n",
    "\n",
    "# Pivot for matrix view\n",
    "switch_matrix = mode_switch_encounters.pivot(\n",
    "    index='prev_mode',\n",
    "    columns='crrt_mode_category',\n",
    "    values='encounter_count'\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "print(\"\\nUnique Encounters with CRRT Mode Switches:\")\n",
    "print(switch_matrix)\n",
    "\n",
    "# Save to CSV\n",
    "switch_matrix.to_csv(\"../output/final/unique_encounter_mode_switches.csv\")\n",
    "\n",
    "# Create heatmap for mode switches by encounter\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(switch_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            cbar_kws={'label': 'Number of Unique Encounters with Mode Switches'})\n",
    "plt.title(f\"CRRT Mode Switches by Unique Encounters {pyCLIF.helper['site_name']}\")\n",
    "plt.xlabel('To Mode')\n",
    "plt.ylabel('From Mode')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/graphs/crrt_mode_transitions_by_encounter_heatmap.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRRT MODE SWITCHES WITH LOCATION CONTEXT\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_mode_switches_with_location():\n",
    "    \"\"\"Analyze CRRT mode switches with location context\"\"\"\n",
    "\n",
    "    print(\"Analyzing CRRT mode switches with location context...\")\n",
    "\n",
    "    crrt_with_location = crrt_df.merge(\n",
    "        clif_wide_df[['encounter_block', 'recorded_dttm', 'location_type']]\n",
    "        .drop_duplicates(subset=['encounter_block', 'recorded_dttm']),\n",
    "        on=['encounter_block', 'recorded_dttm'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(f\"Matched {len(crrt_with_location)} CRRT records with location data\")\n",
    "    print(f\"Location coverage: {(~crrt_with_location['location_type'].isna()).mean()*100:.1f}%\")\n",
    "\n",
    "    crrt_with_location = crrt_with_location.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "    crrt_with_location['prev_mode'] = crrt_with_location.groupby('encounter_block')['crrt_mode_category'].shift(1)\n",
    "    crrt_with_location['mode_switch'] = (\n",
    "        (crrt_with_location['crrt_mode_category'] != crrt_with_location['prev_mode']) &\n",
    "        (~crrt_with_location['prev_mode'].isna())\n",
    "    )\n",
    "\n",
    "    mode_switches_with_location = crrt_with_location[crrt_with_location['mode_switch']].copy()\n",
    "\n",
    "    print(f\"Found {len(mode_switches_with_location)} mode switches\")\n",
    "    print(f\"Unique encounters with switches: {mode_switches_with_location['encounter_block'].nunique()}\")\n",
    "\n",
    "    location_transition_matrices = {}\n",
    "\n",
    "    for location in mode_switches_with_location['location_type'].dropna().unique():\n",
    "        location_switches = mode_switches_with_location[\n",
    "            mode_switches_with_location['location_type'] == location\n",
    "        ]\n",
    "\n",
    "        if len(location_switches) > 0:\n",
    "            transition_counts = (\n",
    "                location_switches\n",
    "                .groupby(['prev_mode', 'crrt_mode_category'])['encounter_block']\n",
    "                .nunique()\n",
    "                .reset_index(name='encounter_count')\n",
    "            )\n",
    "\n",
    "            transition_matrix = transition_counts.pivot(\n",
    "                index='prev_mode',\n",
    "                columns='crrt_mode_category',\n",
    "                values='encounter_count'\n",
    "            ).fillna(0).astype(int)\n",
    "\n",
    "            location_transition_matrices[location] = transition_matrix\n",
    "\n",
    "            unique_encounters = location_switches['encounter_block'].nunique()\n",
    "            print(f\"\\n{location}: {len(location_switches)} switches, {unique_encounters} unique encounters\")\n",
    "            print(transition_matrix)\n",
    "\n",
    "    return mode_switches_with_location, location_transition_matrices\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE LOCATION-STRATIFIED TRANSITION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def create_comprehensive_transition_analysis():\n",
    "    \"\"\"Create comprehensive analysis of mode transitions by location\"\"\"\n",
    "\n",
    "    switches_with_location, location_matrices = analyze_mode_switches_with_location()\n",
    "\n",
    "    location_summary = []\n",
    "\n",
    "    print(\"\\nMode Switch Summary by Location:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for location in switches_with_location['location_type'].dropna().unique():\n",
    "        location_data = switches_with_location[switches_with_location['location_type'] == location]\n",
    "\n",
    "        if len(location_data) > 0:\n",
    "            unique_encounters = location_data['encounter_block'].nunique()\n",
    "            total_switches = len(location_data)\n",
    "\n",
    "            from_modes = location_data['prev_mode'].value_counts().to_dict()\n",
    "            to_modes = location_data['crrt_mode_category'].value_counts().to_dict()\n",
    "\n",
    "            most_common_from = max(from_modes, key=from_modes.get) if from_modes else \"None\"\n",
    "            most_common_to = max(to_modes, key=to_modes.get) if to_modes else \"None\"\n",
    "\n",
    "            print(f\"\\n{location}:\")\n",
    "            print(f\"  Unique encounters with switches: {unique_encounters}\")\n",
    "            print(f\"  Total switch events: {total_switches}\")\n",
    "            print(f\"  Most common FROM mode: {most_common_from}\")\n",
    "            print(f\"  Most common TO mode: {most_common_to}\")\n",
    "\n",
    "            location_summary.append({\n",
    "                'location': location,\n",
    "                'unique_encounters': unique_encounters,\n",
    "                'total_switches': total_switches,\n",
    "                'most_common_from': most_common_from,\n",
    "                'most_common_to': most_common_to\n",
    "            })\n",
    "\n",
    "    location_summary_df = pd.DataFrame(location_summary)\n",
    "\n",
    "    detailed_results = {}\n",
    "\n",
    "    for location, matrix in location_matrices.items():\n",
    "        total_encounters_with_switches = matrix.sum().sum()\n",
    "\n",
    "        detailed_table = []\n",
    "        for from_mode in matrix.index:\n",
    "            for to_mode in matrix.columns:\n",
    "                count = matrix.loc[from_mode, to_mode]\n",
    "                if count > 0:\n",
    "                    pct_of_total = (count / total_encounters_with_switches) * 100\n",
    "                    detailed_table.append({\n",
    "                        'location': location,\n",
    "                        'from_mode': from_mode,\n",
    "                        'to_mode': to_mode,\n",
    "                        'encounter_count': count,\n",
    "                        'pct_of_location_encounters': pct_of_total\n",
    "                    })\n",
    "\n",
    "        detailed_results[location] = pd.DataFrame(detailed_table)\n",
    "\n",
    "    switches_with_location.to_csv(\n",
    "        f\"{output_folder}/intermediate/crrt_switches_with_location.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    location_summary_df.to_csv(\n",
    "        f\"{output_folder}/final/switch_summary_by_location.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    for location, matrix in location_matrices.items():\n",
    "        safe_filename = location.replace('/', '_').replace(' ', '_')\n",
    "        matrix.to_csv(\n",
    "            f\"{output_folder}/final/transition_matrix_{safe_filename}.csv\"\n",
    "        )\n",
    "\n",
    "    if detailed_results:\n",
    "        all_detailed = pd.concat(detailed_results.values(), ignore_index=True)\n",
    "        all_detailed.to_csv(\n",
    "            f\"{output_folder}/final/detailed_transitions_by_location.csv\",\n",
    "            index=False\n",
    "        )\n",
    "    else:\n",
    "        all_detailed = pd.DataFrame()\n",
    "\n",
    "    return switches_with_location, location_matrices, all_detailed\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION OF LOCATION-SPECIFIC TRANSITIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_location_transition_visualizations():\n",
    "    \"\"\"Create visualizations for mode transitions by location\"\"\"\n",
    "\n",
    "    switches_data, matrices, detailed = create_comprehensive_transition_analysis()\n",
    "\n",
    "    if not matrices:\n",
    "        print(\"No transition matrices to visualize!\")\n",
    "        return detailed\n",
    "\n",
    "    n_locations = len(matrices)\n",
    "    n_cols = min(3, n_locations)\n",
    "    n_rows = (n_locations + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "    if n_locations == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = [axes] if n_locations == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, (location, matrix) in enumerate(matrices.items()):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        sns.heatmap(matrix,\n",
    "                    annot=True,\n",
    "                    fmt='d',\n",
    "                    cmap='Blues',\n",
    "                    ax=ax,\n",
    "                    cbar=True)\n",
    "\n",
    "        total_encounters = matrix.sum().sum()\n",
    "        ax.set_title(f'{location}\\n({total_encounters} encounters with switches)')\n",
    "        ax.set_xlabel('To Mode')\n",
    "        ax.set_ylabel('From Mode')\n",
    "\n",
    "    for idx in range(n_locations, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/final/graphs/transition_matrices_by_location.png\",\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if len(switches_data) > 0:\n",
    "        encounter_counts_by_location = (\n",
    "            switches_data.groupby('location_type')['encounter_block']\n",
    "            .nunique()\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(encounter_counts_by_location)), encounter_counts_by_location.values)\n",
    "        plt.xlabel('ICU Location Type')\n",
    "        plt.ylabel('Number of Encounters with Mode Switches')\n",
    "        plt.title('CRRT Mode Switches by ICU Location (Unique Encounters)')\n",
    "        plt.xticks(range(len(encounter_counts_by_location)),\n",
    "                    encounter_counts_by_location.index, rotation=45, ha='right')\n",
    "\n",
    "        for bar, count in zip(bars, encounter_counts_by_location.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                    f'{count}', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_folder}/final/graphs/mode_switches_by_location_summary.png\",\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return detailed\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING CRRT MODE SWITCHES WITH LOCATION CONTEXT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "detailed_transitions = create_location_transition_visualizations()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRANSITION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files created:\")\n",
    "print(\"  - crrt_switches_with_location.csv (raw data)\")\n",
    "print(\"  - switch_summary_by_location.csv (summary by location)\")\n",
    "print(\"  - transition_matrix_[location].csv (one per ICU type)\")\n",
    "print(\"  - detailed_transitions_by_location.csv (detailed results)\")\n",
    "print(\"  - transition_matrices_by_location.png (heatmaps)\")\n",
    "print(\"  - mode_switches_by_location_summary.png (bar chart)\")\n",
    "\n",
    "if len(detailed_transitions) > 0:\n",
    "    print(f\"\\nSample of detailed transitions:\")\n",
    "    print(detailed_transitions.head(10))\n",
    "else:\n",
    "    print(\"\\nNo detailed transitions to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d00e8",
   "metadata": {},
   "source": [
    "# (F) Hourly Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ab615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a relative‐hour field where hour=0 is 24h before CRRT\n",
    "df = (\n",
    "    clif_wide_df\n",
    "    .merge(first_crrt, on=\"encounter_block\", how=\"inner\")\n",
    ")\n",
    "# origin = first_crrt_time - 24h\n",
    "df[\"origin\"] = df[\"first_crrt_time\"] - pd.Timedelta(hours=24)\n",
    "df[\"rel_hr\"] = ((df[\"recorded_dttm\"] - df[\"origin\"]) \n",
    "                / pd.Timedelta(hours=1))\n",
    "# filter to [0, 96]\n",
    "df = df[(df[\"rel_hr\"] >= 0) & (df[\"rel_hr\"] <= 96)]\n",
    "\n",
    "# assign integer hour bins\n",
    "df[\"hour_bin\"] = df[\"rel_hr\"].floordiv(1).astype(int)\n",
    "\n",
    "# 2) Compute per‐hour median, Q1, Q3 for each variable\n",
    "agg = {}\n",
    "for v in continuous_vars:\n",
    "    agg[v] = [\"median\", lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
    "hourly = df.groupby(\"hour_bin\").agg(agg)\n",
    "# flatten columns\n",
    "hourly.columns = pd.Index([f\"{var}_{stat}\"\n",
    "                           for var,stat in hourly.columns],\n",
    "                          name=None)\n",
    "\n",
    "# 3) Plot grid of time‐series with shaded IQR and CRRT‐band\n",
    "n = len(continuous_vars)\n",
    "cols = 4\n",
    "rows = (n + cols - 1)//cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 2.5*rows), sharex=True)\n",
    "for ax, var in zip(axes.flat, continuous_vars):\n",
    "    m   = hourly[f\"{var}_median\"]\n",
    "    q1  = hourly[f\"{var}_<lambda_0>\"]  # this is the 0.25 quantile\n",
    "    q3  = hourly[f\"{var}_<lambda_1>\"]  # this is the 0.75 quantile\n",
    "\n",
    "    ax.plot(hourly.index, m)\n",
    "    ax.fill_between(hourly.index, q1, q3, alpha=0.3)\n",
    "    # highlight the first 24h of CRRT → that's rel_hr 24→48\n",
    "    ax.axvspan(24, 48, color=\"C1\", alpha=0.1)\n",
    "    ax.set_title(f\"{var} [median, IQR]\")\n",
    "    ax.set_xlim(0,96)\n",
    "    ax.set_xlabel(\"Hours\")\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "# hide any empty subplots\n",
    "for extra_ax in axes.flat[n:]:\n",
    "    extra_ax.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.suptitle(\"Trajectories of ICU Labs/Vasos/Vent Settings from CRRT-24hrs to CRRT+72hrs (Median[IQR])\", y=1.02)\n",
    "plt.savefig(\"../output/final/graphs/trajectories_median_iqr.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# save hourly data\n",
    "hourly = hourly.rename(\n",
    "    columns={f\"{v}_<lambda_0>\": f\"{v}_q1\",\n",
    "             f\"{v}_<lambda_1>\": f\"{v}_q3\"}\n",
    ")\n",
    "\n",
    "# 2) reset_index so `hour_bin` becomes a column again\n",
    "site_summary = hourly.reset_index().rename(columns={\"hour_bin\": \"hour\"})\n",
    "site_summary.to_csv(os.path.join(output_folder, \"final\", \"site_hourly_summary_median.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a relative‐hour field where hour=0 is 24h before CRRT\n",
    "df = (\n",
    "    clif_wide_df\n",
    "    .merge(first_crrt, on=\"encounter_block\", how=\"inner\")\n",
    ")\n",
    "# origin = first_crrt_time - 24h\n",
    "df[\"origin\"] = df[\"first_crrt_time\"] - pd.Timedelta(hours=24)\n",
    "df[\"rel_hr\"] = ((df[\"recorded_dttm\"] - df[\"origin\"]) \n",
    "                / pd.Timedelta(hours=1))\n",
    "# filter to [0, 96]\n",
    "df = df[(df[\"rel_hr\"] >= 0) & (df[\"rel_hr\"] <= 96)]\n",
    "\n",
    "# assign integer hour bins\n",
    "df[\"hour_bin\"] = df[\"rel_hr\"].floordiv(1).astype(int)\n",
    "\n",
    "# 2) Compute per‐hour mean and std for each variable\n",
    "agg = {}\n",
    "for v in continuous_vars:\n",
    "    agg[v] = [\"mean\", \"std\"]\n",
    "hourly = df.groupby(\"hour_bin\").agg(agg)\n",
    "# flatten columns\n",
    "hourly.columns = pd.Index([f\"{var}_{stat}\"\n",
    "                           for var,stat in hourly.columns],\n",
    "                          name=None)\n",
    "\n",
    "# 3) Plot grid of time‐series with shaded ±1 SD and CRRT‐band\n",
    "n = len(continuous_vars)\n",
    "cols = 4\n",
    "rows = (n + cols - 1)//cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 2.5*rows), sharex=True)\n",
    "for ax, var in zip(axes.flat, continuous_vars):\n",
    "    mean = hourly[f\"{var}_mean\"]\n",
    "    sd = hourly[f\"{var}_std\"]\n",
    "\n",
    "    ax.plot(hourly.index, mean)\n",
    "    ax.fill_between(hourly.index, mean-sd, mean+sd, alpha=0.3)\n",
    "    # highlight the first 24h of CRRT → that's rel_hr 24→48\n",
    "    ax.axvspan(24, 48, color=\"C1\", alpha=0.1)\n",
    "    ax.set_title(f\"{var} [mean ± SD]\")\n",
    "    ax.set_xlim(0,96)\n",
    "    ax.set_xlabel(\"Hours\")\n",
    "    ax.set_ylabel(var)\n",
    "\n",
    "# hide any empty subplots\n",
    "for extra_ax in axes.flat[n:]:\n",
    "    extra_ax.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.suptitle(\"Trajectories of ICU Labs/Vasos/Vent Settings from CRRT-24hrs to CRRT+72hrs (Mean ± SD)\", y=1.02)\n",
    "plt.savefig(\"../output/final/graphs/trajectories_mean_sd.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# save hourly data\n",
    "hourly = hourly.rename(\n",
    "    columns={f\"{v}_<lambda_0>\": f\"{v}_q1\",\n",
    "             f\"{v}_<lambda_1>\": f\"{v}_q3\"}\n",
    ")\n",
    "\n",
    "# 2) reset_index so `hour_bin` becomes a column again\n",
    "site_summary = hourly.reset_index().rename(columns={\"hour_bin\": \"hour\"})\n",
    "site_summary.to_csv(os.path.join(output_folder, \"final\", \"site_hourly_summary_mean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8930b",
   "metadata": {},
   "source": [
    "# (G) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION G: Chi-Square Statistical Analysis\n",
    "# ============================================================================\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging to both file and console\n",
    "log_dir = \"../output/final\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"statistical_analysis.log\")\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger(\"statistical_analysis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create file handler\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create console handler that prints to notebook\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"=== CHI-SQUARE ANALYSIS FOR CRRT COHORTS ===\")\n",
    "\n",
    "# Use existing cohort flags from Section C - no need to recreate them!\n",
    "logger.info(\"\\n=== COHORT SIZES (from Section C) ===\")\n",
    "logger.info(f\"surv_oncrrt_post24: {enc['surv_oncrrt_post24'].sum()} patients\")\n",
    "logger.info(f\"surv_offcrrt_post24: {enc['surv_offcrrt_post24'].sum()} patients\")\n",
    "logger.info(f\"nonsurv_oncrrt_post24: {enc['non_surv_oncrrt_post24'].sum()} patients\")\n",
    "logger.info(f\"nonsurv_offcrrt_post24: {enc['non_surv_offcrrt_post24'].sum()} patients\")\n",
    "logger.info(f\"surv_oncrrt_post72: {enc['surv_oncrrt_post72'].sum()} patients\")\n",
    "logger.info(f\"surv_offcrrt_post72: {enc['surv_offcrrt_post72'].sum()} patients\")\n",
    "logger.info(f\"nonsurv_oncrrt_post72: {enc['non_surv_oncrrt_post72'].sum()} patients\")\n",
    "logger.info(f\"nonsurv_offcrrt_post72: {enc['non_surv_offcrrt_post72'].sum()} patients\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CRRT LIBERATION vs MORTALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"1. CRRT LIBERATION vs MORTALITY ANALYSIS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "def chi_square_liberation_mortality():\n",
    "    \"\"\"Test if CRRT liberation is associated with survival\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 24-hour analysis\n",
    "    logger.info(\"\\n--- 24-Hour Analysis ---\")\n",
    "\n",
    "    # Create contingency table using existing cohort flags\n",
    "    contingency_24h = pd.DataFrame({\n",
    "        'On_CRRT': [enc['surv_oncrrt_post24'].sum(), enc['non_surv_oncrrt_post24'].sum()],\n",
    "        'Off_CRRT': [enc['surv_offcrrt_post24'].sum(), enc['non_surv_offcrrt_post24'].sum()]\n",
    "    }, index=['Survived', 'Died'])\n",
    "\n",
    "    logger.info(\"\\nContingency Table (24h):\")\n",
    "    logger.info(str(contingency_24h))\n",
    "\n",
    "    # Perform chi-square test\n",
    "    chi2_24h, p_24h, dof_24h, expected_24h = chi2_contingency(contingency_24h)\n",
    "\n",
    "    logger.info(f\"\\nChi-square statistic: {chi2_24h:.4f}\")\n",
    "    logger.info(f\"p-value: {p_24h:.6f}\")\n",
    "    logger.info(f\"Degrees of freedom: {dof_24h}\")\n",
    "\n",
    "    # Calculate effect size (Cramer's V)\n",
    "    n_24h = contingency_24h.sum().sum()\n",
    "    cramers_v_24h = np.sqrt(chi2_24h / (n_24h * (min(contingency_24h.shape) - 1)))\n",
    "    logger.info(f\"Cramer's V (effect size): {cramers_v_24h:.4f}\")\n",
    "\n",
    "    results['24h'] = {\n",
    "        'contingency_table': contingency_24h.to_dict(),\n",
    "        'chi2_stat': chi2_24h,\n",
    "        'p_value': p_24h,\n",
    "        'cramers_v': cramers_v_24h,\n",
    "        'n_total': int(n_24h)\n",
    "    }\n",
    "\n",
    "    # 72-hour analysis (survivors of 24h only)\n",
    "    logger.info(\"\\n--- 72-Hour Analysis (24h survivors only) ---\")\n",
    "\n",
    "    contingency_72h = pd.DataFrame({\n",
    "        'On_CRRT': [enc['surv_oncrrt_post72'].sum(), enc['non_surv_oncrrt_post72'].sum()],\n",
    "        'Off_CRRT': [enc['surv_offcrrt_post72'].sum(), enc['non_surv_offcrrt_post72'].sum()]\n",
    "    }, index=['Survived', 'Died'])\n",
    "\n",
    "    logger.info(\"\\nContingency Table (72h):\")\n",
    "    logger.info(str(contingency_72h))\n",
    "\n",
    "    chi2_72h, p_72h, dof_72h, expected_72h = chi2_contingency(contingency_72h)\n",
    "\n",
    "    logger.info(f\"\\nChi-square statistic: {chi2_72h:.4f}\")\n",
    "    logger.info(f\"p-value: {p_72h:.6f}\")\n",
    "    logger.info(f\"Degrees of freedom: {dof_72h}\")\n",
    "\n",
    "    n_72h = contingency_72h.sum().sum()\n",
    "    cramers_v_72h = np.sqrt(chi2_72h / (n_72h * (min(contingency_72h.shape) - 1)))\n",
    "    logger.info(f\"Cramer's V (effect size): {cramers_v_72h:.4f}\")\n",
    "\n",
    "    results['72h'] = {\n",
    "        'contingency_table': contingency_72h.to_dict(),\n",
    "        'chi2_stat': chi2_72h,\n",
    "        'p_value': p_72h,\n",
    "        'cramers_v': cramers_v_72h,\n",
    "        'n_total': int(n_72h)\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "liberation_mortality_results = chi_square_liberation_mortality()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ICU LOCATION vs OUTCOMES ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"2. ICU LOCATION vs OUTCOMES ANALYSIS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "def chi_square_location_outcomes():\n",
    "    \"\"\"Test if ICU location is associated with outcomes\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Load the combined summary data to get location info\n",
    "    combined_summary = pd.read_csv('../output/intermediate/combined_summary.csv')\n",
    "\n",
    "    # Get location info from the post72 window (this has location_type)\n",
    "    post72_df = combined_summary[combined_summary['window'] == 'Post-72h'].set_index('encounter_block')\n",
    "\n",
    "    # Merge location info with our encounter-level data\n",
    "    enc_with_location = enc.merge(\n",
    "        post72_df[['location_type']],\n",
    "        left_on='encounter_block',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    logger.info(\"\\n--- Location Distribution Analysis ---\")\n",
    "    logger.info(f\"Patients with location data: {(~enc_with_location['location_type'].isna()).sum()}\")\n",
    "    logger.info(f\"Location types: {enc_with_location['location_type'].dropna().unique()}\")\n",
    "\n",
    "    # 24-hour mortality by location\n",
    "    logger.info(\"\\n24-Hour Mortality by ICU Location:\")\n",
    "\n",
    "    # Create contingency table: Location vs 24h mortality\n",
    "    location_24h_mortality = pd.crosstab(\n",
    "        enc_with_location['location_type'],\n",
    "        enc_with_location['died_within_24h'],\n",
    "        margins=True\n",
    "    )\n",
    "\n",
    "    logger.info(str(location_24h_mortality))\n",
    "\n",
    "    # Remove 'All' row/column for chi-square test\n",
    "    contingency_loc_24h = location_24h_mortality.iloc[:-1, :-1]\n",
    "\n",
    "    # Only include locations with sufficient sample size (n>10)\n",
    "    valid_locations = contingency_loc_24h.index[contingency_loc_24h.sum(axis=1) >= 10]\n",
    "    contingency_loc_24h_filtered = contingency_loc_24h.loc[valid_locations]\n",
    "\n",
    "    logger.info(f\"\\nFiltered contingency table (locations with n≥10):\")\n",
    "    logger.info(str(contingency_loc_24h_filtered))\n",
    "\n",
    "    if len(contingency_loc_24h_filtered) > 1 and contingency_loc_24h_filtered.sum().sum() > 0:\n",
    "        chi2_loc_24h, p_loc_24h, dof_loc_24h, expected_loc_24h = chi2_contingency(contingency_loc_24h_filtered)\n",
    "\n",
    "        logger.info(f\"\\nChi-square statistic: {chi2_loc_24h:.4f}\")\n",
    "        logger.info(f\"p-value: {p_loc_24h:.6f}\")\n",
    "        logger.info(f\"Degrees of freedom: {dof_loc_24h}\")\n",
    "\n",
    "        # Cramer's V\n",
    "        n_loc_24h = contingency_loc_24h_filtered.sum().sum()\n",
    "        cramers_v_loc_24h = np.sqrt(chi2_loc_24h / (n_loc_24h * (min(contingency_loc_24h_filtered.shape) - 1)))\n",
    "        logger.info(f\"Cramer's V (effect size): {cramers_v_loc_24h:.4f}\")\n",
    "\n",
    "        results['location_24h_mortality'] = {\n",
    "            'contingency_table': contingency_loc_24h_filtered.to_dict(),\n",
    "            'chi2_stat': chi2_loc_24h,\n",
    "            'p_value': p_loc_24h,\n",
    "            'cramers_v': cramers_v_loc_24h,\n",
    "            'n_total': int(n_loc_24h)\n",
    "        }\n",
    "\n",
    "    # 24-hour CRRT liberation by location\n",
    "    logger.info(\"\\n24-Hour CRRT Liberation by ICU Location:\")\n",
    "\n",
    "    location_24h_liberation = pd.crosstab(\n",
    "        enc_with_location['location_type'],\n",
    "        ~enc_with_location['on_crrt_at_24h'],  # Liberation = NOT on CRRT\n",
    "        margins=True\n",
    "    )\n",
    "\n",
    "    logger.info(str(location_24h_liberation))\n",
    "\n",
    "    contingency_lib_24h = location_24h_liberation.iloc[:-1, :-1]\n",
    "    contingency_lib_24h_filtered = contingency_lib_24h.loc[valid_locations]\n",
    "\n",
    "    if len(contingency_lib_24h_filtered) > 1 and contingency_lib_24h_filtered.sum().sum() > 0:\n",
    "        chi2_lib_24h, p_lib_24h, dof_lib_24h, expected_lib_24h = chi2_contingency(contingency_lib_24h_filtered)\n",
    "\n",
    "        logger.info(f\"\\nChi-square statistic: {chi2_lib_24h:.4f}\")\n",
    "        logger.info(f\"p-value: {p_lib_24h:.6f}\")\n",
    "        logger.info(f\"Degrees of freedom: {dof_lib_24h}\")\n",
    "\n",
    "        n_lib_24h = contingency_lib_24h_filtered.sum().sum()\n",
    "        cramers_v_lib_24h = np.sqrt(chi2_lib_24h / (n_lib_24h * (min(contingency_lib_24h_filtered.shape) - 1)))\n",
    "        logger.info(f\"Cramer's V (effect size): {cramers_v_lib_24h:.4f}\")\n",
    "\n",
    "        results['location_24h_liberation'] = {\n",
    "            'contingency_table': contingency_lib_24h_filtered.to_dict(),\n",
    "            'chi2_stat': chi2_lib_24h,\n",
    "            'p_value': p_lib_24h,\n",
    "            'cramers_v': cramers_v_lib_24h,\n",
    "            'n_total': int(n_lib_24h)\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "location_outcomes_results = chi_square_location_outcomes()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SUMMARY AND EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"3. SUMMARY OF STATISTICAL RESULTS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Compile all results\n",
    "statistical_results = {\n",
    "    'site_name': 'MIMIC',  # Change this for other sites\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'total_patients': len(enc),\n",
    "    'liberation_vs_mortality': liberation_mortality_results,\n",
    "    'location_vs_outcomes': location_outcomes_results\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "logger.info(f\"\\nSite: {statistical_results['site_name']}\")\n",
    "logger.info(f\"Total patients: {statistical_results['total_patients']}\")\n",
    "logger.info(f\"Analysis date: {statistical_results['analysis_date']}\")\n",
    "\n",
    "logger.info(\"\\nKey Findings:\")\n",
    "if 'liberation_vs_mortality' in statistical_results:\n",
    "    p_24h = statistical_results['liberation_vs_mortality']['24h']['p_value']\n",
    "    p_72h = statistical_results['liberation_vs_mortality']['72h']['p_value']\n",
    "    logger.info(f\"- CRRT liberation vs mortality (24h): p = {p_24h:.6f}\")\n",
    "    logger.info(f\"- CRRT liberation vs mortality (72h): p = {p_72h:.6f}\")\n",
    "\n",
    "if 'location_vs_outcomes' in statistical_results:\n",
    "    if 'location_24h_mortality' in statistical_results['location_vs_outcomes']:\n",
    "        p_loc_mort = statistical_results['location_vs_outcomes']['location_24h_mortality']['p_value']\n",
    "        logger.info(f\"- ICU location vs 24h mortality: p = {p_loc_mort:.6f}\")\n",
    "\n",
    "    if 'location_24h_liberation' in statistical_results['location_vs_outcomes']:\n",
    "        p_loc_lib = statistical_results['location_vs_outcomes']['location_24h_liberation']['p_value']\n",
    "        logger.info(f\"- ICU location vs 24h liberation: p = {p_loc_lib:.6f}\")\n",
    "\n",
    "# Export results to JSON for multi-site comparison\n",
    "output_file = f\"../output/final/statistical_analysis_{statistical_results['site_name'].lower()}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "\n",
    "    # Deep convert the dictionary\n",
    "    json_ready = json.loads(json.dumps(statistical_results, default=convert_types))\n",
    "    json.dump(json_ready, f, indent=2)\n",
    "\n",
    "logger.info(f\"\\nResults exported to: {output_file}\")\n",
    "logger.info(f\"Log file saved to: {log_file}\")\n",
    "\n",
    "# Remove handlers to avoid duplicate logging in future cells\n",
    "logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STATISTICAL ANALYSIS VISUALIZATIONS FOR MULTI-SITE COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set style for consistent, publication-ready plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs(\"../output/final/graphs\", exist_ok=True)\n",
    "\n",
    "# Get site name from config\n",
    "site_name = pyCLIF.helper['site_name']\n",
    "\n",
    "print(f\"Creating statistical analysis visualizations for {site_name}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CRRT LIBERATION vs MORTALITY VISUALIZATION (Dynamic)\n",
    "# ============================================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Extract data dynamically from statistical results\n",
    "lib_mort_24h = liberation_mortality_results['24h']\n",
    "lib_mort_72h = liberation_mortality_results['72h']\n",
    "\n",
    "# Recreate contingency tables from the results\n",
    "contingency_24h = pd.DataFrame(lib_mort_24h['contingency_table'])\n",
    "contingency_72h = pd.DataFrame(lib_mort_72h['contingency_table'])\n",
    "\n",
    "# Colors\n",
    "colors = ['#2E8B57', '#DC143C']  # Green for survived, red for died\n",
    "\n",
    "# 24-hour analysis\n",
    "bars1 = ax1.bar(['On CRRT', 'Off CRRT'], contingency_24h.loc['Survived'],\n",
    "                color=colors[0], label='Survived', alpha=0.8)\n",
    "bars2 = ax1.bar(['On CRRT', 'Off CRRT'], contingency_24h.loc['Died'],\n",
    "                bottom=contingency_24h.loc['Survived'], color=colors[1],\n",
    "                label='Died', alpha=0.8)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    total = contingency_24h.iloc[:, i].sum()\n",
    "    surv_pct = contingency_24h.iloc[0, i] / total * 100\n",
    "    died_pct = contingency_24h.iloc[1, i] / total * 100\n",
    "    \n",
    "    # Survived percentage\n",
    "    ax1.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height()/2,\n",
    "            f'{surv_pct:.1f}%\\n(n={contingency_24h.iloc[0, i]})',\n",
    "            ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # Died percentage\n",
    "    ax1.text(bar2.get_x() + bar2.get_width()/2,\n",
    "            bar1.get_height() + bar2.get_height()/2,\n",
    "            f'{died_pct:.1f}%\\n(n={contingency_24h.iloc[1, i]})',\n",
    "            ha='center', va='center', fontweight='bold', color='white')\n",
    "\n",
    "p_val_24h = lib_mort_24h['p_value']\n",
    "cramers_24h = lib_mort_24h['cramers_v']\n",
    "ax1.set_title(f'24-Hour Analysis\\np = {p_val_24h:.6f}, Cramer\\'s V = {cramers_24h:.3f}',\n",
    "            fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Patients')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 72-hour analysis\n",
    "bars3 = ax2.bar(['On CRRT', 'Off CRRT'], contingency_72h.loc['Survived'],\n",
    "                color=colors[0], label='Survived', alpha=0.8)\n",
    "bars4 = ax2.bar(['On CRRT', 'Off CRRT'], contingency_72h.loc['Died'],\n",
    "                bottom=contingency_72h.loc['Survived'], color=colors[1],\n",
    "                label='Died', alpha=0.8)\n",
    "\n",
    "# Add percentage labels for 72h\n",
    "for i, (bar3, bar4) in enumerate(zip(bars3, bars4)):\n",
    "    total = contingency_72h.iloc[:, i].sum()\n",
    "    surv_pct = contingency_72h.iloc[0, i] / total * 100\n",
    "    died_pct = contingency_72h.iloc[1, i] / total * 100\n",
    "    \n",
    "    # Survived percentage\n",
    "    ax2.text(bar3.get_x() + bar3.get_width()/2, bar3.get_height()/2,\n",
    "            f'{surv_pct:.1f}%\\n(n={contingency_72h.iloc[0, i]})',\n",
    "            ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # Died percentage\n",
    "    ax2.text(bar4.get_x() + bar4.get_width()/2,\n",
    "            bar3.get_height() + bar4.get_height()/2,\n",
    "            f'{died_pct:.1f}%\\n(n={contingency_72h.iloc[1, i]})',\n",
    "            ha='center', va='center', fontweight='bold', color='white')\n",
    "\n",
    "p_val_72h = lib_mort_72h['p_value']\n",
    "cramers_72h = lib_mort_72h['cramers_v']\n",
    "ax2.set_title(f'72-Hour Analysis (24h Survivors Only)\\np = {p_val_72h:.6f}, Cramer\\'s V = {cramers_72h:.3f}',\n",
    "            fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Patients')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{site_name}: CRRT Liberation vs Mortality',\n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/crrt_liberation_vs_mortality.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ICU LOCATION OUTCOMES VISUALIZATION (Dynamic)\n",
    "# ============================================================================\n",
    "\n",
    "# Load location data dynamically from results\n",
    "if 'location_vs_outcomes' in statistical_results and location_outcomes_results:\n",
    "    \n",
    "    # Get location info from the combined summary data\n",
    "    combined_summary = pd.read_csv('../output/intermediate/combined_summary.csv')\n",
    "    post72_df = combined_summary[combined_summary['window'] == 'Post-72h'].set_index('encounter_block')\n",
    "    \n",
    "    # Merge location info with our encounter-level data\n",
    "    enc_with_location = enc.merge(\n",
    "        post72_df[['location_type']],\n",
    "        left_on='encounter_block',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate liberation rates by location\n",
    "    location_liberation = pd.crosstab(\n",
    "        enc_with_location['location_type'],\n",
    "        ~enc_with_location['on_crrt_at_24h'],  # Liberation = NOT on CRRT\n",
    "        margins=False\n",
    "    )\n",
    "    \n",
    "    location_liberation.columns = ['Still_on_CRRT', 'Liberated']\n",
    "    location_liberation['Total'] = location_liberation.sum(axis=1)\n",
    "    location_liberation['Liberation_Rate'] = location_liberation['Liberated'] / location_liberation['Total'] * 100\n",
    "    \n",
    "    # Calculate mortality rates by location\n",
    "    location_mortality = pd.crosstab(\n",
    "        enc_with_location['location_type'],\n",
    "        enc_with_location['died_within_24h'],\n",
    "        margins=False\n",
    "    )\n",
    "    \n",
    "    location_mortality.columns = ['Survived', 'Died']\n",
    "    location_mortality['Total'] = location_mortality.sum(axis=1)\n",
    "    location_mortality['Mortality_Rate'] = location_mortality['Died'] / location_mortality['Total'] * 100\n",
    "    \n",
    "    # Filter locations with sufficient sample size (n>=10)\n",
    "    valid_locations = location_liberation.index[location_liberation['Total'] >= 10]\n",
    "    location_liberation_filtered = location_liberation.loc[valid_locations]\n",
    "    location_mortality_filtered = location_mortality.loc[valid_locations]\n",
    "    \n",
    "    if len(location_liberation_filtered) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Liberation rates\n",
    "        colors_lib = sns.color_palette(\"viridis\", len(location_liberation_filtered))\n",
    "        bars1 = ax1.bar(location_liberation_filtered.index, location_liberation_filtered['Liberation_Rate'],\n",
    "                        color=colors_lib, alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, idx in zip(bars1, location_liberation_filtered.index):\n",
    "            rate = location_liberation_filtered.loc[idx, 'Liberation_Rate']\n",
    "            total = location_liberation_filtered.loc[idx, 'Total']\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{rate:.1f}%\\n(n={total})', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Get p-value from results\n",
    "        lib_p_val = location_outcomes_results.get('location_24h_liberation', {}).get('p_value', 'N/A')\n",
    "        if isinstance(lib_p_val, float):\n",
    "            lib_p_str = f'p = {lib_p_val:.6f}'\n",
    "        else:\n",
    "            lib_p_str = 'p = N/A'\n",
    "        \n",
    "        ax1.set_title(f'24-Hour CRRT Liberation Rates by ICU Type\\n{lib_p_str}',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Liberation Rate (%)')\n",
    "        ax1.set_ylim(0, 100)\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Mortality rates\n",
    "        colors_mort = sns.color_palette(\"plasma\", len(location_mortality_filtered))\n",
    "        bars2 = ax2.bar(location_mortality_filtered.index, location_mortality_filtered['Mortality_Rate'],\n",
    "                        color=colors_mort, alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, idx in zip(bars2, location_mortality_filtered.index):\n",
    "            rate = location_mortality_filtered.loc[idx, 'Mortality_Rate']\n",
    "            total = location_mortality_filtered.loc[idx, 'Total']\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                    f'{rate:.1f}%\\n(n={total})', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Get p-value from results\n",
    "        mort_p_val = location_outcomes_results.get('location_24h_mortality', {}).get('p_value', 'N/A')\n",
    "        if isinstance(mort_p_val, float):\n",
    "            mort_p_str = f'p = {mort_p_val:.6f}'\n",
    "        else:\n",
    "            mort_p_str = 'p = N/A'\n",
    "        \n",
    "        ax2.set_title(f'24-Hour Mortality Rates by ICU Type\\n{mort_p_str}',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Mortality Rate (%)')\n",
    "        max_mort_rate = location_mortality_filtered['Mortality_Rate'].max()\n",
    "        ax2.set_ylim(0, max_mort_rate * 1.2)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.suptitle(f'{site_name}: ICU Location vs CRRT Outcomes',\n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Add caption explaining liberation rate definition\n",
    "        fig.text(0.5, 0.02, 'Liberation Rate Definition: Percentage of patients not on CRRT at 24 hours post-initiation',\n",
    "                ha='center', fontsize=10, style='italic')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.1)\n",
    "        plt.savefig('../output/final/graphs/icu_location_outcomes.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SUMMARY DASHBOARD FOR MULTI-SITE COMPARISON (Dynamic)\n",
    "# ============================================================================\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Calculate metrics dynamically from enc dataframe\n",
    "total_patients = len(enc)\n",
    "mortality_24h = enc['died_within_24h'].sum()\n",
    "mortality_72h = enc['died_within_72h'].sum()\n",
    "liberation_24h = (~enc['on_crrt_at_24h']).sum()\n",
    "\n",
    "metrics = {\n",
    "    'Total Patients': total_patients,\n",
    "    '24h Mortality': f\"{mortality_24h/total_patients*100:.1f}%\",\n",
    "    '72h Mortality': f\"{mortality_72h/total_patients*100:.1f}%\",\n",
    "    '24h Liberation': f\"{liberation_24h/total_patients*100:.1f}%\"\n",
    "}\n",
    "\n",
    "# Cohort flow chart data (dynamic)\n",
    "survivors_24h = total_patients - mortality_24h\n",
    "on_crrt_24h = enc['on_crrt_at_24h'].sum()\n",
    "off_crrt_24h = liberation_24h\n",
    "survivors_72h = total_patients - mortality_72h\n",
    "on_crrt_72h = enc[~enc['died_within_72h']]['on_crrt_at_72h'].sum()\n",
    "off_crrt_72h = survivors_72h - on_crrt_72h\n",
    "\n",
    "cohort_flow = pd.DataFrame({\n",
    "    'Time_Point': ['Baseline', '24 Hours', '72 Hours'],\n",
    "    'Total': [total_patients, total_patients, survivors_24h],\n",
    "    'Alive': [total_patients, survivors_24h, survivors_72h],\n",
    "    'On_CRRT': [total_patients, on_crrt_24h, on_crrt_72h],\n",
    "    'Off_CRRT': [0, off_crrt_24h, off_crrt_72h]\n",
    "})\n",
    "\n",
    "# Flow chart visualization\n",
    "ax1.plot(cohort_flow['Time_Point'], cohort_flow['Alive'], 'o-', linewidth=3,\n",
    "        markersize=8, label='Alive', color='green')\n",
    "ax1.plot(cohort_flow['Time_Point'], cohort_flow['On_CRRT'], 's-', linewidth=3,\n",
    "        markersize=8, label='On CRRT', color='blue')\n",
    "ax1.plot(cohort_flow['Time_Point'], cohort_flow['Off_CRRT'], '^-', linewidth=3,\n",
    "        markersize=8, label='Off CRRT', color='orange')\n",
    "\n",
    "ax1.set_title(f'{site_name}: Patient Flow Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Patients')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Effect sizes comparison (dynamic)\n",
    "effects_data = []\n",
    "\n",
    "# Liberation vs Mortality effects\n",
    "effects_data.append(['Liberation vs\\nMortality (24h)',\n",
    "                    liberation_mortality_results['24h']['cramers_v'],\n",
    "                    liberation_mortality_results['24h']['p_value']])\n",
    "\n",
    "effects_data.append(['Liberation vs\\nMortality (72h)',\n",
    "                    liberation_mortality_results['72h']['cramers_v'],\n",
    "                    liberation_mortality_results['72h']['p_value']])\n",
    "\n",
    "# Location effects (if available)\n",
    "if 'location_24h_mortality' in location_outcomes_results:\n",
    "    effects_data.append(['Location vs\\nMortality',\n",
    "                        location_outcomes_results['location_24h_mortality']['cramers_v'],\n",
    "                        location_outcomes_results['location_24h_mortality']['p_value']])\n",
    "\n",
    "if 'location_24h_liberation' in location_outcomes_results:\n",
    "    effects_data.append(['Location vs\\nLiberation',\n",
    "                        location_outcomes_results['location_24h_liberation']['cramers_v'],\n",
    "                        location_outcomes_results['location_24h_liberation']['p_value']])\n",
    "\n",
    "effects = pd.DataFrame(effects_data, columns=['Analysis', 'Cramers_V', 'P_Value'])\n",
    "\n",
    "# Significance levels\n",
    "effects['Significant'] = effects['P_Value'].apply(\n",
    "    lambda p: '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'NS'\n",
    ")\n",
    "\n",
    "colors_effect = ['red' if p > 0.05 else 'darkgreen' for p in effects['P_Value']]\n",
    "bars = ax2.bar(effects['Analysis'], effects['Cramers_V'], color=colors_effect, alpha=0.7)\n",
    "\n",
    "for bar, cramers, sig in zip(bars, effects['Cramers_V'], effects['Significant']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{cramers:.3f}\\n{sig}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax2.set_title(f'{site_name}: Effect Sizes (Cramer\\'s V)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Effect Size')\n",
    "max_effect = effects['Cramers_V'].max()\n",
    "ax2.set_ylim(0, max_effect * 1.2)\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Liberation rate comparison (if location data available)\n",
    "if 'location_liberation_filtered' in locals() and len(location_liberation_filtered) > 0:\n",
    "    lib_comparison = location_liberation_filtered.sort_values('Liberation_Rate', ascending=True)\n",
    "    bars3 = ax3.barh(lib_comparison.index, lib_comparison['Liberation_Rate'],\n",
    "                    color=sns.color_palette(\"viridis\", len(lib_comparison)))\n",
    "    \n",
    "    for i, idx in enumerate(lib_comparison.index):\n",
    "        rate = lib_comparison.loc[idx, 'Liberation_Rate']\n",
    "        total = lib_comparison.loc[idx, 'Total']\n",
    "        ax3.text(rate + 1, i, f'{rate:.1f}% (n={total})', va='center', fontweight='bold')\n",
    "    \n",
    "    # Get p-value\n",
    "    lib_p_val = location_outcomes_results.get('location_24h_liberation', {}).get('p_value', 'N/A')\n",
    "    if isinstance(lib_p_val, float):\n",
    "        lib_p_str = f'p = {lib_p_val:.6f}'\n",
    "    else:\n",
    "        lib_p_str = 'p = N/A'\n",
    "    \n",
    "    ax3.set_title(f'{site_name}: 24h Liberation Rates by ICU\\n{lib_p_str}', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Liberation Rate (%)')\n",
    "    ax3.set_xlim(0, 100)\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Location data\\nnot available', ha='center', va='center',\n",
    "            transform=ax3.transAxes, fontsize=14)\n",
    "    ax3.set_title(f'{site_name}: Liberation Rates by ICU', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Key statistics table (dynamic)\n",
    "stats_text = f\"\"\"\n",
    "{site_name} CRRT EPIDEMIOLOGY SUMMARY\n",
    "\n",
    "Total Patients: {metrics['Total Patients']:,}\n",
    "24h Mortality: {metrics['24h Mortality']} (n={mortality_24h})\n",
    "72h Mortality: {metrics['72h Mortality']} (n={mortality_72h})\n",
    "24h Liberation: {metrics['24h Liberation']} (n={liberation_24h})\n",
    "\n",
    "STATISTICAL TESTS:\n",
    "Liberation vs Mortality (24h): p = {liberation_mortality_results['24h']['p_value']:.6f}\n",
    "Liberation vs Mortality (72h): p = {liberation_mortality_results['72h']['p_value']:.6f}\n",
    "\"\"\"\n",
    "\n",
    "# Add location results if available\n",
    "if 'location_24h_mortality' in location_outcomes_results:\n",
    "    stats_text += f\"Location vs Mortality: p = {location_outcomes_results['location_24h_mortality']['p_value']:.6f}\\n\"\n",
    "if 'location_24h_liberation' in location_outcomes_results:\n",
    "    stats_text += f\"Location vs Liberation: p = {location_outcomes_results['location_24h_liberation']['p_value']:.6f}\\n\"\n",
    "\n",
    "ax4.text(0.1, 0.5, stats_text, transform=ax4.transAxes, fontsize=11,\n",
    "        verticalalignment='center', bbox=dict(boxstyle=\"round,pad=0.3\",\n",
    "        facecolor=\"lightblue\", alpha=0.5))\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.suptitle(f'{site_name}: CRRT Epidemiology Statistical Analysis Dashboard',\n",
    "            fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/statistical_analysis_dashboard.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "## summary of results for a quick view \n",
    "def create_site_comparison_template():\n",
    "    \"\"\"Template function for comparing results across sites\"\"\"\n",
    "    \n",
    "    template_data = {\n",
    "        'Site': site_name,\n",
    "        'N_Patients': total_patients,\n",
    "        'Mortality_24h_pct': mortality_24h/total_patients*100,\n",
    "        'Mortality_72h_pct': mortality_72h/total_patients*100,\n",
    "        'Liberation_24h_pct': liberation_24h/total_patients*100,\n",
    "        'Liberation_vs_Mortality_24h_p': liberation_mortality_results['24h']['p_value'],\n",
    "        'Liberation_vs_Mortality_72h_p': liberation_mortality_results['72h']['p_value'],\n",
    "        'Cramers_V_Liberation_24h': liberation_mortality_results['24h']['cramers_v'],\n",
    "        'Cramers_V_Liberation_72h': liberation_mortality_results['72h']['cramers_v']\n",
    "    }\n",
    "    \n",
    "    # Add location results if available\n",
    "    if 'location_24h_mortality' in location_outcomes_results:\n",
    "        template_data['Location_vs_Mortality_p'] = location_outcomes_results['location_24h_mortality']['p_value']\n",
    "        template_data['Cramers_V_Location_Mortality'] = location_outcomes_results['location_24h_mortality']['cramers_v']\n",
    "    \n",
    "    if 'location_24h_liberation' in location_outcomes_results:\n",
    "        template_data['Location_vs_Liberation_p'] = location_outcomes_results['location_24h_liberation']['p_value']\n",
    "        template_data['Cramers_V_Location_Liberation'] = location_outcomes_results['location_24h_liberation']['cramers_v']\n",
    "    \n",
    "    # Save as CSV for easy comparison\n",
    "    template_df = pd.DataFrame([template_data])\n",
    "    template_df.to_csv(f'../output/final/{site_name.lower()}_statistical_summary.csv', index=False)\n",
    "    \n",
    "    return template_df\n",
    "\n",
    "template_df = create_site_comparison_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82282c63",
   "metadata": {},
   "source": [
    "# (H) Kaplan-Meier Survival Analysis\n",
    "\n",
    "Variable definitions: \n",
    "* Time 0: CRRT start time\n",
    "* Liberation event: Stopped CRRT while alive, so death is treated as a competing risk. \n",
    "* Using death_dttm_proxy as a death indicator which is discharge dttm when discharge category Expired or Hospice\n",
    "* For survival analysis, follow up till 28 days (max_followup cap)\n",
    "* Censoring Rules:\n",
    "    1. Liberation Analysis Censoring\n",
    "        - Event (1): Patient stopped CRRT while alive\n",
    "        - Censored (0): Patient died while on CRRT\n",
    "    2. Survival Analysis Censoring\n",
    "        - Event (1): Patient died\n",
    "        - Censored (0): Patient survived to end of follow-up (28 days)\n",
    "* Landmark times (24h and 72h)\n",
    "    1. Only include patients who survived to landmark time: `landmark_df = df[df['followup_death'] >= landmark_time]`\n",
    "    2. Reset time origin to landmark time: `landmark_df['followup_from_landmark'] = landmark_df['followup_death'] - landmark_time`\n",
    "* Valid locations are defined: A location needs at least 10 patients to be considered for analysis. \n",
    "\n",
    "\n",
    "Limitations:\n",
    "* Not accounting for changes in CRRT course i.e. not handling for time varying covariates\n",
    "* there is no adjustment for baseline differences between ICUs- doesn't handle for confounding\n",
    "* there could be selection bias because ICU admission is not random\n",
    "* administrative censoring because 28 day cap may miss late events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e11e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION H: Kaplan-Meier Survival Analysis\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test, multivariate_logrank_test\n",
    "from lifelines.plotting import plot_lifetimes\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger('km_analysis')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create file handler\n",
    "fh = logging.FileHandler('../output/final/km_analysis.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "# Create console handler\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(\"=== KAPLAN-MEIER SURVIVAL ANALYSIS ===\")\n",
    "\n",
    "# Get site name from config\n",
    "site_name = pyCLIF.helper['site_name']\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CREATE SURVIVAL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def create_survival_dataset():\n",
    "    \"\"\"Create time-to-event dataset for survival analysis\"\"\"\n",
    "\n",
    "    logger.info(\"Creating survival dataset...\")\n",
    "\n",
    "    # Start with encounter-level data\n",
    "    survival_df = enc.copy()\n",
    "\n",
    "    # Calculate time to events (in hours)\n",
    "    survival_df['time_to_death'] = np.where(\n",
    "        survival_df['death_dttm_proxy'].notna(),\n",
    "        (survival_df['death_dttm_proxy'] - survival_df['first_crrt_time']).dt.total_seconds() / 3600,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    survival_df['time_to_liberation'] = (\n",
    "        survival_df['end_crrt_time'] - survival_df['first_crrt_time']\n",
    "    ).dt.total_seconds() / 3600\n",
    "\n",
    "    # Create event indicators\n",
    "    survival_df['death_event'] = survival_df['death_dttm_proxy'].notna().astype(int)\n",
    "\n",
    "    # Liberation event: stopped CRRT while alive\n",
    "    # If patient died, they cannot be \"liberated\" - this is a competing risk\n",
    "    survival_df['liberation_event'] = np.where(\n",
    "        survival_df['death_event'] == 1,\n",
    "        0,  # Dead patients cannot be liberated\n",
    "        1   # Alive patients who stopped CRRT are liberated\n",
    "    )\n",
    "\n",
    "    # Get location data\n",
    "    combined_summary = pd.read_csv('../output/intermediate/combined_summary.csv')\n",
    "    post72_df = combined_summary[combined_summary['window'] == 'Post-72h'].set_index('encounter_block')\n",
    "\n",
    "    # Merge location info\n",
    "    survival_df = survival_df.merge(\n",
    "        post72_df[['location_type']],\n",
    "        left_on='encounter_block',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Get baseline SOFA scores for risk adjustment\n",
    "    pre24_df = combined_summary[combined_summary['window'] == 'Pre-24h'].set_index('encounter_block')\n",
    "    survival_df = survival_df.merge(\n",
    "        pre24_df[['sofa_total']],\n",
    "        left_on='encounter_block',\n",
    "        right_index=True,\n",
    "        how='left',\n",
    "        suffixes=('', '_baseline')\n",
    "    )\n",
    "\n",
    "    # Create follow-up time for each outcome\n",
    "    # For liberation: follow until death or liberation (whichever comes first)\n",
    "    survival_df['followup_liberation'] = np.where(\n",
    "        survival_df['death_event'] == 1,\n",
    "        survival_df['time_to_death'],      # Follow until death\n",
    "        survival_df['time_to_liberation']   # Follow until liberation\n",
    "    )\n",
    "\n",
    "    # For mortality: follow until death or end of observation\n",
    "    max_followup = 28 * 24  # 28 days in hours\n",
    "    survival_df['followup_death'] = np.where(\n",
    "        survival_df['death_event'] == 1,\n",
    "        survival_df['time_to_death'],\n",
    "        max_followup  # Censored at 28 days\n",
    "    )\n",
    "\n",
    "    # Cap follow-up times at reasonable limits\n",
    "    survival_df['followup_liberation'] = np.minimum(survival_df['followup_liberation'], max_followup)\n",
    "    survival_df['followup_death'] = np.minimum(survival_df['followup_death'], max_followup)\n",
    "\n",
    "    # Filter out invalid times\n",
    "    survival_df = survival_df[\n",
    "        (survival_df['followup_liberation'] > 0) &\n",
    "        (survival_df['followup_death'] > 0)\n",
    "    ].copy()\n",
    "\n",
    "    logger.info(f\"Created survival dataset with {len(survival_df)} patients\")\n",
    "    logger.info(f\"Liberation events: {survival_df['liberation_event'].sum()}\")\n",
    "    logger.info(f\"Death events: {survival_df['death_event'].sum()}\")\n",
    "\n",
    "    return survival_df\n",
    "\n",
    "survival_data = create_survival_dataset()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. KAPLAN-MEIER ANALYSIS: TIME TO CRRT LIBERATION\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"2. TIME TO CRRT LIBERATION ANALYSIS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "def km_liberation_analysis(df):\n",
    "    \"\"\"Kaplan-Meier analysis for time to CRRT liberation\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Overall liberation curve\n",
    "    kmf_overall = KaplanMeierFitter()\n",
    "    kmf_overall.fit(\n",
    "        durations=df['followup_liberation'],\n",
    "        event_observed=df['liberation_event'],\n",
    "        label='Overall'\n",
    "    )\n",
    "\n",
    "    results['overall'] = kmf_overall\n",
    "\n",
    "    # Liberation by ICU location\n",
    "    valid_locations = df['location_type'].value_counts()\n",
    "    valid_locations = valid_locations[valid_locations >= 10].index  # Minimum 10 patients\n",
    "\n",
    "    location_results = {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    colors = sns.color_palette(\"husl\", len(valid_locations))\n",
    "\n",
    "    for i, location in enumerate(valid_locations):\n",
    "        location_data = df[df['location_type'] == location]\n",
    "\n",
    "        if len(location_data) > 5:  # Minimum for meaningful analysis\n",
    "            kmf_loc = KaplanMeierFitter()\n",
    "            kmf_loc.fit(\n",
    "                durations=location_data['followup_liberation'],\n",
    "                event_observed=location_data['liberation_event'],\n",
    "                label=f'{location} (n={len(location_data)})'\n",
    "            )\n",
    "\n",
    "            location_results[location] = kmf_loc\n",
    "            kmf_loc.plot_survival_function(ax=ax, color=colors[i])\n",
    "\n",
    "    ax.set_title(f'{site_name}: Time to CRRT Liberation by ICU Location',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time from CRRT Initiation (hours)')\n",
    "    ax.set_ylabel('Probability of Remaining on CRRT')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/final/graphs/km_liberation_by_location.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    results['by_location'] = location_results\n",
    "\n",
    "    # Log-rank test for differences between locations\n",
    "    if len(location_results) > 1:\n",
    "        location_list = list(location_results.keys())\n",
    "        durations_list = []\n",
    "        events_list = []\n",
    "        groups_list = []\n",
    "\n",
    "        for loc in location_list:\n",
    "            loc_data = df[df['location_type'] == loc]\n",
    "            durations_list.extend(loc_data['followup_liberation'].tolist())\n",
    "            events_list.extend(loc_data['liberation_event'].tolist())\n",
    "            groups_list.extend([loc] * len(loc_data))\n",
    "\n",
    "        try:\n",
    "            logrank_result = multivariate_logrank_test(\n",
    "                durations_list, groups_list, events_list\n",
    "            )\n",
    "\n",
    "            logger.info(f\"\\nLog-rank test for liberation differences across ICU locations:\")\n",
    "            logger.info(f\"Chi-square statistic: {logrank_result.test_statistic:.4f}\")\n",
    "            logger.info(f\"p-value: {logrank_result.p_value:.6f}\")\n",
    "\n",
    "            results['logrank_liberation'] = {\n",
    "                'chi2': logrank_result.test_statistic,\n",
    "                'p_value': logrank_result.p_value\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Could not perform log-rank test: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "liberation_results = km_liberation_analysis(survival_data)\n",
    "\n",
    "# ============================================================================  \n",
    "# 3. KAPLAN-MEIER ANALYSIS: SURVIVAL (TIME TO DEATH)\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"3. SURVIVAL ANALYSIS\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "def km_survival_analysis(df):\n",
    "    \"\"\"Kaplan-Meier analysis for survival\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Overall survival curve\n",
    "    kmf_overall = KaplanMeierFitter()\n",
    "    kmf_overall.fit(\n",
    "        durations=df['followup_death'],\n",
    "        event_observed=df['death_event'],\n",
    "        label='Overall'\n",
    "    )\n",
    "\n",
    "    results['overall'] = kmf_overall\n",
    "\n",
    "    # Survival by ICU location\n",
    "    valid_locations = df['location_type'].value_counts()\n",
    "    valid_locations = valid_locations[valid_locations >= 10].index\n",
    "\n",
    "    location_results = {}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    colors = sns.color_palette(\"husl\", len(valid_locations))\n",
    "\n",
    "    for i, location in enumerate(valid_locations):\n",
    "        location_data = df[df['location_type'] == location]\n",
    "\n",
    "        if len(location_data) > 5:\n",
    "            kmf_loc = KaplanMeierFitter()\n",
    "            kmf_loc.fit(\n",
    "                durations=location_data['followup_death'],\n",
    "                event_observed=location_data['death_event'],\n",
    "                label=f'{location} (n={len(location_data)})'\n",
    "            )\n",
    "\n",
    "            location_results[location] = kmf_loc\n",
    "            kmf_loc.plot_survival_function(ax=ax, color=colors[i])\n",
    "\n",
    "    ax.set_title(f'{site_name}: Survival by ICU Location',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time from CRRT Initiation (hours)')\n",
    "    ax.set_ylabel('Survival Probability')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../output/final/graphs/km_survival_by_location.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    results['by_location'] = location_results\n",
    "\n",
    "    # Log-rank test for survival differences\n",
    "    if len(location_results) > 1:\n",
    "        location_list = list(location_results.keys())\n",
    "        durations_list = []\n",
    "        events_list = []\n",
    "        groups_list = []\n",
    "\n",
    "        for loc in location_list:\n",
    "            loc_data = df[df['location_type'] == loc]\n",
    "            durations_list.extend(loc_data['followup_death'].tolist())\n",
    "            events_list.extend(loc_data['death_event'].tolist())\n",
    "            groups_list.extend([loc] * len(loc_data))\n",
    "\n",
    "        try:\n",
    "            logrank_result = multivariate_logrank_test(\n",
    "                durations_list, groups_list, events_list\n",
    "            )\n",
    "\n",
    "            logger.info(f\"\\nLog-rank test for survival differences across ICU locations:\")\n",
    "            logger.info(f\"Chi-square statistic: {logrank_result.test_statistic:.4f}\")\n",
    "            logger.info(f\"p-value: {logrank_result.p_value:.6f}\")\n",
    "\n",
    "            results['logrank_survival'] = {\n",
    "                'chi2': logrank_result.test_statistic,\n",
    "                'p_value': logrank_result.p_value\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Could not perform log-rank test: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "survival_results = km_survival_analysis(survival_data)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. LANDMARK ANALYSIS: SURVIVAL BY EARLY LIBERATION STATUS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"4. LANDMARK ANALYSIS: EARLY LIBERATION vs SURVIVAL\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "def landmark_analysis(df, landmark_time=24):\n",
    "    \"\"\"Landmark analysis: survival by liberation status at landmark time\"\"\"\n",
    "\n",
    "    logger.info(f\"Landmark analysis at {landmark_time} hours...\")\n",
    "\n",
    "    # Only include patients who survived to landmark time\n",
    "    landmark_df = df[df['followup_death'] >= landmark_time].copy()\n",
    "\n",
    "    logger.info(f\"Patients surviving to {landmark_time}h landmark: {len(landmark_df)}\")\n",
    "\n",
    "    # Determine liberation status at landmark time\n",
    "    landmark_df['liberated_at_landmark'] = (\n",
    "        (landmark_df['followup_liberation'] <= landmark_time) &\n",
    "        (landmark_df['liberation_event'] == 1)\n",
    "    )\n",
    "\n",
    "    # Adjust follow-up time from landmark\n",
    "    landmark_df['followup_from_landmark'] = landmark_df['followup_death'] - landmark_time\n",
    "\n",
    "    # Death events remain the same (but follow-up time is adjusted)\n",
    "\n",
    "    lib_counts = landmark_df['liberated_at_landmark'].value_counts()\n",
    "    logger.info(f\"Liberation status at {landmark_time}h:\")\n",
    "    logger.info(f\"  Still on CRRT: {lib_counts.get(False, 0)}\")\n",
    "    logger.info(f\"  Liberated: {lib_counts.get(True, 0)}\")\n",
    "\n",
    "    # Kaplan-Meier by liberation status\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for lib_status in [False, True]:\n",
    "        status_data = landmark_df[landmark_df['liberated_at_landmark'] == lib_status]\n",
    "\n",
    "        if len(status_data) > 5:\n",
    "            label = f\"Liberated by {landmark_time}h (n={len(status_data)})\" if lib_status else f\"Still on CRRT at {landmark_time}h (n={len(status_data)})\"\n",
    "\n",
    "            kmf = KaplanMeierFitter()\n",
    "            kmf.fit(\n",
    "                durations=status_data['followup_from_landmark'],\n",
    "                event_observed=status_data['death_event'],\n",
    "                label=label\n",
    "            )\n",
    "\n",
    "            color = 'green' if lib_status else 'red'\n",
    "            kmf.plot_survival_function(ax=ax, color=color)\n",
    "\n",
    "    ax.set_title(f'{site_name}: Survival from {landmark_time}h by Early Liberation Status',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(f'Time from {landmark_time}h Landmark (hours)')\n",
    "    ax.set_ylabel('Survival Probability')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.figtext(0.5, 0.02, f'Landmark Analysis: Only includes patients who survived to {landmark_time} hours',\n",
    "                ha='center', fontsize=10, style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    plt.savefig(f'../output/final/graphs/landmark_analysis_{landmark_time}h.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Log-rank test for landmark analysis\n",
    "    if len(landmark_df[landmark_df['liberated_at_landmark'] == True]) > 5 and \\\n",
    "        len(landmark_df[landmark_df['liberated_at_landmark'] == False]) > 5:\n",
    "\n",
    "        liberated_data = landmark_df[landmark_df['liberated_at_landmark'] == True]\n",
    "        not_liberated_data = landmark_df[landmark_df['liberated_at_landmark'] == False]\n",
    "\n",
    "        try:\n",
    "            logrank_result = logrank_test(\n",
    "                liberated_data['followup_from_landmark'],\n",
    "                not_liberated_data['followup_from_landmark'],\n",
    "                liberated_data['death_event'],\n",
    "                not_liberated_data['death_event']\n",
    "            )\n",
    "\n",
    "            logger.info(f\"\\nLog-rank test for survival difference by {landmark_time}h liberation status:\")\n",
    "            logger.info(f\"Chi-square statistic: {logrank_result.test_statistic:.4f}\")\n",
    "            logger.info(f\"p-value: {logrank_result.p_value:.6f}\")\n",
    "\n",
    "            return {\n",
    "                'landmark_time': landmark_time,\n",
    "                'n_landmark_survivors': len(landmark_df),\n",
    "                'n_liberated': len(liberated_data),\n",
    "                'n_not_liberated': len(not_liberated_data),\n",
    "                'logrank_chi2': logrank_result.test_statistic,\n",
    "                'logrank_p': logrank_result.p_value\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Could not perform log-rank test: {e}\")\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# Perform landmark analyses at 24h and 72h\n",
    "landmark_24h = landmark_analysis(survival_data, landmark_time=24)\n",
    "landmark_72h = landmark_analysis(survival_data, landmark_time=72)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SUMMARY OF SURVIVAL ANALYSIS RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"5. SURVIVAL ANALYSIS SUMMARY\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Compile results for export\n",
    "km_results = {\n",
    "    'site_name': site_name,\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'n_patients': len(survival_data),\n",
    "    'n_liberation_events': int(survival_data['liberation_event'].sum()),\n",
    "    'n_death_events': int(survival_data['death_event'].sum()),\n",
    "    'liberation_analysis': {},\n",
    "    'survival_analysis': {},\n",
    "    'landmark_analysis': {}\n",
    "}\n",
    "\n",
    "# Add liberation results\n",
    "if 'logrank_liberation' in liberation_results:\n",
    "    km_results['liberation_analysis'] = {\n",
    "        'logrank_chi2': liberation_results['logrank_liberation']['chi2'],\n",
    "        'logrank_p': liberation_results['logrank_liberation']['p_value']\n",
    "    }\n",
    "\n",
    "# Add survival results  \n",
    "if 'logrank_survival' in survival_results:\n",
    "    km_results['survival_analysis'] = {\n",
    "        'logrank_chi2': survival_results['logrank_survival']['chi2'],\n",
    "        'logrank_p': survival_results['logrank_survival']['p_value']\n",
    "    }\n",
    "\n",
    "# Add landmark results\n",
    "if landmark_24h:\n",
    "    km_results['landmark_analysis']['24h'] = landmark_24h\n",
    "\n",
    "if landmark_72h:\n",
    "    km_results['landmark_analysis']['72h'] = landmark_72h\n",
    "\n",
    "# Export results\n",
    "import json\n",
    "with open(f'../output/final/km_analysis_{site_name.lower()}.json', 'w') as f:\n",
    "    def convert_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "\n",
    "    json_ready = json.loads(json.dumps(km_results, default=convert_types))\n",
    "    json.dump(json_ready, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "logger.info(f\"\\nKaplan-Meier Analysis Summary for {site_name}:\")\n",
    "logger.info(f\"Total patients analyzed: {len(survival_data)}\")\n",
    "logger.info(f\"Liberation events: {survival_data['liberation_event'].sum()}\")\n",
    "logger.info(f\"Death events: {survival_data['death_event'].sum()}\")\n",
    "\n",
    "if 'logrank_liberation' in liberation_results:\n",
    "    p_lib = liberation_results['logrank_liberation']['p_value']\n",
    "    logger.info(f\"Liberation differences by ICU location: p = {p_lib:.6f}\")\n",
    "\n",
    "if 'logrank_survival' in survival_results:\n",
    "    p_surv = survival_results['logrank_survival']['p_value']\n",
    "    logger.info(f\"Survival differences by ICU location: p = {p_surv:.6f}\")\n",
    "\n",
    "if landmark_24h:\n",
    "    p_land = landmark_24h['logrank_p']\n",
    "    logger.info(f\"24h landmark analysis (early liberation vs survival): p = {p_land:.6f}\")\n",
    "\n",
    "if landmark_72h:\n",
    "    p_land = landmark_72h['logrank_p']\n",
    "    logger.info(f\"72h landmark analysis (early liberation vs survival): p = {p_land:.6f}\")\n",
    "\n",
    "logger.info(f\"\\nFiles created:\")\n",
    "logger.info(f\"  - km_liberation_by_location.png\")\n",
    "logger.info(f\"  - km_survival_by_location.png\")\n",
    "logger.info(f\"  - landmark_analysis_24h.png\")\n",
    "logger.info(f\"  - landmark_analysis_72h.png\")\n",
    "logger.info(f\"  - km_analysis_{site_name.lower()}.json\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"KAPLAN-MEIER ANALYSIS COMPLETE\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Create combined dashboard figure\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Liberation curve\n",
    "plt.subplot(2, 2, 1)\n",
    "for location, kmf in liberation_results['by_location'].items():\n",
    "    kmf.plot_survival_function(label=f'{location}')\n",
    "plt.title('Time to CRRT Liberation by ICU Location')\n",
    "plt.xlabel('Time from CRRT Initiation (hours)')\n",
    "plt.ylabel('Probability of Remaining on CRRT')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Survival curve\n",
    "plt.subplot(2, 2, 2)\n",
    "for location, kmf in survival_results['by_location'].items():\n",
    "    kmf.plot_survival_function(label=f'{location}')\n",
    "plt.title('Survival by ICU Location')\n",
    "plt.xlabel('Time from CRRT Initiation (hours)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 24h landmark analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "landmark_df_24 = survival_data[survival_data['followup_death'] >= 24].copy()\n",
    "landmark_df_24['liberated_at_landmark'] = (\n",
    "    (landmark_df_24['followup_liberation'] <= 24) &\n",
    "    (landmark_df_24['liberation_event'] == 1)\n",
    ")\n",
    "landmark_df_24['followup_from_landmark'] = landmark_df_24['followup_death'] - 24\n",
    "\n",
    "for lib_status in [False, True]:\n",
    "    status_data = landmark_df_24[landmark_df_24['liberated_at_landmark'] == lib_status]\n",
    "    if len(status_data) > 5:\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(\n",
    "            durations=status_data['followup_from_landmark'],\n",
    "            event_observed=status_data['death_event'],\n",
    "            label=f\"{'Liberated' if lib_status else 'Still on CRRT'} at 24h\"\n",
    "        )\n",
    "        kmf.plot_survival_function()\n",
    "plt.title('24h Landmark Analysis')\n",
    "plt.xlabel('Time from 24h (hours)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# 72h landmark analysis\n",
    "plt.subplot(2, 2, 4)\n",
    "landmark_df_72 = survival_data[survival_data['followup_death'] >= 72].copy()\n",
    "landmark_df_72['liberated_at_landmark'] = (\n",
    "    (landmark_df_72['followup_liberation'] <= 72) &\n",
    "    (landmark_df_72['liberation_event'] == 1)\n",
    ")\n",
    "landmark_df_72['followup_from_landmark'] = landmark_df_72['followup_death'] - 72\n",
    "\n",
    "for lib_status in [False, True]:\n",
    "    status_data = landmark_df_72[landmark_df_72['liberated_at_landmark'] == lib_status]\n",
    "    if len(status_data) > 5:\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(\n",
    "            durations=status_data['followup_from_landmark'],\n",
    "            event_observed=status_data['death_event'],\n",
    "            label=f\"{'Liberated' if lib_status else 'Still on CRRT'} at 72h\"\n",
    "        )\n",
    "        kmf.plot_survival_function()\n",
    "plt.title('72h Landmark Analysis')\n",
    "plt.xlabel('Time from 72h (hours)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(f'{site_name}: Survival Analysis Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/final/graphs/survival_analysis_all.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99873ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORT SURVIVAL CURVE DATA FOR MULTI-SITE COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def export_survival_curves_for_comparison():\n",
    "    \"\"\"Export survival curve coordinates for multi-site plotting\"\"\"\n",
    "\n",
    "    curve_data = {}\n",
    "\n",
    "    # 1. Liberation curves by ICU\n",
    "    liberation_curves = {}\n",
    "    for location, kmf in liberation_results['by_location'].items():\n",
    "        liberation_curves[location] = {\n",
    "            'timeline': kmf.timeline.tolist(),\n",
    "            'survival_function': kmf.survival_function_.iloc[:, 0].tolist(),\n",
    "            'n_patients': int(kmf.durations.shape[0]),\n",
    "            'n_events': int(kmf.event_observed.sum())\n",
    "        }\n",
    "\n",
    "    curve_data['liberation_by_icu'] = liberation_curves\n",
    "\n",
    "    # 2. Survival curves by ICU  \n",
    "    survival_curves = {}\n",
    "    for location, kmf in survival_results['by_location'].items():\n",
    "        survival_curves[location] = {\n",
    "            'timeline': kmf.timeline.tolist(),\n",
    "            'survival_function': kmf.survival_function_.iloc[:, 0].tolist(),\n",
    "            'n_patients': int(kmf.durations.shape[0]),\n",
    "            'n_events': int(kmf.event_observed.sum())\n",
    "        }\n",
    "\n",
    "    curve_data['survival_by_icu'] = survival_curves\n",
    "\n",
    "    # 3. Landmark analysis curves (if available)\n",
    "    if landmark_72h:\n",
    "        # Get the landmark KM curves\n",
    "        landmark_df = survival_data[survival_data['followup_death'] >= 72].copy()\n",
    "        landmark_df['liberated_at_landmark'] = (\n",
    "            (landmark_df['followup_liberation'] <= 72) &\n",
    "            (landmark_df['liberation_event'] == 1)\n",
    "        )\n",
    "        landmark_df['followup_from_landmark'] = landmark_df['followup_death'] - 72\n",
    "\n",
    "        landmark_curves = {}\n",
    "\n",
    "        for lib_status in [False, True]:\n",
    "            status_data = landmark_df[landmark_df['liberated_at_landmark'] == lib_status]\n",
    "\n",
    "            if len(status_data) > 5:\n",
    "                kmf = KaplanMeierFitter()\n",
    "                kmf.fit(\n",
    "                    durations=status_data['followup_from_landmark'],\n",
    "                    event_observed=status_data['death_event']\n",
    "                )\n",
    "\n",
    "                status_label = ('liberated_by_72h' if lib_status \n",
    "                              else 'still_on_crrt_at_72h')\n",
    "                landmark_curves[status_label] = {\n",
    "                    'timeline': kmf.timeline.tolist(),\n",
    "                    'survival_function': kmf.survival_function_.iloc[:, 0].tolist(),\n",
    "                    'n_patients': int(len(status_data)),\n",
    "                    'n_events': int(status_data['death_event'].sum())\n",
    "                }\n",
    "\n",
    "        curve_data['landmark_72h'] = landmark_curves\n",
    "\n",
    "    # Save curve data\n",
    "    curve_data['site_name'] = site_name\n",
    "    curve_data['analysis_date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    with open(f'../output/final/km_curves_{site_name.lower()}.json', 'w') as f:\n",
    "        json.dump(curve_data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Exported survival curve data to km_curves_{site_name.lower()}.json\")\n",
    "\n",
    "    return curve_data\n",
    "\n",
    "# Export the curves\n",
    "curve_export = export_survival_curves_for_comparison()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY TABLE FOR MULTI-SITE COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def create_km_summary_table():\n",
    "    \"\"\"Create summary table of key KM metrics for comparison\"\"\"\n",
    "\n",
    "    # Calculate median survival times\n",
    "    icu_summaries = []\n",
    "\n",
    "    for location, kmf in liberation_results['by_location'].items():\n",
    "        try:\n",
    "            median_liberation = kmf.median_survival_time_\n",
    "            median_liberation = (median_liberation \n",
    "                               if not pd.isna(median_liberation) else None)\n",
    "        except:\n",
    "            median_liberation = None\n",
    "\n",
    "        liberation_rate_24h = (1 - kmf.survival_function_at_times(24).iloc[0] \n",
    "                             if 24 in kmf.timeline else None)\n",
    "        liberation_rate_72h = (1 - kmf.survival_function_at_times(72).iloc[0]\n",
    "                             if 72 in kmf.timeline else None)\n",
    "\n",
    "        icu_summaries.append({\n",
    "            'site': site_name,\n",
    "            'icu_type': location,\n",
    "            'n_patients': int(kmf.durations.shape[0]),\n",
    "            'n_liberation_events': int(kmf.event_observed.sum()),\n",
    "            'median_liberation_time_hrs': median_liberation,\n",
    "            'liberation_rate_24h': liberation_rate_24h,\n",
    "            'liberation_rate_72h': liberation_rate_72h\n",
    "        })\n",
    "\n",
    "    # Add survival data\n",
    "    for i, (location, kmf) in enumerate(survival_results['by_location'].items()):\n",
    "        try:\n",
    "            median_survival = kmf.median_survival_time_\n",
    "            median_survival = (median_survival \n",
    "                             if not pd.isna(median_survival) else None)\n",
    "        except:\n",
    "            median_survival = None\n",
    "\n",
    "        mortality_24h = (1 - kmf.survival_function_at_times(24).iloc[0]\n",
    "                        if 24 in kmf.timeline else None)\n",
    "        mortality_72h = (1 - kmf.survival_function_at_times(72).iloc[0]\n",
    "                        if 72 in kmf.timeline else None)\n",
    "\n",
    "        icu_summaries[i].update({\n",
    "            'n_death_events': int(kmf.event_observed.sum()),\n",
    "            'median_survival_time_hrs': median_survival,\n",
    "            'mortality_24h': mortality_24h,\n",
    "            'mortality_72h': mortality_72h\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(icu_summaries)\n",
    "    summary_df.to_csv(f'../output/final/km_summary_{site_name.lower()}.csv', index=False)\n",
    "\n",
    "    print(f\"✅ Created KM summary table: km_summary_{site_name.lower()}.csv\")\n",
    "    print(\"\\nSample of summary data:\")\n",
    "    print(summary_df[['icu_type', 'n_patients', 'liberation_rate_24h',\n",
    "                     'mortality_24h']].round(3))\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "km_summary = create_km_summary_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22801d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this diagnostic code to understand the exclusions\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='../output/final/km_analysis.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"=== DIAGNOSTIC ===\")\n",
    "\n",
    "# Start with original enc dataframe \n",
    "logger.info(f\"Original enc dataframe: {len(enc)} patients\")\n",
    "\n",
    "# Step 1: Check time calculations\n",
    "enc_with_times = enc.copy()\n",
    "\n",
    "# Calculate time to events (same as in function)\n",
    "enc_with_times['time_to_death'] = np.where(\n",
    "    enc_with_times['death_dttm_proxy'].notna(),\n",
    "    (enc_with_times['death_dttm_proxy'] - enc_with_times['first_crrt_time']).dt.total_seconds() / 3600,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "enc_with_times['time_to_liberation'] = (\n",
    "    enc_with_times['end_crrt_time'] - enc_with_times['first_crrt_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Check for negative or zero liberation times\n",
    "bad_liberation_times = (enc_with_times['time_to_liberation'] <= 0)\n",
    "logger.info(f\"Patients with bad liberation times (≤0): {bad_liberation_times.sum()}\")\n",
    "\n",
    "if bad_liberation_times.sum() > 0:\n",
    "    logger.info(\"Sample of bad liberation times: check enc_with_times[bad_liberation_times][['encounter_block', 'time_to_liberation', 'first_crrt_time', 'end_crrt_time']].head()\")\n",
    "    # logger.info(enc_with_times[bad_liberation_times][['encounter_block', 'time_to_liberation', 'first_crrt_time', 'end_crrt_time']].head())\n",
    "\n",
    "# Create follow-up times\n",
    "enc_with_times['death_event'] = enc_with_times['death_dttm_proxy'].notna().astype(int)\n",
    "\n",
    "enc_with_times['followup_liberation'] = np.where(\n",
    "    enc_with_times['death_event'] == 1,\n",
    "    enc_with_times['time_to_death'],\n",
    "    enc_with_times['time_to_liberation']\n",
    ")\n",
    "\n",
    "max_followup = 28 * 24\n",
    "enc_with_times['followup_death'] = np.where(\n",
    "    enc_with_times['death_event'] == 1,\n",
    "    enc_with_times['time_to_death'],\n",
    "    max_followup\n",
    ")\n",
    "\n",
    "# Cap follow-up times\n",
    "enc_with_times['followup_liberation'] = np.minimum(enc_with_times['followup_liberation'], max_followup)\n",
    "enc_with_times['followup_death'] = np.minimum(enc_with_times['followup_death'], max_followup)\n",
    "\n",
    "# Check for invalid follow-up times\n",
    "bad_followup_lib = (enc_with_times['followup_liberation'] <= 0)\n",
    "bad_followup_death = (enc_with_times['followup_death'] <= 0)\n",
    "\n",
    "logger.info(f\"Patients with bad liberation follow-up (≤0): {bad_followup_lib.sum()}\")\n",
    "logger.info(f\"Patients with bad death follow-up (≤0): {bad_followup_death.sum()}\")\n",
    "\n",
    "# Check location merge\n",
    "combined_summary = pd.read_csv('../output/intermediate/combined_summary.csv')\n",
    "post72_df = combined_summary[combined_summary['window'] == 'Post-72h'].set_index('encounter_block')\n",
    "\n",
    "logger.info(f\"Patients in post72 location data: {len(post72_df)}\")\n",
    "logger.info(f\"Patients in enc: {len(enc)}\")\n",
    "\n",
    "# Check merge success\n",
    "merge_result = enc.merge(post72_df[['location_type']], left_on='encounter_block', right_index=True, how='left')\n",
    "logger.info(f\"Patients after location merge: {len(merge_result)}\")\n",
    "\n",
    "# Final filter\n",
    "final_valid = (\n",
    "    (enc_with_times['followup_liberation'] > 0) &\n",
    "    (enc_with_times['followup_death'] > 0)\n",
    ")\n",
    "\n",
    "logger.info(f\"Patients passing final filter: {final_valid.sum()}\")\n",
    "logger.info(f\"Patients excluded by final filter: {(~final_valid).sum()}\")\n",
    "\n",
    "# Show which patients are excluded\n",
    "if (~final_valid).sum() > 0:\n",
    "    excluded = enc_with_times[~final_valid]\n",
    "    logger.info(\"\\nSample of excluded patients: check excluded[['encounter_block', 'followup_liberation', 'followup_death', 'time_to_liberation', 'time_to_death']].head() \")\n",
    "    # logger.info(excluded[['encounter_block', 'followup_liberation', 'followup_death', 'time_to_liberation', 'time_to_death']].head())\n",
    "\n",
    "    # Check if these are data errors\n",
    "    logger.info(\"\\nBreakdown of exclusion reasons:\")\n",
    "    logger.info(f\"- Bad liberation follow-up: {(excluded['followup_liberation'] <= 0).sum()}\")\n",
    "    logger.info(f\"- Bad death follow-up: {(excluded['followup_death'] <= 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841b7a9",
   "metadata": {},
   "source": [
    "# (I) Final HTML Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HTML TABLE GENERATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_html_table_with_styling(df, title, subtitle=\"\", highlight_significant=False, p_value_col=None):\n",
    "    \"\"\"Create a nicely formatted HTML table with custom styling\"\"\"\n",
    "\n",
    "    # Create HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>{title}</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                margin: 20px;\n",
    "                background-color: #f5f5f5;\n",
    "            }}\n",
    "            h1 {{\n",
    "                color: #333;\n",
    "                text-align: center;\n",
    "                margin-bottom: 5px;\n",
    "            }}\n",
    "            h3 {{\n",
    "                color: #666;\n",
    "                text-align: center;\n",
    "                margin-top: 5px;\n",
    "                font-weight: normal;\n",
    "            }}\n",
    "            table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                background-color: white;\n",
    "                box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                margin-top: 20px;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                text-align: left;\n",
    "                padding: 12px;\n",
    "                font-weight: bold;\n",
    "                position: sticky;\n",
    "                top: 0;\n",
    "                z-index: 10;\n",
    "            }}\n",
    "            td {{\n",
    "                border: 1px solid #ddd;\n",
    "                padding: 8px;\n",
    "                text-align: left;\n",
    "            }}\n",
    "            tr:nth-child(even) {{\n",
    "                background-color: #f9f9f9;\n",
    "            }}\n",
    "            tr:hover {{\n",
    "                background-color: #f5f5f5;\n",
    "            }}\n",
    "            .index-col {{\n",
    "                font-weight: bold;\n",
    "                background-color: #e8f5e9;\n",
    "                width: 250px;\n",
    "            }}\n",
    "            .header-row {{\n",
    "                background-color: #2196F3 !important;\n",
    "            }}\n",
    "            .count-row {{\n",
    "                background-color: #FFC107 !important;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .significant {{\n",
    "                background-color: #ffeb3b;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .p-value {{\n",
    "                text-align: center;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .group-header {{\n",
    "                background-color: #e0e0e0;\n",
    "                font-weight: bold;\n",
    "                font-style: italic;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>{title}</h1>\n",
    "        <h3>{subtitle}</h3>\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame to HTML with custom formatting\n",
    "    html_table = \"<table>\\n\"\n",
    "\n",
    "    # Add header row\n",
    "    html_table += \"<tr>\"\n",
    "    html_table += \"<th>Variable</th>\"\n",
    "    for col in df.columns:\n",
    "        html_table += f\"<th>{col}</th>\"\n",
    "    html_table += \"</tr>\\n\"\n",
    "\n",
    "    # Add data rows\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if this is a special row\n",
    "        row_class = \"\"\n",
    "        if idx == \"n\":\n",
    "            row_class = \"count-row\"\n",
    "        elif isinstance(idx, str) and idx.lower() == \"header\":\n",
    "            row_class = \"header-row\"\n",
    "\n",
    "        html_table += f\"<tr class='{row_class}'>\"\n",
    "\n",
    "        # Format index\n",
    "        if isinstance(idx, tuple):\n",
    "            # Handle multi-index\n",
    "            index_text = idx[0] if idx[1] == '' else f\"{idx[0]} - {idx[1]}\"\n",
    "        else:\n",
    "            index_text = str(idx)\n",
    "\n",
    "        # Check if this is a group header\n",
    "        if index_text.endswith(', n (%)') and row.isna().all():\n",
    "            html_table += f\"<td colspan='{len(df.columns)+1}' class='group-header'>{index_text}</td>\"\n",
    "        else:\n",
    "            html_table += f\"<td class='index-col'>{index_text}</td>\"\n",
    "\n",
    "            # Add data cells\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                cell_class = \"\"\n",
    "\n",
    "                # Highlight significant p-values\n",
    "                if highlight_significant and p_value_col and col == p_value_col:\n",
    "                    try:\n",
    "                        if cell_value != \"\" and cell_value != \"NA\":\n",
    "                            if cell_value == \"<0.001\" or (isinstance(cell_value, str) and float(cell_value) < 0.05):\n",
    "                                cell_class = \"significant p-value\"\n",
    "                            else:\n",
    "                                cell_class = \"p-value\"\n",
    "                    except:\n",
    "                        cell_class = \"p-value\"\n",
    "\n",
    "                # Handle NaN values\n",
    "                if pd.isna(cell_value):\n",
    "                    cell_value = \"\"\n",
    "\n",
    "                html_table += f\"<td class='{cell_class}'>{cell_value}</td>\"\n",
    "\n",
    "        html_table += \"</tr>\\n\"\n",
    "\n",
    "    html_table += \"</table>\"\n",
    "\n",
    "    # Close HTML\n",
    "    html_content += html_table\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    return html_content\n",
    "\n",
    "def save_tables_as_html():\n",
    "    \"\"\"Convert CSV tables to nicely formatted HTML\"\"\"\n",
    "\n",
    "    print(\"\\nGenerating HTML versions of tables...\")\n",
    "\n",
    "    # 1. Table 1 - All subcohorts\n",
    "    try:\n",
    "        table1_df = pd.read_csv(f'{output_folder}/final/table1_subgroups2_{pyCLIF.helper[\"site_name\"].lower()}.csv', index_col=0)\n",
    "\n",
    "        # Create subtitle with cohort definitions\n",
    "        subtitle1 = \"\"\"\n",
    "        Pre-24h: Baseline before CRRT | Post-24h: 0-24 hours after CRRT start | Post-72h: 24-72 hours after CRRT start<br>\n",
    "        Survivors: Alive at end of window | Non-survivors: Died within window | On/Off CRRT: Status at end of window\n",
    "        \"\"\"\n",
    "\n",
    "        html1 = create_html_table_with_styling(\n",
    "            table1_df,\n",
    "            f\"Table 1: Patient Characteristics by Time Window and Outcome - {pyCLIF.helper['site_name']}\",\n",
    "            subtitle=subtitle1\n",
    "        )\n",
    "\n",
    "        with open(f'{output_folder}/final/table1_subgroups2_{pyCLIF.helper[\"site_name\"].lower()}.html', 'w') as f:\n",
    "            f.write(html1)\n",
    "        print(\"  ✓ Saved table1_all_subcohorts.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating Table 1 HTML: {e}\")\n",
    "\n",
    "    # 2. Table 2 - By location with p-values\n",
    "    try:\n",
    "        table2_df = pd.read_csv(f'{output_folder}/final/table2_by_location_with_stats_{pyCLIF.helper[\"site_name\"].lower()}.csv', index_col=0)\n",
    "\n",
    "        subtitle2 = (\"Patient characteristics at 72 hours post-CRRT by ICU location type. P-values from ANOVA (continuous) or \"\n",
    "                    \"Chi-square (categorical) tests.\")\n",
    "\n",
    "        html2 = create_html_table_with_styling(\n",
    "            table2_df,\n",
    "            f\"Table 2: Patient Characteristics by ICU Location Type - {pyCLIF.helper['site_name']}\",\n",
    "            subtitle=subtitle2,\n",
    "            highlight_significant=True,\n",
    "            p_value_col='p_value'\n",
    "        )\n",
    "\n",
    "        with open(f'{output_folder}/final/table2_by_location_with_stats.html', 'w') as f:\n",
    "            f.write(html2)\n",
    "        print(\"  ✓ Saved table2_by_location_with_stats.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating Table 2 HTML: {e}\")\n",
    "\n",
    "    # 3. Create a combined HTML with both tables\n",
    "    try:\n",
    "        combined_html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>CRRT Epidemiology Tables - {pyCLIF.helper['site_name']}</title>\n",
    "            <style>\n",
    "                body {{\n",
    "                    font-family: Arial, sans-serif;\n",
    "                    margin: 20px;\n",
    "                    background-color: #f5f5f5;\n",
    "                }}\n",
    "                .table-container {{\n",
    "                    margin-bottom: 50px;\n",
    "                }}\n",
    "                h1 {{\n",
    "                    color: #333;\n",
    "                    text-align: center;\n",
    "                    margin-bottom: 5px;\n",
    "                }}\n",
    "                h2 {{\n",
    "                    color: #444;\n",
    "                    margin-top: 40px;\n",
    "                    border-bottom: 2px solid #4CAF50;\n",
    "                    padding-bottom: 10px;\n",
    "                }}\n",
    "                h3 {{\n",
    "                    color: #666;\n",
    "                    text-align: center;\n",
    "                    margin-top: 5px;\n",
    "                    font-weight: normal;\n",
    "                }}\n",
    "                table {{\n",
    "                    border-collapse: collapse;\n",
    "                    width: 100%;\n",
    "                    background-color: white;\n",
    "                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                    margin-top: 20px;\n",
    "                }}\n",
    "                th {{\n",
    "                    background-color: #4CAF50;\n",
    "                    color: white;\n",
    "                    text-align: left;\n",
    "                    padding: 12px;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "                td {{\n",
    "                    border: 1px solid #ddd;\n",
    "                    padding: 8px;\n",
    "                    text-align: left;\n",
    "                }}\n",
    "                tr:nth-child(even) {{\n",
    "                    background-color: #f9f9f9;\n",
    "                }}\n",
    "                tr:hover {{\n",
    "                    background-color: #f5f5f5;\n",
    "                }}\n",
    "                .index-col {{\n",
    "                    font-weight: bold;\n",
    "                    background-color: #e8f5e9;\n",
    "                    width: 250px;\n",
    "                }}\n",
    "                .count-row {{\n",
    "                    background-color: #FFC107 !important;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "                .significant {{\n",
    "                    background-color: #ffeb3b;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "                .p-value {{\n",
    "                    text-align: center;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>CRRT Epidemiology Analysis - {pyCLIF.helper['site_name']}</h1>\n",
    "            \n",
    "            <div class=\"table-container\">\n",
    "                <h2>Table 1: Patient Characteristics by Time Window and Outcome</h2>\n",
    "                <h3>Pre-24h: Baseline before CRRT | Post-24h: 0-24 hours after CRRT start | Post-72h: 24-72 hours after CRRT start<br>\n",
    "                Survivors: Alive at end of window | Non-survivors: Died within window | On/Off CRRT: Status at end of window</h3>\n",
    "                {table1_df.to_html(classes='styled-table', escape=False)}\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"table-container\">\n",
    "                <h2>Table 2: Patient Characteristics by ICU Location Type (72 hours post-CRRT)</h2>\n",
    "                <h3>P-values from ANOVA (continuous) or Chi-square (categorical) tests. Highlighted values indicate p < 0.05</h3>\n",
    "                {table2_df.to_html(classes='styled-table', escape=False)}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        with open(f'{output_folder}/final/all_tables_combined.html', 'w') as f:\n",
    "            f.write(combined_html)\n",
    "        print(\"  ✓ Saved all_tables_combined.html\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating combined HTML: {e}\")\n",
    "\n",
    "# Call the function to generate HTML tables\n",
    "save_tables_as_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".crrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

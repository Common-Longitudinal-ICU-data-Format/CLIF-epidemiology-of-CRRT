{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fd2e7e",
   "metadata": {},
   "source": [
    "## CRRT Cohort Check\n",
    "\n",
    "Required Checks for hospitalizations since 2021:\n",
    "\n",
    "1. Definition I  : Hospitalizations that are on a ventilator for the first 24 hours of their first icu stay.\n",
    "2. Definition II : Hospitalizations that are on vasoactive medications during the first 24 hrs of their first ICU stay. \n",
    "3. Definition III: Hospitalizations that have stage I AKI defined as \n",
    "\n",
    "    3a. 0.3 mg/dl absolute increase in serum creatinine over a 48 hour period since admission   \n",
    "\n",
    "    3b. 50% increase in serum creatinine over 7 days "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd6d30",
   "metadata": {},
   "source": [
    "## 00 Load libraries and core CLIF tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fa9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "\n",
    "import pyCLIF\n",
    "import pyCLIF_mimic\n",
    "import waterfall\n",
    "## import outlier json\n",
    "with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "    outlier_cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = pyCLIF.load_data('clif_patient')\n",
    "hospitalization = pyCLIF.load_data('clif_hospitalization')\n",
    "adt = pyCLIF.load_data('clif_adt')\n",
    "\n",
    "# ensure id variable is of dtype character\n",
    "hospitalization['hospitalization_id']= hospitalization['hospitalization_id'].astype(str)\n",
    "patient['patient_id']= patient['patient_id'].astype(str)\n",
    "adt['hospitalization_id']= adt['hospitalization_id'].astype(str)\n",
    "\n",
    "# check for duplicates\n",
    "# patient table should be unique by patient id\n",
    "patient = pyCLIF.remove_duplicates(patient, ['patient_id'], 'patient')\n",
    "# hospitalization table should be unique by hospitalization id\n",
    "hospitalization = pyCLIF.remove_duplicates(hospitalization, ['hospitalization_id'], 'hospitalization')\n",
    "# adt table should be unique by hospitalization id and in dttm\n",
    "adt = pyCLIF.remove_duplicates(adt, ['hospitalization_id', 'hospital_id', 'in_dttm'], 'adt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all _dttm variables to the same format\n",
    "patient = pyCLIF.convert_datetime_columns_to_site_tz(patient,  pyCLIF.helper['timezone'])\n",
    "hospitalization = pyCLIF.convert_datetime_columns_to_site_tz(hospitalization, pyCLIF.helper['timezone'])\n",
    "adt = pyCLIF.convert_datetime_columns_to_site_tz(adt,  pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abc0b3",
   "metadata": {},
   "source": [
    "#### Hospitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = hospitalization[(hospitalization['admission_dttm'].dt.year >= 2021) & \n",
    "                   (hospitalization['admission_dttm'].dt.year <= 2024) & \n",
    "                   (hospitalization['age_at_admission'] >=18)&\n",
    "                    (hospitalization['age_at_admission'] <=119)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}\n",
    "strobe_counts[\"A_adult_hospitalizations_since_2021\"] = len(cohort['hospitalization_id'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699b74a",
   "metadata": {},
   "source": [
    "#### ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71628f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert location category to lowercase and filter for ICU\n",
    "# Filter ADT table to include only hospitalizations from the cohort\n",
    "adt_cohort = adt[adt['hospitalization_id'].isin(cohort['hospitalization_id'])]\n",
    "adt_cohort['location_category'] = adt_cohort['location_category'].str.lower()\n",
    "# Filter to encounters that had at least one ICU stay\n",
    "icu_hospitalization_ids = adt_cohort[adt_cohort['location_category'] == 'icu']['hospitalization_id'].unique()\n",
    "adt_filtered = adt_cohort[adt_cohort['hospitalization_id'].isin(icu_hospitalization_ids)]\n",
    "strobe_counts[\"B_adult_hospitalizations_since_2021_with_icu\"] = len(adt_filtered['hospitalization_id'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a752490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = cohort[cohort['hospitalization_id'].isin(adt_filtered['hospitalization_id'])]\n",
    "print(\"Final list of cohort ids\", len(cohort['hospitalization_id'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a82569",
   "metadata": {},
   "source": [
    "# Hourly Scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798dd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) define the 'end_time' for the sequence from vitals or outcome.\n",
    "vitals_cohort = pyCLIF.load_data('clif_vitals',\n",
    "    filters={'hospitalization_id': cohort['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "vitals_cohort = pyCLIF.convert_datetime_columns_to_site_tz(vitals_cohort, pyCLIF.helper['timezone'])\n",
    "vitals_cohort = vitals_cohort.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "# Get first and last vitals timestamp for each hospitalization\n",
    "vital_bounds = (\n",
    "    vitals_cohort\n",
    "    .groupby('hospitalization_id')\n",
    "    .agg({\n",
    "        'recorded_dttm': ['min', 'max']\n",
    "    })\n",
    "    .droplevel(0, axis=1)\n",
    "    .rename(columns={'min': 'first_vital_dttm', 'max': 'last_vital_dttm'})\n",
    ")\n",
    "\n",
    "# Create hourly scaffold for each hospitalization\n",
    "hourly_scaffold = pd.DataFrame([\n",
    "    (hosp_id, time)\n",
    "    for hosp_id, start, end in zip(\n",
    "        vital_bounds.index,\n",
    "        vital_bounds['first_vital_dttm'],\n",
    "        vital_bounds['last_vital_dttm']\n",
    "    )\n",
    "    for time in pd.date_range(start=start, end=end, freq='H', tz=pyCLIF.helper['timezone'])\n",
    "], columns=['hospitalization_id', 'recorded_dttm'])\n",
    "\n",
    "# Add date and hour columns\n",
    "hourly_scaffold['recorded_date'] = hourly_scaffold['recorded_dttm'].dt.date\n",
    "hourly_scaffold['recorded_hour'] = hourly_scaffold['recorded_dttm'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb9fda",
   "metadata": {},
   "source": [
    "# Definition I\n",
    "\n",
    "Hospitalizations that are on a ventilator for the first 24 hours of their first icu stay.\n",
    "\n",
    "Notes: \n",
    "\n",
    "- Use ADT table to identify hospitalizations first ICU stay ; location_category.lower == \"icu\". Fields in ADT table = hospitalization_id, location_category, in_dttm, out_dttm\n",
    "- Use Respiratory Support table to identify the duration of ventilator for the first ICU stay. Use device_category.lower() == \"imv\" to identify those on vent. Other vars in the table- hospitalization_id, recorded_dttm, device_category, mode_category\n",
    "- Identify hospitalizations that were on vent for the first 24 hours of their first ICU stay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803432cb",
   "metadata": {},
   "source": [
    "#### First ICU Stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first ICU stay for each hospitalization \n",
    "# Convert location category to lowercase and filter for ICU\n",
    "icu_stays = adt_filtered[adt_filtered['location_category'] == 'icu'].copy()\n",
    "# Sort by hospitalization_id and in_dttm to get first ICU stay\n",
    "icu_stays = icu_stays.sort_values(['hospitalization_id', 'in_dttm', 'out_dttm'])\n",
    "# Get first ICU stay for each hospitalization\n",
    "first_icu_stays = icu_stays.groupby('hospitalization_id').first().reset_index()\n",
    "\n",
    "# Calculate duration of first ICU stay\n",
    "first_icu_stays['icu_duration_hours'] = (\n",
    "    first_icu_stays['out_dttm'] - first_icu_stays['in_dttm']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "first_icu_stays['icu_duration_days'] = first_icu_stays['icu_duration_hours'] / 24\n",
    "\n",
    "# Check how many have at least 24 hours (1 day) ICU stay\n",
    "icu_24h_plus = first_icu_stays[first_icu_stays['icu_duration_hours'] >= 24]\n",
    "print(f\"\\nHospitalizations with ICU stay ≥ 24 hours: {len(icu_24h_plus)} ({len(icu_24h_plus)/len(first_icu_stays)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating hourly_scaffold but BEFORE merging any clinical data\n",
    "# Add ICU admission information to ALL patients\n",
    "hourly_scaffold = hourly_scaffold.merge(\n",
    "    first_icu_stays[['hospitalization_id', 'in_dttm', 'out_dttm']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert to UTC first to avoid DST issues, then floor, then convert back\n",
    "def safe_floor_datetime(series, freq='S'):\n",
    "    \"\"\"Safely floor datetime series handling DST transitions\"\"\"\n",
    "    # Convert to UTC, floor, then back to original timezone\n",
    "    utc_series = series.dt.tz_convert('UTC')\n",
    "    floored_utc = utc_series.dt.floor(freq)\n",
    "    return floored_utc.dt.tz_convert(series.dt.tz)\n",
    "\n",
    "# Apply safe flooring\n",
    "hourly_scaffold['recorded_dttm'] = safe_floor_datetime(hourly_scaffold['recorded_dttm'])\n",
    "hourly_scaffold['in_dttm'] = safe_floor_datetime(hourly_scaffold['in_dttm'])\n",
    "hourly_scaffold['out_dttm'] = safe_floor_datetime(hourly_scaffold['out_dttm'])\n",
    "\n",
    "# Step 1: First, identify which hours each patient was ACTUALLY in ICU\n",
    "hourly_scaffold['in_icu'] = (\n",
    "    (hourly_scaffold['recorded_dttm'] >= hourly_scaffold['in_dttm']) &\n",
    "    (hourly_scaffold['recorded_dttm'] <= hourly_scaffold['out_dttm'])\n",
    ").astype(int)\n",
    "\n",
    "# Step 2: Calculate actual ICU duration for filtering\n",
    "hourly_scaffold['icu_duration_hours'] = (\n",
    "    hourly_scaffold['out_dttm'] - hourly_scaffold['in_dttm']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Step 3: Only for patients with ≥24h ICU stay, mark their first 24 hours\n",
    "hourly_scaffold['hours_since_icu_admission'] = (\n",
    "    hourly_scaffold['recorded_dttm'] - hourly_scaffold['in_dttm']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "hourly_scaffold['in_icu_24h'] = (\n",
    "    (hourly_scaffold['icu_duration_hours'] >= 24) &  # Patient stayed ≥24h in ICU\n",
    "    (hourly_scaffold['in_icu'] == 1) &               # Patient was in ICU at this hour\n",
    "    (hourly_scaffold['hours_since_icu_admission'] >= 0) &  # At or after ICU admission\n",
    "    (hourly_scaffold['hours_since_icu_admission'] < 24)    # Within first 24 hours\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c42f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many patients have in_icu_24h flag at all\n",
    "patients_with_flag = hourly_scaffold[hourly_scaffold['in_icu_24h'].notna()]['hospitalization_id'].nunique()\n",
    "print(f\"Patients with in_icu_24h flag: {patients_with_flag}\")\n",
    "\n",
    "# Check how many patients have in_icu_24h == 1\n",
    "patients_with_24h = hourly_scaffold[hourly_scaffold['in_icu_24h'] == 1]['hospitalization_id'].nunique()\n",
    "print(f\"Patients with in_icu_24h == 1: {patients_with_24h}\")\n",
    "\n",
    "strobe_counts[\"C_adult_hospitalizations_since_2021_with_icu_atleast_24hr\"] = hourly_scaffold[hourly_scaffold['in_icu_24h'] == 1]['hospitalization_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if icu_24h_plus exists and matches patients_with_24h\n",
    "try:\n",
    "    if len(icu_24h_plus) != patients_with_24h:\n",
    "        print(f\"WARNING: icu_24h_plus length ({len(icu_24h_plus)}) does not match patients_with_24h ({patients_with_24h})\")\n",
    "    else:\n",
    "        print(f\"✓ icu_24h_plus length matches patients_with_24h: {patients_with_24h}\")\n",
    "except NameError:\n",
    "    print(\"WARNING: Different aggregates for hospitalizations in ICU for 24 hrs. Check results/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed66b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_final = cohort.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a43bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join admission_dttm with hourly_scaffold\n",
    "hourly_scaffold = hourly_scaffold.merge(\n",
    "    hospitalization[['hospitalization_id', 'admission_dttm']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f565d8c",
   "metadata": {},
   "source": [
    "#### Respiratory Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf41d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load resp support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set'\n",
    "\n",
    "]\n",
    "\n",
    "# 1) Load respiratory support\n",
    "resp_support_raw = pyCLIF.load_data(\n",
    "    'clif_respiratory_support',\n",
    "    columns=rst_required_columns,\n",
    "    filters={'hospitalization_id': cohort_final['hospitalization_id'].unique().tolist()}\n",
    ")\n",
    "\n",
    "resp_support = resp_support_raw.copy()\n",
    "resp_support['device_category'] = resp_support['device_category'].str.lower()\n",
    "resp_support['mode_category'] = resp_support['mode_category'].str.lower()\n",
    "resp_support['lpm_set'] = pd.to_numeric(resp_support['lpm_set'], errors='coerce')\n",
    "resp_support['resp_rate_set'] = pd.to_numeric(resp_support['resp_rate_set'], errors='coerce')\n",
    "resp_support['peep_set'] = pd.to_numeric(resp_support['peep_set'], errors='coerce')\n",
    "resp_support['resp_rate_obs'] = pd.to_numeric(resp_support['resp_rate_obs'], errors='coerce')\n",
    "resp_support = resp_support.sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "# del resp_support_raw\n",
    "\n",
    "print(\"\\n=== Apply outlier thresholds ===\\n\")\n",
    "resp_support['fio2_set'] = pd.to_numeric(resp_support['fio2_set'], errors='coerce')\n",
    "# (Optional) If FiO2 is >1 on average => scale by /100\n",
    "fio2_mean = resp_support['fio2_set'].mean(skipna=True)\n",
    "# If the mean is greater than 1, divide 'fio2_set' by 100\n",
    "if fio2_mean and fio2_mean > 1.0:\n",
    "    # Only divide values greater than 1 to avoid re-dividing already correct values\n",
    "    resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] = \\\n",
    "        resp_support.loc[resp_support['fio2_set'] > 1, 'fio2_set'] / 100\n",
    "    print(\"Updated fio2_set to be between 0.21 and 1\")\n",
    "else:\n",
    "    print(\"FIO2_SET mean=\", fio2_mean, \"is within the required range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify encounters on IMV\n",
    "# Create mask to identify IMV entries\n",
    "imv_mask = resp_support['device_category'].str.contains(\"imv\", case=False, na=False)\n",
    "\n",
    "# Get unique hospitalization_ids with at least one IMV entry\n",
    "resp_stitched_imv_ids = resp_support[imv_mask][['hospitalization_id']].drop_duplicates()\n",
    "\n",
    "strobe_counts[\"D_adult_hospitalizations_since_2021_with_icu_imv\"] = len(resp_stitched_imv_ids['hospitalization_id'].drop_duplicates())\n",
    "# Filter the full table to just these hospitalization_ids\n",
    "resp_support_filtered = resp_support[\n",
    "    resp_support[\"hospitalization_id\"].isin(resp_stitched_imv_ids[\"hospitalization_id\"])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# filter down to only those hospitalization_ids that are in the cohort\n",
    "all_ids = cohort[cohort['hospitalization_id'].isin(resp_support_filtered['hospitalization_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ec386",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final list of ids adult_hospitalizations_since_2021_with_icu_imv\", len(all_ids['hospitalization_id'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "processed_resp_support = waterfall.process_resp_support_waterfall(resp_support_filtered, \n",
    "                                                        id_col = \"hospitalization_id\",\n",
    "                                                        verbose = True)\n",
    "\n",
    "processed_resp_support = pyCLIF.convert_datetime_columns_to_site_tz(processed_resp_support, pyCLIF.helper['timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vent_records = processed_resp_support.merge(\n",
    "    first_icu_stays[['hospitalization_id', 'in_dttm', 'out_dttm']], \n",
    "    on='hospitalization_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Create on_vent column (1 when device_category is IMV, 0 otherwise)\n",
    "vent_records['on_vent'] = (vent_records['device_category'].str.lower() == 'imv').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356507d1",
   "metadata": {},
   "source": [
    "While aggregating flags at the hourly level, I used the last value during that hour assuming the last value better represents the patient's status going into the next hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_hourly = vent_records[['hospitalization_id', 'recorded_dttm', 'on_vent']]\n",
    "\n",
    "# Create recorded_date and recorded_hour columns\n",
    "cohort_hourly['recorded_date'] = cohort_hourly['recorded_dttm'].dt.date\n",
    "cohort_hourly['recorded_hour'] = cohort_hourly['recorded_dttm'].dt.hour\n",
    "\n",
    "# Aggregate by hospitalization_id, recorded_date, and recorded_hour\n",
    "# First sort by time and get last value in hour, preserving the actual timestamp\n",
    "cohort_hourly_agg = (\n",
    "    cohort_hourly\n",
    "    .sort_values(['hospitalization_id', 'recorded_dttm'])\n",
    "    .groupby(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "    .agg({\n",
    "        'on_vent': 'last',        # Last vent status in hour\n",
    "    })\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = hourly_scaffold.merge(\n",
    "    cohort_hourly_agg,\n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  forward fill missing hours\n",
    "final_df = (\n",
    "    final_df\n",
    "    .set_index('hospitalization_id')\n",
    "    .groupby('hospitalization_id')\n",
    "    .ffill()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "final_df = (\n",
    "    final_df\n",
    "    .sort_values(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "    .groupby(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify hosp satisfying def 1\n",
    "def_1_status = (\n",
    "    final_df[final_df['in_icu_24h'] == 1]\n",
    "    .groupby('hospitalization_id')\n",
    "    .agg({'on_vent': 'min'})\n",
    "    .reset_index() \n",
    ")\n",
    "def_1_status['def_1'] = (def_1_status['on_vent'] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4337cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count patients with ventilation data in first 24h of ICU\n",
    "patients_with_vent_data_24h = int(def_1_status['on_vent'].notna().sum())\n",
    "strobe_counts[\"D1_patients_with_vent_data_in_first_24h_icu\"] = patients_with_vent_data_24h\n",
    "\n",
    "# Count patients meeting Definition 1 (on ventilator for first 24h)\n",
    "patients_meeting_def1 = int(def_1_status['def_1'].sum())\n",
    "strobe_counts[\"D2_patients_meeting_definition_1_24h_ventilation\"] = patients_meeting_def1\n",
    "\n",
    "# Count patients without ventilation data in first 24h\n",
    "patients_without_vent_data_24h = int(def_1_status['on_vent'].isna().sum())\n",
    "strobe_counts[\"D3_patients_without_vent_data_in_first_24h_icu\"] = patients_without_vent_data_24h\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n=== DEFINITION 1 SUMMARY ===\")\n",
    "print(f\"Total eligible patients (≥24h ICU): {len(def_1_status)}\")\n",
    "print(f\"With ventilation data in first 24h: {patients_with_vent_data_24h} ({patients_with_vent_data_24h/len(def_1_status)*100:.1f}%)\")\n",
    "print(f\"Without ventilation data in first 24h: {patients_without_vent_data_24h} ({patients_without_vent_data_24h/len(def_1_status)*100:.1f}%)\")\n",
    "print(f\"Meeting Definition 1 (24h ventilation): {patients_meeting_def1} ({patients_meeting_def1/len(def_1_status)*100:.1f}%)\")\n",
    "print(f\"Meeting Definition 1 among those with vent data: {patients_meeting_def1}/{patients_with_vent_data_24h} ({patients_meeting_def1/patients_with_vent_data_24h*100:.1f}%)\")\n",
    "\n",
    "# Verify totals\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"With vent data + Without vent data = {patients_with_vent_data_24h + patients_without_vent_data_24h} (should equal {len(def_1_status)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all hospitalization IDs from final_df\n",
    "all_hosp_ids = final_df['hospitalization_id'].drop_duplicates().reset_index(drop=True).to_frame()\n",
    "\n",
    "# Join with def_1_status and replace missing with 0\n",
    "all_defs = (\n",
    "    all_hosp_ids\n",
    "    .merge(def_1_status[['hospitalization_id', 'def_1']], on='hospitalization_id', how='left')\n",
    "    .fillna({'def_1': 0})\n",
    ")\n",
    "all_defs['def_1'] = all_defs['def_1'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57ee5b",
   "metadata": {},
   "source": [
    "# Definition II\n",
    "Hospitalizations that are on vasoactive medications during the first 24 hrs of their first ICU stay.\n",
    "\n",
    "\n",
    "#### Medication Admin Continuous\n",
    "\n",
    "- Filter down to the required meds and the cohort\n",
    "- Identify if any of these meds were administered continuously during that hour, and create a flag for each med at the hourly level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin','dobutamine'\n",
    "]\n",
    "\n",
    "meds_filters = {\n",
    "    'hospitalization_id': cohort_final['hospitalization_id'].unique().tolist(),\n",
    "    'med_category': meds_of_interest\n",
    "}\n",
    "meds = pyCLIF.load_data('clif_medication_admin_continuous', \n",
    "                        columns=meds_required_columns, \n",
    "                        filters=meds_filters)\n",
    "\n",
    "# ensure correct format\n",
    "meds['hospitalization_id']= meds['hospitalization_id'].astype(str)\n",
    "meds['med_dose_unit'] = meds['med_dose_unit'].str.lower()\n",
    "meds = pyCLIF.convert_datetime_columns_to_site_tz(meds,  pyCLIF.helper['timezone'])\n",
    "meds['med_dose'] = pd.to_numeric(meds['med_dose'], errors='coerce')\n",
    "# Create 'date' and 'hour_of_day' columns\n",
    "meds['recorded_date'] = meds['admin_dttm'].dt.date\n",
    "meds['recorded_hour'] = meds['admin_dttm'].dt.hour\n",
    "\n",
    "strobe_counts[\"E_adult_hospitalizations_since_2021_icu_meds\"] = len(meds['hospitalization_id'].drop_duplicates())\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter meds_filtered for the medications in red_meds_list\n",
    "meds_filtered = meds[meds['med_category'].isin(meds_of_interest)].copy()\n",
    "\n",
    "# Create a flag for each medication in red_meds_list\n",
    "for med in meds_of_interest:\n",
    "    # Create a flag that is 1 if the medication was administered in that hour, 0 otherwise\n",
    "    meds_filtered[med + '_flag'] = np.where((meds_filtered['med_category'] == med) & \n",
    "                                         (meds_filtered['med_dose'] > 0.0) & \n",
    "                                         (meds_filtered['med_dose'].notna()), 1, 0).astype(int)\n",
    "\n",
    "# Aggregate to get the maximum value for each flag (per hospitalization_id, recorded_date, recorded_hour)\n",
    "# This ensures that if the medication was administered even once in the hour, the flag is 1\n",
    "meds_flags = meds_filtered.groupby(['hospitalization_id', 'recorded_date', 'recorded_hour']).agg(\n",
    "    {med + '_flag': 'max' for med in meds_of_interest}\n",
    ").reset_index()\n",
    "\n",
    "#  combine all flags into a single 'red_meds_flag', you can do so like this:\n",
    "meds_flags['vasoactive_meds_flag'] = meds_flags[[med + '_flag' for med in meds_of_interest]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abe84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= final_df.merge(\n",
    "    meds_flags[['hospitalization_id', 'recorded_date', 'recorded_hour', 'vasoactive_meds_flag']],\n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill vasoactive_meds_flag within each hospitalization\n",
    "final_df = (\n",
    "    final_df\n",
    "    .sort_values(['hospitalization_id', 'recorded_date', 'recorded_hour'])\n",
    "    .assign(vasoactive_meds_flag=lambda x: x.groupby('hospitalization_id')['vasoactive_meds_flag'].ffill())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate def_2 flag for vasoactive medications in first 24h\n",
    "def_2_status = (\n",
    "    final_df[final_df['in_icu_24h'] == 1]  # Only look at records in first 24h\n",
    "    .groupby('hospitalization_id')\n",
    "    .agg({\n",
    "        'vasoactive_meds_flag': 'min'  # 1 if ALL hour in first 24h had vasoactive meds\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "def_2_status['def_2'] = (def_2_status['vasoactive_meds_flag'] == 1).astype(int)\n",
    "\n",
    "# Count hospitalizations with vasoactive meds data in first 24h of ICU\n",
    "strobe_counts['E1_patients_with_meds_data_in_first_24h_icu'] = int(def_2_status['vasoactive_meds_flag'].notna().sum())\n",
    "\n",
    "# Count hospitalizations meeting definition 2 (vasoactive meds for all 24h)\n",
    "strobe_counts['E2_patients_meeting_definition_2_24h_vasoactive_meds'] = int(def_2_status['def_2'].sum())\n",
    "\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_defs = all_defs.merge(def_2_status[['hospitalization_id', 'def_2']], \n",
    "                          on='hospitalization_id', \n",
    "                          how='left')\n",
    "all_defs['def_2'] = all_defs['def_2'].fillna(0)\n",
    "all_defs['def_2'] = all_defs['def_2'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94434772",
   "metadata": {},
   "source": [
    "# Definition III\n",
    "\n",
    "Hospitalizations that have stage I AKI defined as \n",
    "\n",
    "    3a. 0.3 mg/dl absolute increase in serum creatinine over a 48 hour period since admission   \n",
    "    3b. 50% increase in serum creatinine over 7 days\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Baseline creatinine is the first recorded creatinine value since hospital admission\n",
    "\n",
    "- The change in creatinine dose is calculated by comparing the baseline value to the max in a 48 hour time window for 3a, and a 7 day time window for 3b\n",
    "\n",
    "- This definition is satisfied when the patient satisfies either 3a or 3b or both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccde3e9",
   "metadata": {},
   "source": [
    "#### Labs- Creatinine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_name',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['creatinine']\n",
    "\n",
    "# Import labs\n",
    "labs_filters = {\n",
    "    'hospitalization_id': cohort_final['hospitalization_id'].unique().tolist(),\n",
    "    'lab_category': labs_of_interest\n",
    "}\n",
    "labs = pyCLIF.load_data('clif_labs', columns=labs_required_columns, filters=labs_filters)\n",
    "print(\"unique encounters in labs\", pyCLIF.count_unique_encounters(labs))\n",
    "labs['hospitalization_id']= labs['hospitalization_id'].astype(str)\n",
    "labs = labs.sort_values(by=['hospitalization_id', 'lab_result_dttm'])\n",
    "labs = pyCLIF.convert_datetime_columns_to_site_tz(labs, pyCLIF.helper['timezone'])\n",
    "labs['lab_value_numeric'] = pd.to_numeric(labs['lab_value_numeric'], errors='coerce')\n",
    "labs['recorded_hour'] = labs['lab_result_dttm'].dt.hour\n",
    "labs['recorded_date'] = labs['lab_result_dttm'].dt.date\n",
    "\n",
    "creatinine = labs[['hospitalization_id','recorded_date', 'recorded_hour', 'lab_value_numeric']]\n",
    "creatinine = creatinine.rename(columns={'lab_value_numeric': 'creatinine'})\n",
    "creatinine = creatinine.sort_values(by=['hospitalization_id', 'recorded_date', 'recorded_hour', 'creatinine'])\n",
    "\n",
    "strobe_counts[\"E_adult_hospitalizations_since_2021_icu_creatinine\"] = len(creatinine['hospitalization_id'].drop_duplicates())\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.merge(\n",
    "    creatinine,\n",
    "    on=['hospitalization_id', 'recorded_date', 'recorded_hour'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# AKI (Definition 3) using hospital admission anchor + window maxima\n",
    "# Baseline: first creatinine at/after admission_dttm\n",
    "# Maxima:   max creatinine within 0–48h and 0–7d after admission_dttm\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure consistent TZ & dtypes\n",
    "assert 'admission_dttm' in final_df.columns, \"final_df must contain admission_dttm\"\n",
    "assert pd.api.types.is_datetime64_any_dtype(final_df['admission_dttm']), \"admission_dttm must be datetime\"\n",
    "\n",
    "# One admission row per hospitalization\n",
    "admit = (final_df[['hospitalization_id','admission_dttm']]\n",
    "         .drop_duplicates('hospitalization_id', keep='first'))\n",
    "\n",
    "# Keep plausible creatinine, rename for clarity\n",
    "labs_cr = (labs.loc[\n",
    "              labs['lab_value_numeric'].between(0.1, 20.0, inclusive='both')\n",
    "           , ['hospitalization_id','lab_result_dttm','lab_value_numeric']]\n",
    "           .rename(columns={'lab_value_numeric':'creatinine'})\n",
    "           .copy())\n",
    "\n",
    "# Align labs to admission clock (hospital admission)\n",
    "labs_cr = labs_cr.merge(admit, on='hospitalization_id', how='inner')\n",
    "\n",
    "# Use only labs at/after hospital admission\n",
    "labs_cr = labs_cr[labs_cr['lab_result_dttm'] >= labs_cr['admission_dttm']].copy()\n",
    "\n",
    "# Baseline = FIRST value after admission\n",
    "labs_cr.sort_values(['hospitalization_id','lab_result_dttm'], inplace=True)\n",
    "baseline = (labs_cr.groupby('hospitalization_id', as_index=False)\n",
    "                  .first()[['hospitalization_id','creatinine']]\n",
    "                  .rename(columns={'creatinine':'baseline_creatinine'}))\n",
    "\n",
    "# Window membership (half-open intervals)\n",
    "labs_cr['in_48h'] = labs_cr['lab_result_dttm'] < (labs_cr['admission_dttm'] + pd.Timedelta(hours=48))\n",
    "labs_cr['in_7d']  = labs_cr['lab_result_dttm'] < (labs_cr['admission_dttm'] + pd.Timedelta(days=7))\n",
    "\n",
    "# Window maxima\n",
    "max48 = (labs_cr.loc[labs_cr['in_48h']]\n",
    "                .groupby('hospitalization_id', as_index=False)['creatinine']\n",
    "                .max()\n",
    "                .rename(columns={'creatinine':'peak_creatinine_48h'}))\n",
    "\n",
    "max7  = (labs_cr.loc[labs_cr['in_7d']]\n",
    "                .groupby('hospitalization_id', as_index=False)['creatinine']\n",
    "                .max()\n",
    "                .rename(columns={'creatinine':'peak_creatinine_7d'}))\n",
    "\n",
    "# Assemble per-encounter AKI table\n",
    "def_3_status = (admit[['hospitalization_id']]\n",
    "        .merge(baseline, on='hospitalization_id', how='left')\n",
    "        .merge(max48,   on='hospitalization_id', how='left')\n",
    "        .merge(max7,    on='hospitalization_id', how='left'))\n",
    "\n",
    "# Flags (NaNs -> 0)\n",
    "def_3_status['def_3a'] = (\n",
    "    (def_3_status['baseline_creatinine'].notna()) &\n",
    "    (def_3_status['peak_creatinine_48h'].notna()) &\n",
    "    ((def_3_status['peak_creatinine_48h'] - def_3_status['baseline_creatinine']) >= 0.3)\n",
    ").astype(int)\n",
    "\n",
    "def_3_status['def_3b'] = (\n",
    "    (def_3_status['baseline_creatinine'].notna()) &\n",
    "    (def_3_status['baseline_creatinine'] > 0) &\n",
    "    (def_3_status['peak_creatinine_7d'].notna()) &\n",
    "    ((def_3_status['peak_creatinine_7d'] / def_3_status['baseline_creatinine']) >= 1.5)\n",
    ").astype(int)\n",
    "\n",
    "def_3_status['def_3'] = ((def_3_status['def_3a'] == 1) | (def_3_status['def_3b'] == 1)).astype(int)\n",
    "\n",
    "# Merge back to final_df (dedup columns if present)\n",
    "final_df = final_df.drop(columns=[c for c in ['baseline_creatinine',\n",
    "                                              'peak_creatinine_48h',\n",
    "                                              'peak_creatinine_7d',\n",
    "                                              'def_3a','def_3b','def_3']\n",
    "                                  if c in final_df], errors='ignore')\n",
    "final_df = final_df.merge(\n",
    "    def_3_status[['hospitalization_id','baseline_creatinine',\n",
    "          'peak_creatinine_48h','peak_creatinine_7d','def_3a','def_3b','def_3']],\n",
    "    on='hospitalization_id', how='left'\n",
    ")\n",
    "\n",
    "# STROBE-style counts\n",
    "patients_with_baseline  = int(def_3_status['baseline_creatinine'].notna().sum())\n",
    "patients_with_48h_data  = int(def_3_status['peak_creatinine_48h'].notna().sum())\n",
    "patients_with_7d_data   = int(def_3_status['peak_creatinine_7d'].notna().sum())\n",
    "patients_meeting_def3a  = int(def_3_status['def_3a'].sum())\n",
    "patients_meeting_def3b  = int(def_3_status['def_3b'].sum())\n",
    "patients_meeting_def3   = int(def_3_status['def_3'].sum())\n",
    "\n",
    "print(\"\\n=== DEFINITION 3 RESULTS (hospital admission anchor) ===\")\n",
    "print(f\"Patients with baseline creatinine: {patients_with_baseline}\")\n",
    "print(f\"Patients with 48h creatinine data: {patients_with_48h_data}\")\n",
    "print(f\"Patients with 7d creatinine data : {patients_with_7d_data}\")\n",
    "print(f\"Meeting def_3a (0.3 mg/dL in 48h): {patients_meeting_def3a}\")\n",
    "print(f\"Meeting def_3b (50% in 7d)      : {patients_meeting_def3b}\")\n",
    "print(f\"Meeting def_3 (either)           : {patients_meeting_def3}\")\n",
    "\n",
    "# Update strobe counts dict (if you use it downstream)\n",
    "strobe_counts[\"F1_patients_with_baseline_creatinine\"] = patients_with_baseline\n",
    "strobe_counts[\"F2_patients_with_48h_creatinine_data\"] = patients_with_48h_data\n",
    "strobe_counts[\"F3_patients_with_7d_creatinine_data\"]  = patients_with_7d_data\n",
    "strobe_counts[\"F4_patients_meeting_definition_3a_48h_aki\"] = patients_meeting_def3a\n",
    "strobe_counts[\"F5_patients_meeting_definition_3b_7d_aki\"]  = patients_meeting_def3b\n",
    "strobe_counts[\"F6_patients_meeting_definition_3_aki\"]      = patients_meeting_def3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_defs = all_defs.merge(def_3_status[['hospitalization_id', 'def_3']], \n",
    "                          on='hospitalization_id', \n",
    "                          how='left')\n",
    "all_defs['def_3'] = all_defs['def_3'].fillna(0)\n",
    "all_defs['def_3'] = all_defs['def_3'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c79de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mortality variable from hospitalization discharge category\n",
    "mortality_data = hospitalization[['hospitalization_id', 'discharge_category']].copy()\n",
    "\n",
    "# Create mortality flag based on discharge category\n",
    "mortality_data['mortality'] = mortality_data['discharge_category'].str.lower().isin(['expired', 'hospice']).astype(int)\n",
    "\n",
    "# Join mortality data with all_defs\n",
    "all_defs = all_defs.merge(\n",
    "    mortality_data[['hospitalization_id', 'mortality']], \n",
    "    on='hospitalization_id', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " crrt_columns = [\n",
    "    'hospitalization_id', \n",
    "    'recorded_dttm',\n",
    "    'crrt_mode_name',\n",
    "    'crrt_mode_category',\n",
    "]\n",
    " crrt_df = pyCLIF.load_data(\n",
    "        'clif_crrt_therapy',\n",
    "        columns=crrt_columns,\n",
    "        filters={'hospitalization_id': cohort_final['hospitalization_id'].unique().tolist()}\n",
    "    )\n",
    "\n",
    "# Create CRRT flag - patients who have any CRRT record\n",
    "crrt_patients = crrt_df['hospitalization_id'].unique()\n",
    "crrt_flag = pd.DataFrame({\n",
    "    'hospitalization_id': all_defs['hospitalization_id'].unique()\n",
    "})\n",
    "crrt_flag['crrt'] = crrt_flag['hospitalization_id'].isin(crrt_patients).astype(int)\n",
    "\n",
    "# Join CRRT data with all_defs\n",
    "all_defs = all_defs.merge(\n",
    "    crrt_flag[['hospitalization_id', 'crrt']], \n",
    "    on='hospitalization_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "all_defs['crrt'] = all_defs['crrt'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45f7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc560ee0",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89073de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_indicators\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../output/final', exist_ok=True)\n",
    "\n",
    "# Create a summary dataframe with all definitions\n",
    "summary_df = all_defs[['hospitalization_id', 'def_1', 'def_2', 'def_3']].drop_duplicates()\n",
    "\n",
    "# Fill NaN values with 0 for the definitions\n",
    "summary_df = summary_df.fillna(0)\n",
    "\n",
    "# Convert to boolean for upset plot\n",
    "summary_df['def_1'] = summary_df['def_1'].astype(bool)\n",
    "summary_df['def_2'] = summary_df['def_2'].astype(bool) \n",
    "summary_df['def_3'] = summary_df['def_3'].astype(bool)\n",
    "\n",
    "# Create upset plot with better sizing\n",
    "fig = plt.figure(figsize=(16, 12))  # Larger figure size\n",
    "upset_data = from_indicators(['def_1', 'def_2', 'def_3'], \n",
    "                           data=summary_df.set_index('hospitalization_id'))\n",
    "\n",
    "upset = UpSet(upset_data, \n",
    "              subset_size='count',\n",
    "              show_counts=True,\n",
    "              sort_by='cardinality',\n",
    "              element_size=50,  # Larger dots\n",
    "              with_lines=True)  # Add connecting lines for clarity\n",
    "\n",
    "# Plot with custom spacing\n",
    "upset.plot(fig=fig)\n",
    "\n",
    "# Adjust spacing to prevent overlapping\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, right=0.95, top=0.85, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Add title with more space\n",
    "plt.suptitle('Clinical Definition Intersections', \n",
    "             fontsize=16, y=0.95)\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "for ax in fig.get_axes():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                 ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('../output/final/definition_overlap_upset_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create detailed summary table\n",
    "combinations = []\n",
    "\n",
    "# Individual definitions\n",
    "combinations.append({\n",
    "    'Combination': 'def_1 only',\n",
    "    'Description': '24h ventilation only',\n",
    "    'Count': int(((summary_df['def_1']) & (~summary_df['def_2']) & (~summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "combinations.append({\n",
    "    'Combination': 'def_2 only', \n",
    "    'Description': '24h vasoactives only',\n",
    "    'Count': int(((~summary_df['def_1']) & (summary_df['def_2']) & (~summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "combinations.append({\n",
    "    'Combination': 'def_3 only',\n",
    "    'Description': 'AKI only', \n",
    "    'Count': int(((~summary_df['def_1']) & (~summary_df['def_2']) & (summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "# Pairwise combinations\n",
    "combinations.append({\n",
    "    'Combination': 'def_1 & def_2',\n",
    "    'Description': '24h ventilation + 24h vasoactives',\n",
    "    'Count': int(((summary_df['def_1']) & (summary_df['def_2']) & (~summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "combinations.append({\n",
    "    'Combination': 'def_1 & def_3',\n",
    "    'Description': '24h ventilation + AKI',\n",
    "    'Count': int(((summary_df['def_1']) & (~summary_df['def_2']) & (summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "combinations.append({\n",
    "    'Combination': 'def_2 & def_3', \n",
    "    'Description': '24h vasoactives + AKI',\n",
    "    'Count': int(((~summary_df['def_1']) & (summary_df['def_2']) & (summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "# All three\n",
    "combinations.append({\n",
    "    'Combination': 'def_1 & def_2 & def_3',\n",
    "    'Description': 'All three conditions',\n",
    "    'Count': int(((summary_df['def_1']) & (summary_df['def_2']) & (summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "# None\n",
    "combinations.append({\n",
    "    'Combination': 'None',\n",
    "    'Description': 'No conditions met',\n",
    "    'Count': int(((~summary_df['def_1']) & (~summary_df['def_2']) & (~summary_df['def_3'])).sum())\n",
    "})\n",
    "\n",
    "# Create summary table\n",
    "combo_df = pd.DataFrame(combinations)\n",
    "combo_df['Percentage'] = (combo_df['Count'] / len(summary_df) * 100).round(1)\n",
    "\n",
    "print(\"Summary Table of Definition Combinations:\")\n",
    "print(\"=\" * 60)\n",
    "print(combo_df.to_string(index=False))\n",
    "\n",
    "# Also show totals for each individual definition\n",
    "individual_totals = {\n",
    "    'def_1_24h_ventilation_count': int(summary_df['def_1'].sum()),\n",
    "    'def_1_24h_ventilation_percentage': float(summary_df['def_1'].mean()*100),\n",
    "    'def_2_24h_vasoactives_count': int(summary_df['def_2'].sum()),\n",
    "    'def_2_24h_vasoactives_percentage': float(summary_df['def_2'].mean()*100),\n",
    "    'def_3_aki_count': int(summary_df['def_3'].sum()),\n",
    "    'def_3_aki_percentage': float(summary_df['def_3'].mean()*100),\n",
    "    'total_hospitalizations': int(len(summary_df))\n",
    "}\n",
    "\n",
    "print(\"\\n\\nIndividual Definition Totals:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"def_1 (24h ventilation): {individual_totals['def_1_24h_ventilation_count']} ({individual_totals['def_1_24h_ventilation_percentage']:.1f}%)\")\n",
    "print(f\"def_2 (24h vasoactives): {individual_totals['def_2_24h_vasoactives_count']} ({individual_totals['def_2_24h_vasoactives_percentage']:.1f}%)\")\n",
    "print(f\"def_3 (AKI): {individual_totals['def_3_aki_count']} ({individual_totals['def_3_aki_percentage']:.1f}%)\")\n",
    "print(f\"Total hospitalizations: {individual_totals['total_hospitalizations']}\")\n",
    "\n",
    "# Save the summary tables\n",
    "combo_df.to_csv('../output/final/definition_combinations_summary.csv', index=False)\n",
    "\n",
    "# Save individual totals as JSON for easy reading\n",
    "import json\n",
    "with open('../output/final/individual_definition_totals.json', 'w') as f:\n",
    "    json.dump(individual_totals, f, indent=2)\n",
    "\n",
    "# Save the final strobe counts\n",
    "with open('../output/final/strobe_counts.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    strobe_counts_serializable = {}\n",
    "    for key, value in strobe_counts.items():\n",
    "        if hasattr(value, 'item'):  # numpy scalar\n",
    "            strobe_counts_serializable[key] = value.item()\n",
    "        elif isinstance(value, (np.integer, np.int64, np.int32)):\n",
    "            strobe_counts_serializable[key] = int(value)\n",
    "        elif isinstance(value, (np.floating, np.float64, np.float32)):\n",
    "            strobe_counts_serializable[key] = float(value)\n",
    "        else:\n",
    "            strobe_counts_serializable[key] = value\n",
    "    json.dump(strobe_counts_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\n\\nFiles saved to output/final/:\")\n",
    "print(\"- definition_overlap_upset_plot.png\")\n",
    "print(\"- definition_combinations_summary.csv\")\n",
    "print(\"- individual_definition_totals.json\")\n",
    "print(\"- strobe_counts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48101568",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = summary_df['def_1'].astype(bool)\n",
    "B = summary_df['def_2'].astype(bool)\n",
    "C = summary_df['def_3'].astype(bool)\n",
    "\n",
    "# EXCLUSIVE regions (match the UpSet bars)\n",
    "excl = {\n",
    "    '1_only':  int(( A & ~B & ~C).sum()),\n",
    "    '2_only':  int((~A &  B & ~C).sum()),\n",
    "    '3_only':  int((~A & ~B &  C).sum()),\n",
    "    '1&2_only':int(( A &  B & ~C).sum()),\n",
    "    '1&3_only':int(( A & ~B &  C).sum()),\n",
    "    '2&3_only':int((~A &  B &  C).sum()),\n",
    "    'all3':    int(( A &  B &  C).sum()),\n",
    "    'none':    int((~A & ~B & ~C).sum())\n",
    "}\n",
    "\n",
    "# INCLUSIVE pairs (what people intuitively compare to all3)\n",
    "incl = {\n",
    "    '1∩2_incl': excl['1&2_only'] + excl['all3'],\n",
    "    '1∩3_incl': excl['1&3_only'] + excl['all3'],\n",
    "    '2∩3_incl': excl['2&3_only'] + excl['all3'],\n",
    "    'all3':     excl['all3'],\n",
    "    '|1|':      int(A.sum()),\n",
    "    '|2|':      int(B.sum()),\n",
    "    '|3|':      int(C.sum())\n",
    "}\n",
    "\n",
    "# Sanity checks (assert won’t raise if all good)\n",
    "assert sum(excl.values()) == len(summary_df)             # exclusive bins partition N\n",
    "assert incl['all3'] <= incl['1∩2_incl']                  # all3 ≤ each inclusive pair\n",
    "assert incl['all3'] <= incl['1∩3_incl']\n",
    "assert incl['all3'] <= incl['2∩3_incl']\n",
    "assert excl['1&2_only'] <= incl['|1|'] and excl['1&2_only'] <= incl['|2|']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960c6cc",
   "metadata": {},
   "source": [
    "# Venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f400e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "import matplotlib.pyplot as plt\n",
    "A = summary_df['def_1'].astype(bool)\n",
    "B = summary_df['def_2'].astype(bool)\n",
    "C = summary_df['def_3'].astype(bool)\n",
    "\n",
    "subsets = (\n",
    "    int((A & ~B & ~C).sum()),   # 100\n",
    "    int((~A & B & ~C).sum()),   # 010\n",
    "    int((A & B & ~C).sum()),    # 110\n",
    "    int((~A & ~B & C).sum()),   # 001\n",
    "    int((A & ~B & C).sum()),    # 101\n",
    "    int((~A & B & C).sum()),    # 011\n",
    "    int((A & B & C).sum()),     # 111\n",
    ")\n",
    "venn3(subsets=subsets, set_labels=('def_1: 24h vent','def_2: 24h vaso','def_3: AKI'))\n",
    "plt.savefig('../output/final/venn.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb6b68",
   "metadata": {},
   "source": [
    "# CONSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import numpy as np\n",
    "\n",
    "def create_consort_diagram(strobe_counts):\n",
    "    \"\"\"\n",
    "    Create a CONSORT flow diagram using the strobe_counts data with properly spaced arrows\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create figure and axis with equal margins on all sides\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    ax.set_xlim(-1, 11)  # Extended left and right margins\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Box styling - all white with black borders\n",
    "    box_style = \"round,pad=0.1\"\n",
    "    \n",
    "    # Track all box positions for arrow drawing\n",
    "    boxes = {}\n",
    "    \n",
    "    # Helper function to create boxes\n",
    "    def create_box(x, y, width, height, text, box_id=None, fontsize=10, fontweight='normal'):\n",
    "        # Create fancy box - all white\n",
    "        box = FancyBboxPatch(\n",
    "            (x - width/2, y - height/2), width, height,\n",
    "            boxstyle=box_style,\n",
    "            facecolor='white',\n",
    "            edgecolor='black',\n",
    "            linewidth=1.5\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(x, y, text, ha='center', va='center', \n",
    "                fontsize=fontsize, fontweight=fontweight, wrap=True)\n",
    "        \n",
    "        # Store box boundaries\n",
    "        box_info = {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'left': x - width/2,\n",
    "            'right': x + width/2,\n",
    "            'top': y + height/2,\n",
    "            'bottom': y - height/2\n",
    "        }\n",
    "        \n",
    "        if box_id:\n",
    "            boxes[box_id] = box_info\n",
    "        \n",
    "        return box_info\n",
    "    \n",
    "    # Helper function to create arrows with proper gaps\n",
    "    def create_arrow(from_box, to_box, from_point='bottom_center', to_point='top_center', style='->', lw=2):\n",
    "        # Define connection points with gaps\n",
    "        gap = 0.1  # Gap between arrow and box\n",
    "        \n",
    "        # Calculate from coordinates\n",
    "        if from_point == 'bottom_center':\n",
    "            x1 = from_box['x']\n",
    "            y1 = from_box['bottom'] - gap\n",
    "        elif from_point == 'bottom_left':\n",
    "            x1 = from_box['x'] - from_box['width'] * 0.2\n",
    "            y1 = from_box['bottom'] - gap\n",
    "        elif from_point == 'bottom_right':\n",
    "            x1 = from_box['x'] + from_box['width'] * 0.2\n",
    "            y1 = from_box['bottom'] - gap\n",
    "        elif from_point == 'right':\n",
    "            x1 = from_box['right'] + gap\n",
    "            y1 = from_box['y']\n",
    "        \n",
    "        # Calculate to coordinates\n",
    "        if to_point == 'top_center':\n",
    "            x2 = to_box['x']\n",
    "            y2 = to_box['top'] + gap\n",
    "        elif to_point == 'left':\n",
    "            x2 = to_box['left'] - gap\n",
    "            y2 = to_box['y']\n",
    "        \n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                   arrowprops=dict(arrowstyle=style, lw=lw, color='black'))\n",
    "    \n",
    "    # Main title\n",
    "    ax.text(5, 11.5, 'CONSORT Flow Diagram: Adult Hospitalizations Since 2021', \n",
    "            ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Define consistent box height\n",
    "    box_height = 0.7\n",
    "    \n",
    "    # Level 1: Total Adult Hospitalizations\n",
    "    box1 = create_box(5, 10.3, 3, box_height, \n",
    "               f\"Total Adult Hospitalizations\\nn = {strobe_counts['A_adult_hospitalizations_since_2021']:,}\",\n",
    "               'total', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Level 2: ICU Admissions - with more vertical space\n",
    "    box2 = create_box(5, 8.8, 3, box_height,\n",
    "               f\"ICU Admissions\\nn = {strobe_counts['B_adult_hospitalizations_since_2021_with_icu']:,}\",\n",
    "               'icu', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Arrow from Level 1 to Level 2\n",
    "    create_arrow(box1, box2)\n",
    "    \n",
    "    # Level 3: Three branches - with more vertical space\n",
    "    box3_left = create_box(2, 7.2, 2.2, box_height,\n",
    "               f\"ICU with IMV\\nn = {strobe_counts['D_adult_hospitalizations_since_2021_with_icu_imv']:,}\",\n",
    "               'imv', fontsize=10)\n",
    "    \n",
    "    box3_center = create_box(5, 7.2, 2.2, box_height,\n",
    "               f\"ICU with Medications\\nn = {strobe_counts['E_adult_hospitalizations_since_2021_icu_meds']:,}\",\n",
    "               'meds', fontsize=10)\n",
    "    \n",
    "    box3_right = create_box(8, 7.2, 2.2, box_height,\n",
    "               f\"ICU with Creatinine\\nn = {strobe_counts['E_adult_hospitalizations_since_2021_icu_creatinine']:,}\",\n",
    "               'creat', fontsize=10)\n",
    "    \n",
    "    # Branching arrows from ICU to three branches\n",
    "    create_arrow(box2, box3_left)\n",
    "    create_arrow(box2, box3_center)\n",
    "    create_arrow(box2, box3_right)\n",
    "    \n",
    "    # Level 4: Definitions - with more vertical space\n",
    "    box4_left = create_box(2, 5.5, 2.2, box_height + 0.1,\n",
    "               f\"Definition 1\\nOn vent for first 24 hrs\\nof first ICU stay\\nn = {strobe_counts['D2_patients_meeting_definition_1_24h_ventilation']:,}\",\n",
    "               'def1', fontsize=9)\n",
    "    \n",
    "    box4_center = create_box(5, 5.5, 2.2, box_height + 0.1,\n",
    "               f\"Definition 2\\nVasoactive meds in\\nfirst 24h\\nn = {strobe_counts['E2_patients_meeting_definition_2_24h_vasoactive_meds']:,}\",\n",
    "               'def2', fontsize=9)\n",
    "    \n",
    "    box4_right = create_box(8, 5.5, 2.2, box_height + 0.1,\n",
    "               f\"Definition 3\\nCreatinine criteria\\n(Either 3a or 3b)\\nn = {strobe_counts['F6_patients_meeting_definition_3_aki']:,}\",\n",
    "               'def3', fontsize=9)\n",
    "    \n",
    "    # Arrows from Level 3 to Level 4\n",
    "    create_arrow(box3_left, box4_left)\n",
    "    create_arrow(box3_center, box4_center)\n",
    "    create_arrow(box3_right, box4_right)\n",
    "    \n",
    "    # Level 5: Sub-definitions for Definition 3 - with more vertical space\n",
    "    box5_left = create_box(6.5, 3.7, 2.2, box_height + 0.1,\n",
    "               f\"Definition 3a\\n0.3 mg/dl increase\\nin 48h\\nn = {strobe_counts['F4_patients_meeting_definition_3a_48h_aki']:,}\",\n",
    "               'def3a', fontsize=9)\n",
    "    \n",
    "    box5_right = create_box(9.5, 3.7, 2.2, box_height + 0.1,\n",
    "               f\"Definition 3b\\n50% increase\\nin 7d\\nn = {strobe_counts['F5_patients_meeting_definition_3b_7d_aki']:,}\",\n",
    "               'def3b', fontsize=9)\n",
    "    \n",
    "    # Arrows from Definition 3 to sub-definitions (split from bottom)\n",
    "    create_arrow(box4_right, box5_left, from_point='bottom_left')\n",
    "    create_arrow(box4_right, box5_right, from_point='bottom_right')\n",
    "    \n",
    "    # Add exclusion counts box - adjusted position to fit within margins\n",
    "    excluded_icu = strobe_counts['A_adult_hospitalizations_since_2021'] - strobe_counts['B_adult_hospitalizations_since_2021_with_icu']\n",
    "    \n",
    "    exclusion_box = create_box(8.5, 10.3, 1.8, 0.5,\n",
    "                               f'Excluded: No ICU\\nn = {excluded_icu:,}',\n",
    "                               'exclusion', fontsize=9)\n",
    "    \n",
    "    # Arrow to exclusion box\n",
    "    create_arrow(box1, exclusion_box, from_point='right', to_point='left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Create the diagram\n",
    "fig, ax = create_consort_diagram(strobe_counts)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../output/final/consort_flow_diagram_clean.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Clean CONSORT Flow Diagram created and saved to:\")\n",
    "print(\"- ../output/final/consort_flow_diagram_clean.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778be49",
   "metadata": {},
   "source": [
    "# TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# TABLE ONE: Patient Characteristics by Definition Groups (Enhanced)\n",
    "# ========================================\n",
    "\n",
    "print(\"Creating Enhanced Table One...\")\n",
    "\n",
    "# Create patient-level summary with definitions from all_defs (including mortality and CRRT)\n",
    "patient_summary = all_defs[['hospitalization_id', 'def_1', 'def_2', 'def_3', 'mortality', 'crrt']].copy()\n",
    "patient_summary = patient_summary.fillna(0)\n",
    "\n",
    "# Merge with hospitalization data for age and outcome\n",
    "patient_demo = patient_summary.merge(\n",
    "    hospitalization[['hospitalization_id', 'patient_id', 'age_at_admission']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with patient data for sex, race, ethnicity\n",
    "patient_demo = patient_demo.merge(\n",
    "    patient[['patient_id', 'sex_category', 'race_category', 'ethnicity_category']],\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Get ICU length of stay from first_icu_stays\n",
    "icu_los_summary = first_icu_stays[['hospitalization_id', 'icu_duration_hours', 'icu_duration_days']].copy()\n",
    "patient_demo = patient_demo.merge(icu_los_summary, on='hospitalization_id', how='left')\n",
    "\n",
    "# Get baseline creatinine for each patient (from final_df if available)\n",
    "if 'baseline_creatinine' in final_df.columns:\n",
    "    baseline_creat_summary = final_df[['hospitalization_id', 'baseline_creatinine']].drop_duplicates()\n",
    "    patient_demo = patient_demo.merge(baseline_creat_summary, on='hospitalization_id', how='left')\n",
    "else:\n",
    "    patient_demo['baseline_creatinine'] = np.nan\n",
    "\n",
    "# Get medication flags - any vasoactive med use during stay (from final_df if available)\n",
    "if 'vasoactive_meds_flag' in final_df.columns:\n",
    "    med_summary = final_df[['hospitalization_id', 'vasoactive_meds_flag']].groupby('hospitalization_id').max().reset_index()\n",
    "    patient_demo = patient_demo.merge(med_summary, on='hospitalization_id', how='left')\n",
    "else:\n",
    "    patient_demo['vasoactive_meds_flag'] = 0\n",
    "\n",
    "# Create definition groups for comparison\n",
    "def create_def_groups(row):\n",
    "    if row['def_1'] == 1 and row['def_2'] == 1 and row['def_3'] == 1:\n",
    "        return 'All three (def_1+2+3)'\n",
    "    elif row['def_1'] == 1 and row['def_2'] == 1:\n",
    "        return 'Vent + Vaso (def_1+2)'\n",
    "    elif row['def_1'] == 1 and row['def_3'] == 1:\n",
    "        return 'Vent + AKI (def_1+3)'\n",
    "    elif row['def_2'] == 1 and row['def_3'] == 1:\n",
    "        return 'Vaso + AKI (def_2+3)'\n",
    "    elif row['def_1'] == 1:\n",
    "        return 'Ventilation only (def_1)'\n",
    "    elif row['def_2'] == 1:\n",
    "        return 'Vasoactive only (def_2)'\n",
    "    elif row['def_3'] == 1:\n",
    "        return 'AKI only (def_3)'\n",
    "    else:\n",
    "        return 'No definitions'\n",
    "\n",
    "patient_demo['definition_group'] = patient_demo.apply(create_def_groups, axis=1)\n",
    "\n",
    "# Function to create table one statistics\n",
    "def create_table_one():\n",
    "    groups = ['Overall'] + sorted(patient_demo['definition_group'].unique().tolist())\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    for group in groups:\n",
    "        if group == 'Overall':\n",
    "            data = patient_demo\n",
    "        else:\n",
    "            data = patient_demo[patient_demo['definition_group'] == group]\n",
    "        \n",
    "        # Basic counts\n",
    "        n_hospitalizations = len(data)\n",
    "        n_patients = data['patient_id'].nunique()\n",
    "        \n",
    "        # Demographics - handle missing values\n",
    "        age_stats = data['age_at_admission'].describe() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Sex distribution\n",
    "        sex_dist = data['sex_category'].value_counts() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Race distribution\n",
    "        race_dist = data['race_category'].value_counts() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Ethnicity distribution\n",
    "        ethnicity_dist = data['ethnicity_category'].value_counts() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Mortality\n",
    "        mortality_count = int(data['mortality'].sum()) if n_hospitalizations > 0 else 0\n",
    "        \n",
    "        # CRRT\n",
    "        crrt_count = int(data['crrt'].sum()) if n_hospitalizations > 0 else 0\n",
    "        \n",
    "        # ICU Length of Stay\n",
    "        icu_los_stats = data['icu_duration_days'].describe() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Medication use\n",
    "        med_use = int(data['vasoactive_meds_flag'].sum()) if n_hospitalizations > 0 else 0\n",
    "        \n",
    "        # Creatinine distribution\n",
    "        creat_stats = data['baseline_creatinine'].describe() if n_hospitalizations > 0 else pd.Series()\n",
    "        \n",
    "        # Store results with safe handling of missing data\n",
    "        group_stats = {\n",
    "            'Group': group,\n",
    "            'N_Hospitalizations': n_hospitalizations,\n",
    "            'N_Patients': n_patients,\n",
    "            'Age_Median': age_stats.get('50%', 0) if not pd.isna(age_stats.get('50%', np.nan)) else 0,\n",
    "            'Age_Q1': age_stats.get('25%', 0) if not pd.isna(age_stats.get('25%', np.nan)) else 0,\n",
    "            'Age_Q3': age_stats.get('75%', 0) if not pd.isna(age_stats.get('75%', np.nan)) else 0,\n",
    "            'Female_N': sex_dist.get('Female', 0),\n",
    "            'Female_Pct': (sex_dist.get('Female', 0) / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'Male_N': sex_dist.get('Male', 0),\n",
    "            'Male_Pct': (sex_dist.get('Male', 0) / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'White_N': race_dist.get('White', 0),\n",
    "            'White_Pct': (race_dist.get('White', 0) / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'Black_N': race_dist.get('Black or African American', 0),\n",
    "            'Black_Pct': (race_dist.get('Black or African American', 0) / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'Hispanic_N': ethnicity_dist.get('Hispanic', 0),\n",
    "            'Hispanic_Pct': (ethnicity_dist.get('Hispanic', 0) / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'Mortality_N': mortality_count,\n",
    "            'Mortality_Pct': (mortality_count / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'CRRT_N': crrt_count,\n",
    "            'CRRT_Pct': (crrt_count / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'ICU_LOS_Median': icu_los_stats.get('50%', 0) if not pd.isna(icu_los_stats.get('50%', np.nan)) else 0,\n",
    "            'ICU_LOS_Q1': icu_los_stats.get('25%', 0) if not pd.isna(icu_los_stats.get('25%', np.nan)) else 0,\n",
    "            'ICU_LOS_Q3': icu_los_stats.get('75%', 0) if not pd.isna(icu_los_stats.get('75%', np.nan)) else 0,\n",
    "            'Vasoactive_Meds_N': med_use,\n",
    "            'Vasoactive_Meds_Pct': (med_use / n_hospitalizations * 100) if n_hospitalizations > 0 else 0,\n",
    "            'Creatinine_Median': creat_stats.get('50%', 0) if not pd.isna(creat_stats.get('50%', np.nan)) else 0,\n",
    "            'Creatinine_Q1': creat_stats.get('25%', 0) if not pd.isna(creat_stats.get('25%', np.nan)) else 0,\n",
    "            'Creatinine_Q3': creat_stats.get('75%', 0) if not pd.isna(creat_stats.get('75%', np.nan)) else 0,\n",
    "        }\n",
    "        \n",
    "        table_data.append(group_stats)\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "# Create the table\n",
    "table_one = create_table_one()\n",
    "\n",
    "# Format the table for better presentation\n",
    "def format_table_one(df):\n",
    "    formatted_data = []\n",
    "    \n",
    "    # Define the order of characteristics (added mortality, CRRT, and ICU LOS)\n",
    "    characteristics = [\n",
    "        'N (Hospitalizations)',\n",
    "        'N (Unique Patients)', \n",
    "        'Age, median [Q1, Q3]',\n",
    "        'Female, n (%)',\n",
    "        'Male, n (%)',\n",
    "        'White, n (%)',\n",
    "        'Black, n (%)',\n",
    "        'Hispanic, n (%)',\n",
    "        'Mortality, n (%)',\n",
    "        'CRRT, n (%)',\n",
    "        'ICU Length of Stay (days), median [Q1, Q3]',\n",
    "        'Vasoactive Medications, n (%)',\n",
    "        'Baseline Creatinine, median [Q1, Q3]'\n",
    "    ]\n",
    "    \n",
    "    # Create formatted table\n",
    "    final_table = []\n",
    "    \n",
    "    for char in characteristics:\n",
    "        row_dict = {'Characteristic': char}\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            group_name = row['Group']\n",
    "            \n",
    "            if char == 'N (Hospitalizations)':\n",
    "                value = f\"{int(row['N_Hospitalizations'])}\"\n",
    "            elif char == 'N (Unique Patients)':\n",
    "                value = f\"{int(row['N_Patients'])}\"\n",
    "            elif char == 'Age, median [Q1, Q3]':\n",
    "                if row['Age_Median'] > 0:\n",
    "                    value = f\"{row['Age_Median']:.1f} [{row['Age_Q1']:.1f}, {row['Age_Q3']:.1f}]\"\n",
    "                else:\n",
    "                    value = \"N/A\"\n",
    "            elif char == 'Female, n (%)':\n",
    "                value = f\"{int(row['Female_N'])} ({row['Female_Pct']:.1f})\"\n",
    "            elif char == 'Male, n (%)':\n",
    "                value = f\"{int(row['Male_N'])} ({row['Male_Pct']:.1f})\"\n",
    "            elif char == 'White, n (%)':\n",
    "                value = f\"{int(row['White_N'])} ({row['White_Pct']:.1f})\"\n",
    "            elif char == 'Black, n (%)':\n",
    "                value = f\"{int(row['Black_N'])} ({row['Black_Pct']:.1f})\"\n",
    "            elif char == 'Hispanic, n (%)':\n",
    "                value = f\"{int(row['Hispanic_N'])} ({row['Hispanic_Pct']:.1f})\"\n",
    "            elif char == 'Mortality, n (%)':\n",
    "                value = f\"{int(row['Mortality_N'])} ({row['Mortality_Pct']:.1f})\"\n",
    "            elif char == 'CRRT, n (%)':\n",
    "                value = f\"{int(row['CRRT_N'])} ({row['CRRT_Pct']:.1f})\"\n",
    "            elif char == 'ICU Length of Stay (days), median [Q1, Q3]':\n",
    "                if row['ICU_LOS_Median'] > 0:\n",
    "                    value = f\"{row['ICU_LOS_Median']:.1f} [{row['ICU_LOS_Q1']:.1f}, {row['ICU_LOS_Q3']:.1f}]\"\n",
    "                else:\n",
    "                    value = \"N/A\"\n",
    "            elif char == 'Vasoactive Medications, n (%)':\n",
    "                value = f\"{int(row['Vasoactive_Meds_N'])} ({row['Vasoactive_Meds_Pct']:.1f})\"\n",
    "            elif char == 'Baseline Creatinine, median [Q1, Q3]':\n",
    "                if row['Creatinine_Median'] > 0:\n",
    "                    value = f\"{row['Creatinine_Median']:.2f} [{row['Creatinine_Q1']:.2f}, {row['Creatinine_Q3']:.2f}]\"\n",
    "                else:\n",
    "                    value = \"N/A\"\n",
    "            else:\n",
    "                value = \"N/A\"\n",
    "            \n",
    "            row_dict[group_name] = value\n",
    "        \n",
    "        final_table.append(row_dict)\n",
    "    \n",
    "    return pd.DataFrame(final_table)\n",
    "\n",
    "# Create formatted table\n",
    "formatted_table_one = format_table_one(table_one)\n",
    "\n",
    "# Display the table\n",
    "print(\"Table One: Patient Characteristics by Definition Groups (Enhanced)\")\n",
    "print(\"=\" * 90)\n",
    "print(formatted_table_one.to_string(index=False))\n",
    "\n",
    "# Save the tables\n",
    "import os\n",
    "os.makedirs('../output/final', exist_ok=True)\n",
    "\n",
    "table_one.to_csv('../output/final/table_one_raw_enhanced.csv', index=False)\n",
    "formatted_table_one.to_csv('../output/final/table_one_formatted_enhanced.csv', index=False)\n",
    "\n",
    "print(f\"\\nTables saved to:\")\n",
    "print(\"- ../output/final/table_one_raw_enhanced.csv\")\n",
    "print(\"- ../output/final/table_one_formatted_enhanced.csv\")\n",
    "\n",
    "# Create a simplified version with key comparisons\n",
    "simplified_groups = ['Overall', 'No definitions', 'Ventilation only (def_1)', \n",
    "                    'Vasoactive only (def_2)', 'AKI only (def_3)', \n",
    "                    'Vent + Vaso (def_1+2)', 'All three (def_1+2+3)']\n",
    "\n",
    "# Filter to only include columns that exist\n",
    "available_groups = [col for col in simplified_groups if col in formatted_table_one.columns]\n",
    "simplified_table = formatted_table_one[['Characteristic'] + available_groups]\n",
    "\n",
    "print(f\"\\nSimplified Table One:\")\n",
    "print(\"=\" * 70)\n",
    "print(simplified_table.to_string(index=False))\n",
    "\n",
    "simplified_table.to_csv('../output/final/table_one_simplified_enhanced.csv', index=False)\n",
    "\n",
    "# Print summary statistics (enhanced)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total patients analyzed: {len(patient_demo)}\")\n",
    "print(f\"Definition groups found: {len(patient_demo['definition_group'].unique())}\")\n",
    "\n",
    "# Overall mortality rate\n",
    "overall_mortality = patient_demo['mortality'].sum()\n",
    "overall_mortality_rate = (overall_mortality / len(patient_demo) * 100)\n",
    "print(f\"Overall mortality: {int(overall_mortality)} ({overall_mortality_rate:.1f}%)\")\n",
    "\n",
    "# Overall CRRT rate\n",
    "overall_crrt = patient_demo['crrt'].sum()\n",
    "overall_crrt_rate = (overall_crrt / len(patient_demo) * 100)\n",
    "print(f\"Overall CRRT: {int(overall_crrt)} ({overall_crrt_rate:.1f}%)\")\n",
    "\n",
    "# Overall ICU LOS\n",
    "overall_icu_los = patient_demo['icu_duration_days'].median()\n",
    "print(f\"Overall ICU LOS median: {overall_icu_los:.1f} days\")\n",
    "\n",
    "print(\"\\nDefinition group distribution:\")\n",
    "group_counts = patient_demo['definition_group'].value_counts().sort_values(ascending=False)\n",
    "for group, count in group_counts.items():\n",
    "    pct = count / len(patient_demo) * 100\n",
    "    group_mortality = patient_demo[patient_demo['definition_group'] == group]['mortality'].sum()\n",
    "    group_mortality_rate = (group_mortality / count * 100) if count > 0 else 0\n",
    "    group_crrt = patient_demo[patient_demo['definition_group'] == group]['crrt'].sum()\n",
    "    group_crrt_rate = (group_crrt / count * 100) if count > 0 else 0\n",
    "    print(f\"  {group}: {count} ({pct:.1f}%) - Mortality: {int(group_mortality)} ({group_mortality_rate:.1f}%) - CRRT: {int(group_crrt)} ({group_crrt_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebb505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e771b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcf26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".crrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

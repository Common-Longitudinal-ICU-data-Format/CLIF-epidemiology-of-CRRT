{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a55492",
   "metadata": {},
   "source": [
    "# Epidemiology of CRRT\n",
    "\n",
    "Author: Kaveri Chhikara\n",
    "\n",
    "This script identifies the cohort using CLIF 2.1 tables\n",
    "**Requirements**\n",
    "\n",
    "* Required table filenames should be `clif_patient`, `clif_hospitalization`, `clif_adt`, `clif_vitals`, `clif_labs`, `clif_medication_admin_continuous`, `clif_respiratory_support` ,`crrt_therapy`, `clif_hospital_diagnosis`\n",
    "* Within each table, the following variables and categories are required.\n",
    "\n",
    "| Table Name | Required Variables | Required Categories |\n",
    "| --- | --- | --- |\n",
    "| **clif_patient** | `patient_id`, `race_category`, `ethnicity_category`, `sex_category`, `death_dttm` | - |\n",
    "| **clif_hospitalization** | `patient_id`, `hospitalization_id`, `admission_dttm`, `discharge_dttm`, `age_at_admission`, `discharge_category` | - |\n",
    "| **clif_adt** |  `hospitalization_id`, `hospital_id`,`in_dttm`, `out_dttm`, `location_category`, `location_type` | - |\n",
    "| **clif_vitals** | `hospitalization_id`, `recorded_dttm`, `vital_category`, `vital_value` | heart_rate, resp_rate, sbp, dbp, map, spo2, weight_kg, height_cm |\n",
    "| **clif_labs** | `hospitalization_id`, `lab_result_dttm`, `lab_category`, `lab_value` | sodium, potassium, chloride, bicarbonate, bun, creatinine, glucose_serum, calcium_total, lactate, magnesium, ph_arterial, ph_venous, po2_arterial |\n",
    "| **clif_medication_admin_continuous** | `hospitalization_id`, `admin_dttm`, `med_name`, `med_category`, `med_dose`, `med_dose_unit` | norepinephrine, epinephrine, phenylephrine, vasopressin, dopamine, angiotensin, dobutamine, milrinone, isoproterenol |\n",
    "| **clif_respiratory_support** | `hospitalization_id`, `recorded_dttm`, `device_category`, `mode_category`, `tracheostomy`, `fio2_set`, `lpm_set`, `resp_rate_set`, `peep_set`, `resp_rate_obs`, `tidal_volume_set`, `pressure_control_set`, `pressure_support_set`, `peak_inspiratory_pressure_set`, `tidal_volume_obs` | - |\n",
    "| **clif_crrt_therapy** | `hospitalization_id`, `recorded_dttm`, `crrt_mode_name`, `crrt_mode_category`, `device_id`, `blood_flow_rate`, `dialysate_flow_rate`, `pre_filter_replacement_fluid_rate`,`post_filter_replacement_fluid_rate`, `ultrafilteration_out` | - |\n",
    "| **clif_hospital_diagnosis** | `hospitalization_id`, `diagnosis_code`, `present_on_admission` | - |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3527d88",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ae96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pyarrow\n",
    "import warnings\n",
    "import clifpy\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import clifpy\n",
    "import os\n",
    "\n",
    "print(\"=== Environment Verification ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"clifpy version: {clifpy.__version__}\")\n",
    "print(f\"clifpy location: {clifpy.__file__}\")\n",
    "\n",
    "print(\"\\n=== Python Path Check ===\")\n",
    "local_clifpy_path = \"/Users/kavenchhikara/Desktop/CLIF/CLIFpy\"\n",
    "if any(local_clifpy_path in path for path in sys.path):\n",
    "    print(\"⚠️  WARNING: Local CLIFpy still in path!\")\n",
    "    for path in sys.path:\n",
    "        if local_clifpy_path in path:\n",
    "            print(f\"   Found: {path}\")\n",
    "else:\n",
    "    print(\"✅ Clean environment - no local CLIFpy in path\")\n",
    "\n",
    "print(f\"\\n=== Working Directory ===\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "## import outlier json\n",
    "# with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "#     outlier_cfg = json.load(f)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b73fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create output directories if they do not exist\n",
    "os.makedirs(\"../output/final/graphs\", exist_ok=True)\n",
    "os.makedirs(\"../output/intermediate\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3f969",
   "metadata": {},
   "source": [
    "# Required columns and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Defining Required Data Elements\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Full patient table \n",
    "\n",
    "# Full hospitalization table \n",
    "\n",
    "# Full ADT table\n",
    "\n",
    "# Vitals\n",
    "vitals_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'vital_category',\n",
    "    'vital_value'\n",
    "]\n",
    "vitals_of_interest = ['heart_rate', 'respiratory_rate', 'sbp', 'dbp', 'map', 'spo2', 'weight_kg', 'height_cm']\n",
    "\n",
    "#Labs\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "labs_of_interest = ['po2_arterial','pco2_arterial', 'ph_arterial','ph_venous', 'bicarbonate','so2_arterial',\n",
    "                    'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', \n",
    "                    'bun', 'glucose_serum', 'lactate', 'hemoglobin' ]\n",
    "\n",
    "# Continuous administered meds\n",
    "meds_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'admin_dttm',\n",
    "    'med_name',\n",
    "    'med_category',\n",
    "    'med_dose',\n",
    "    'med_dose_unit'\n",
    "]\n",
    "meds_of_interest = [\n",
    "    'norepinephrine', 'epinephrine', 'phenylephrine', 'vasopressin',\n",
    "    'dopamine', 'angiotensin', 'dobutamine', 'milrinone', 'isoproterenol',\n",
    "    'propofol', 'midazolam', 'lorazepam', 'dexmedetomidine', \n",
    "    'vecuronium', 'rocuronium', 'cisatracurium', 'pancuronium'\n",
    "]\n",
    "\n",
    "# Respiratory Support \n",
    "rst_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'device_name',\n",
    "    'device_category',\n",
    "    'mode_name', \n",
    "    'mode_category',\n",
    "    'tracheostomy',\n",
    "    'fio2_set',\n",
    "    'lpm_set',\n",
    "    'resp_rate_set',\n",
    "    'peep_set',\n",
    "    'resp_rate_obs',\n",
    "    'tidal_volume_set', \n",
    "    'pressure_control_set',\n",
    "    'pressure_support_set',\n",
    "    'peak_inspiratory_pressure_set',\n",
    "    'peak_inspiratory_pressure_obs',\n",
    "    'plateau_pressure_obs',\n",
    "    'minute_vent_obs'\n",
    "]\n",
    "\n",
    "# Full crrt table\n",
    "crrt_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'recorded_dttm',\n",
    "    'crrt_mode_category',\n",
    "    'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc995a",
   "metadata": {},
   "source": [
    "# Cohort Identification\n",
    "\n",
    "\n",
    "**Inclusion**\n",
    "1. Adults\n",
    "2. Admitted between January 1, 2018 to December, 31, 2024\n",
    "3. Receiving CRRT- must have DFR or UF documented at any point in the hospitalization\n",
    "4. Data completeness- Must have weight & CRRT settings  documented\n",
    "\n",
    "**Exclusion**\n",
    "1. Prior to admission ICD codes for ESRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155f53b",
   "metadata": {},
   "source": [
    "## Step0: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38583e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81062564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name)\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = clif.hospitalization.df\n",
    "adt_df = clif.adt.df\n",
    "\n",
    "# Merge to get age information\n",
    "all_encounters = pd.merge(\n",
    "    hosp_df[[\"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\", \n",
    "             \"age_at_admission\", \"discharge_category\"]],\n",
    "    adt_df[[\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \n",
    "            \"location_category\", \"location_type\"]],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by ['hospitalization_id', 'in_dttm', 'out_dttm']\n",
    "dup_counts = all_encounters.duplicated(subset=['hospitalization_id', 'in_dttm', 'out_dttm']).sum()\n",
    "if dup_counts > 0:\n",
    "    print(f\"Warning: {dup_counts} duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")\n",
    "else:\n",
    "    print(\"No duplicate (hospitalization_id, in_dttm, out_dttm) entries found in all_encounters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a74bac",
   "metadata": {},
   "source": [
    "## Step1: Date & Age filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Identify Adult Patients (Age >= 18) and Admissions 2018-2024\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 1: Identifying Adult Patients (Age >= 18) and Admissions 2018-2024\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Applying initial cohort filters...\")\n",
    "\n",
    "# Use only the relevant columns from all_encounters\n",
    "adult_encounters = all_encounters[\n",
    "    [\n",
    "        'patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'discharge_category', 'hospital_id',\n",
    "        'in_dttm', 'out_dttm', 'location_category', 'location_type'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# Filter for adult patients (age >= 18) and valid age\n",
    "adult_encounters = adult_encounters[\n",
    "    (adult_encounters['age_at_admission'] >= 18) & (adult_encounters['age_at_admission'].notna())\n",
    "]\n",
    "\n",
    "# Filter for admission years 2018-2024\n",
    "if config['site_name'].lower() != \"mimic\":\n",
    "    adult_encounters = adult_encounters[\n",
    "        (adult_encounters['admission_dttm'].dt.year >= 2018) & (adult_encounters['admission_dttm'].dt.year <= 2024)\n",
    "    ]\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"   Total hospitalizations: {len(all_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Adult hospitalizations (age >= 18, 2018-2024): {len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "print(f\"   Excluded (age < 18 or outside 2018-2024): {len(all_encounters['hospitalization_id'].unique()) - len(adult_encounters['hospitalization_id'].unique()):,}\")\n",
    "\n",
    "\n",
    "strobe_counts[\"0_total_hospitalizations\"] = len(all_encounters['hospitalization_id'].unique())\n",
    "strobe_counts[\"1_adult_hospitalizations\"] = len(adult_encounters['hospitalization_id'].unique())\n",
    "# Get list of adult hospitalization IDs for filtering\n",
    "adult_hosp_ids = set(adult_encounters['hospitalization_id'].unique())\n",
    "print(f\"\\n   Unique adult hospitalization IDs: {len(adult_hosp_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76e43d",
   "metadata": {},
   "source": [
    "## Step1B: Stitch hospitalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clifpy.utils.stitching_encounters import stitch_encounters\n",
    "\n",
    "# Instead of multiple copies, work with references and clean up\n",
    "hosp_filtered = clif.hospitalization.df[clif.hospitalization.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "adt_filtered = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)]\n",
    "\n",
    "hosp_stitched, adt_stitched, encounter_mapping = stitch_encounters(\n",
    "    hospitalization=hosp_filtered,\n",
    "    adt=adt_filtered,\n",
    "    time_interval=6  \n",
    ")\n",
    "\n",
    "# Direct assignment without additional copies\n",
    "clif.hospitalization.df = hosp_stitched\n",
    "clif.adt.df = adt_stitched\n",
    "\n",
    "# Store the encounter mapping in the orchestrator for later use\n",
    "clif.encounter_mapping = encounter_mapping\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del hosp_filtered, adt_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f522545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your stitching code, add these calculations:\n",
    "\n",
    "# Calculate stitching statistics\n",
    "strobe_counts['1b_before_stitching'] = len(adult_hosp_ids)  # Original adult hospitalizations\n",
    "strobe_counts['1b_after_stitching'] = len(hosp_stitched['encounter_block'].unique())  # Unique encounter blocks after stitching\n",
    "strobe_counts['1b_stitched_hosp_ids'] = strobe_counts['1b_before_stitching'] - strobe_counts['1b_after_stitching']  # Number of hospitalizations that were linked\n",
    "\n",
    "print(f\"\\nEncounter Stitching Results:\")\n",
    "print(f\"   Number of unique hospitalizations before stitching: {strobe_counts['1b_before_stitching']:,}\")\n",
    "print(f\"   Number of unique encounter blocks after stitching: {strobe_counts['1b_after_stitching']:,}\")\n",
    "print(f\"   Number of linked hospitalization ids: {strobe_counts['1b_stitched_hosp_ids']:,}\")\n",
    "\n",
    "# Optional: Show the encounter mapping details\n",
    "print(f\"\\nEncounter Mapping Details:\")\n",
    "print(f\"   Total encounter mappings created: {len(encounter_mapping):,}\")\n",
    "if len(encounter_mapping) > 0:\n",
    "    # Show some examples of how many original hospitalizations were combined\n",
    "    mapping_counts = encounter_mapping.groupby('encounter_block').size()\n",
    "    print(f\"   Encounter blocks with multiple hospitalizations: {(mapping_counts > 1).sum():,}\")\n",
    "    print(f\"   Maximum hospitalizations combined into one block: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b61028",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = encounter_mapping.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8ef2e",
   "metadata": {},
   "source": [
    "## Step2: Identify CRRT Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading crrt_therapy table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'crrt_therapy',\n",
    "        filters={'hospitalization_id': list(adult_hosp_ids)}\n",
    "    )\n",
    "    print(f\"   CRRT therapy loaded: {len(clif.crrt_therapy.df):,} rows\")\n",
    "    print(f\"   Unique CRRT therapy hospitalizations: {clif.crrt_therapy.df['hospitalization_id'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   CRRT therapy not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update CRRT therapy DataFrame with encounter blocks\n",
    "clif.crrt_therapy.df = clif.crrt_therapy.df.merge(\n",
    "    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "n_crrt_hosp = clif.crrt_therapy.df['hospitalization_id'].nunique()\n",
    "n_crrt_blocks = clif.crrt_therapy.df['encounter_block'].nunique()\n",
    "crrt_hosp_ids = set(clif.crrt_therapy.df['hospitalization_id'].unique())\n",
    "\n",
    "print(f\"Updated CRRT therapy DataFrame:\")\n",
    "print(f\"   Total CRRT records: {len(clif.crrt_therapy.df):,}\")\n",
    "print(f\"   Records with encounter blocks: {clif.crrt_therapy.df['encounter_block'].notna().sum():,}\")\n",
    "print(f\"   Unique encounter blocks in CRRT data: {n_crrt_blocks}\")\n",
    "print(f\"   Unique hospitalizations  in CRRT data: {n_crrt_hosp}\")\n",
    "\n",
    "strobe_counts[\"2_crrt_hospitalizations\"] = n_crrt_hosp\n",
    "strobe_counts[\"2_crrt_blocks\"] = n_crrt_blocks\n",
    "\n",
    "# Filter cohort_df to only hospitalizations present in CRRT data\n",
    "cohort_df = cohort_df[cohort_df['hospitalization_id'].isin(crrt_hosp_ids)].copy()\n",
    "crrt_df = clif.crrt_therapy.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0472a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import handle_crrt_outliers\n",
    "\n",
    "# Apply outlier removal\n",
    "crrt_df, outlier_summary = handle_crrt_outliers(\n",
    "    crrt_df,\n",
    "    config_path='../config/outlier_config.json'\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Validate Outlier Ranges\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CRRT Parameter Distribution Validation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CRRT parameters to validate and plot\n",
    "crrt_params = {\n",
    "    'blood_flow_rate': {'expected_range': [150, 5000], 'unit': 'mL/min'},\n",
    "    'dialysate_flow_rate': {'expected_range': [0, 20000], 'unit': 'mL/hr'},\n",
    "    'pre_filter_replacement_fluid_rate': {'expected_range': [0, 20000], 'unit': 'mL/hr'},\n",
    "    'post_filter_replacement_fluid_rate': {'expected_range': [0, 20000], 'unit': 'mL/hr'},\n",
    "    'ultrafiltration_out': {'expected_range': [0, 20000], 'unit': 'mL/hr'}\n",
    "}\n",
    "\n",
    "# Check if means are within expected ranges\n",
    "print(\"\\n1. Validating parameter distributions...\")\n",
    "unit_warnings = []\n",
    "\n",
    "for param, info in crrt_params.items():\n",
    "    if param not in crrt_df.columns:\n",
    "        continue\n",
    "\n",
    "    values = crrt_df[param].dropna()\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "\n",
    "    mean_val = values.mean()\n",
    "    min_range, max_range = info['expected_range']\n",
    "    unit = info['unit']\n",
    "\n",
    "    print(f\"\\n   {param}:\")\n",
    "    print(f\"     Mean: {mean_val:.0f} {unit}\")\n",
    "    print(f\"     Expected range: {min_range}-{max_range} {unit}\")\n",
    "\n",
    "    # Check if mean is within range\n",
    "    if mean_val < min_range or mean_val > max_range:\n",
    "        warning_msg = f\"⚠️  WARNING: Mean ({mean_val:.0f}) is OUTSIDE expected range [{min_range}-{max_range}]\"\n",
    "        print(f\"     {warning_msg}\")\n",
    "        print(f\"     → Check if units are correct (expected: {unit})\")\n",
    "        unit_warnings.append({\n",
    "            'parameter': param,\n",
    "            'mean': mean_val,\n",
    "            'expected_range': [min_range, max_range],\n",
    "            'unit': unit\n",
    "        })\n",
    "    else:\n",
    "        print(f\"     ✓ Mean is within expected range\")\n",
    "\n",
    "if unit_warnings:\n",
    "    print(\"\\n\" + \"!\" * 80)\n",
    "    print(\"UNIT MISMATCH WARNINGS:\")\n",
    "    for w in unit_warnings:\n",
    "        print(f\"\\n   {w['parameter']}:\")\n",
    "        print(f\"     Mean: {w['mean']:.0f}\")\n",
    "        print(f\"     Expected range: {w['expected_range']} {w['unit']}\")\n",
    "        print(f\"     → Data may be in different units than expected!\")\n",
    "    print(\"!\" * 80)\n",
    "else:\n",
    "    print(\"\\n✓ All parameter means are within expected ranges\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Generate GRID Histograms: Parameters (rows) × Modes (columns)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Generating CRRT Parameter Histograms Grid by Mode\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get unique modes (sorted)\n",
    "modes = sorted(crrt_df['crrt_mode_category'].dropna().unique())\n",
    "n_modes = len(modes)\n",
    "n_params = len(crrt_params)\n",
    "\n",
    "# Create grid: rows = parameters, columns = modes\n",
    "fig, axes = plt.subplots(n_params, n_modes, figsize=(5 * n_modes, 4 * n_params))\n",
    "\n",
    "# Handle single row/column cases\n",
    "if n_params == 1 and n_modes == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_params == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "elif n_modes == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "fig.suptitle('CRRT Parameter Distributions by Mode (After Outlier Removal)',\n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# Plot grid\n",
    "for row_idx, (param, info) in enumerate(crrt_params.items()):\n",
    "    for col_idx, mode in enumerate(modes):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Check if parameter exists in data\n",
    "        if param not in crrt_df.columns:\n",
    "            ax.text(0.5, 0.5, f'{param}\\nNot Available',\n",
    "                    ha='center', va='center', fontsize=10)\n",
    "            ax.set_title(f'{mode.upper()}')\n",
    "            continue\n",
    "\n",
    "        # Filter data for this mode and parameter\n",
    "        mode_data = crrt_df[\n",
    "            (crrt_df['crrt_mode_category'] == mode) &\n",
    "            (crrt_df[param].notna())\n",
    "        ][param]\n",
    "\n",
    "        if len(mode_data) == 0:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', fontsize=10)\n",
    "            ax.set_title(f'{mode.upper()}')\n",
    "            continue\n",
    "\n",
    "        # Create histogram\n",
    "        ax.hist(mode_data, bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "\n",
    "        # Calculate statistics\n",
    "        n = len(mode_data)\n",
    "        mean_val = mode_data.mean()\n",
    "        median_val = mode_data.median()\n",
    "\n",
    "        # Add vertical lines for mean and median\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.0f}')\n",
    "        ax.axvline(median_val, color='green', linestyle=':', linewidth=2, label=f'Median: {median_val:.0f}')\n",
    "\n",
    "        # Add statistics text box\n",
    "        stats_text = f'N = {n:,}\\nMean = {mean_val:.0f}\\nMedian = {median_val:.0f}'\n",
    "        ax.text(0.98, 0.97, stats_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=9)\n",
    "\n",
    "        # Set title (mode name) for top row only\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f'{mode.upper()}', fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Set ylabel (parameter name) for first column only\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f'{param.replace(\"_\", \" \").title()}\\n({info[\"unit\"]})',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "\n",
    "        # Set xlabel for bottom row only\n",
    "        if row_idx == n_params - 1:\n",
    "            ax.set_xlabel(f'{info[\"unit\"]}', fontsize=9)\n",
    "\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "Path(\"../output/final\").mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig('../output/final/graphs/crrt_parameter_histograms_grid.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Grid histograms saved to: output/final/crrt_parameter_histograms_grid.png\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Statistics: Distribution of Settings by Mode Category\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CRRT Settings Distribution by Mode Category\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for mode in modes:\n",
    "    mode_df = crrt_df[crrt_df['crrt_mode_category'] == mode]\n",
    "\n",
    "    row = {\n",
    "        'Mode': mode.upper(),\n",
    "        'N_Total': len(mode_df)\n",
    "    }\n",
    "\n",
    "    for param, info in crrt_params.items():\n",
    "        if param in mode_df.columns:\n",
    "            values = mode_df[param].dropna()\n",
    "            n = len(values)\n",
    "\n",
    "            if n > 0:\n",
    "                row[f'{param}_N'] = n\n",
    "                row[f'{param}_Mean'] = round(values.mean(), 1)\n",
    "                row[f'{param}_SD'] = round(values.std(), 1)\n",
    "                row[f'{param}_Median'] = round(values.median(), 1)\n",
    "                row[f'{param}_Q25'] = round(values.quantile(0.25), 1)\n",
    "                row[f'{param}_Q75'] = round(values.quantile(0.75), 1)\n",
    "                row[f'{param}_Min'] = round(values.min(), 1)\n",
    "                row[f'{param}_Max'] = round(values.max(), 1)\n",
    "            else:\n",
    "                row[f'{param}_N'] = 0\n",
    "                row[f'{param}_Mean'] = np.nan\n",
    "                row[f'{param}_SD'] = np.nan\n",
    "                row[f'{param}_Median'] = np.nan\n",
    "                row[f'{param}_Q25'] = np.nan\n",
    "                row[f'{param}_Q75'] = np.nan\n",
    "                row[f'{param}_Min'] = np.nan\n",
    "                row[f'{param}_Max'] = np.nan\n",
    "\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nSummary by Mode (first few columns):\")\n",
    "display_cols = ['Mode', 'N_Total'] + [col for col in summary_df.columns if '_Mean' in col or '_Median' in col]\n",
    "print(summary_df[display_cols].to_string(index=False))\n",
    "\n",
    "# Save detailed summary\n",
    "summary_df.to_csv('../output/final/crrt_settings_distribution_by_mode.csv', index=False)\n",
    "print(f\"\\n✓ Detailed summary saved to: output/final/crrt_settings_distribution_by_mode.csv\")\n",
    "\n",
    "# Also create a simplified summary for quick reference\n",
    "simple_summary = []\n",
    "for mode in modes:\n",
    "    mode_df = crrt_df[crrt_df['crrt_mode_category'] == mode]\n",
    "\n",
    "    row = {'Mode': mode.upper(), 'N': len(mode_df)}\n",
    "\n",
    "    for param, info in crrt_params.items():\n",
    "        if param in mode_df.columns:\n",
    "            values = mode_df[param].dropna()\n",
    "            if len(values) > 0:\n",
    "                # Format as \"Median [Q25-Q75]\"\n",
    "                row[param] = f\"{values.median():.0f} [{values.quantile(0.25):.0f}-{values.quantile(0.75):.0f}]\"\n",
    "            else:\n",
    "                row[param] = \"No data\"\n",
    "\n",
    "    simple_summary.append(row)\n",
    "\n",
    "simple_summary_df = pd.DataFrame(simple_summary)\n",
    "\n",
    "# Display simple summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Quick Reference: Median [IQR] by Mode\")\n",
    "print(\"=\" * 80)\n",
    "print(simple_summary_df.to_string(index=False))\n",
    "\n",
    "# Save simple summary\n",
    "simple_summary_df.to_csv('../output/final/crrt_settings_summary_simple.csv', index=False)\n",
    "print(f\"\\n✓ Simple summary saved to: output/final/crrt_settings_summary_simple.csv\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where crrt_mode_category == \"scuf\"\n",
    "n_rows_before = len(crrt_df)\n",
    "n_encounter_blocks_before = crrt_df['encounter_block'].nunique()\n",
    "\n",
    "# Identify encounter_blocks where all rows are scuf (i.e., only scuf)\n",
    "only_scuf_blocks = (\n",
    "    crrt_df.groupby('encounter_block')['crrt_mode_category']\n",
    "    .apply(lambda x: set(x.dropna()) == set(['scuf']))\n",
    "    .loc[lambda x: x].index\n",
    ")\n",
    "n_only_scuf_blocks = len(only_scuf_blocks)\n",
    "\n",
    "# Drop rows where crrt_mode_category == \"scuf\"\n",
    "crrt_df = crrt_df[crrt_df['crrt_mode_category'].str.lower() != \"scuf\"]\n",
    "n_rows_after = len(crrt_df)\n",
    "n_encounter_blocks_after = crrt_df['encounter_block'].nunique()\n",
    "n_rows_dropped = n_rows_before - n_rows_after\n",
    "\n",
    "print(f\"Dropped {n_rows_dropped} rows where crrt_mode_category == 'scuf'\")\n",
    "print(f\"Encounter_blocks with only SCUF (all records scuf): {n_only_scuf_blocks}\")\n",
    "strobe_counts[\"blocks_with_only_sucf\"] = n_only_scuf_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8207b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Processing CRRT Data\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   CRRT therapy loaded: {len(clif.crrt_therapy.df):,} rows\")\n",
    "print(f\"   Unique CRRT therapy hospitalizations: {clif.crrt_therapy.df['hospitalization_id'].nunique()}\")\n",
    "# ============================================================================\n",
    "#  Define CRRT Initiation Time\n",
    "# ============================================================================\n",
    "print(\"\\n1. Defining CRRT initiation time...\")\n",
    "\n",
    "# Filter crrt_df to only include hospitalization_ids present in the cohort\n",
    "crrt_cohort = crrt_df[crrt_df['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "\n",
    "# Sort by encounter_block and time\n",
    "crrt_cohort = crrt_cohort.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "# Create indicator for any CRRT activity (any non-null flow rate)\n",
    "crrt_cohort['has_crrt_activity'] = (\n",
    "    (\n",
    "        crrt_cohort['dialysate_flow_rate'].notna() & (crrt_cohort['dialysate_flow_rate'] > 0)\n",
    "    ) |\n",
    "    (\n",
    "        crrt_cohort['pre_filter_replacement_fluid_rate'].notna() & (crrt_cohort['pre_filter_replacement_fluid_rate'] > 0)\n",
    "    ) |\n",
    "    (\n",
    "        crrt_cohort['post_filter_replacement_fluid_rate'].notna() & (crrt_cohort['post_filter_replacement_fluid_rate'] > 0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get CRRT initiation time (first non-null flow rate per encounter_block)\n",
    "crrt_initiation = (crrt_cohort[crrt_cohort['has_crrt_activity']]\n",
    "                    .groupby('encounter_block')\n",
    "                    .agg({'recorded_dttm': 'min'})\n",
    "                    .reset_index())\n",
    "crrt_initiation.rename(columns={'recorded_dttm': 'crrt_initiation_time'}, inplace=True)\n",
    "\n",
    "print(f\"   CRRT initiation times identified for: {len(crrt_initiation):,} encounter_blocks\")\n",
    "print(f\"   Date range: {crrt_initiation['crrt_initiation_time'].min()} to {crrt_initiation['crrt_initiation_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35903590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of unique encounter_blocks in crrt_cohort and crrt_initiation\n",
    "num_blocks_cohort = crrt_cohort['encounter_block'].nunique()\n",
    "num_blocks_initiation = crrt_initiation['encounter_block'].nunique()\n",
    "\n",
    "print(f\"Unique encounter_blocks in crrt_cohort: {num_blocks_cohort}\")\n",
    "print(f\"Unique encounter_blocks in crrt_initiation: {num_blocks_initiation}\")\n",
    "\n",
    "# Analyze and explain the apparent paradox\n",
    "blocks_cohort_set = set(crrt_cohort['encounter_block'])\n",
    "blocks_initiation_set = set(crrt_initiation['encounter_block'])\n",
    "\n",
    "missing_initiation = blocks_cohort_set - blocks_initiation_set\n",
    "missing_cohort = blocks_initiation_set - blocks_cohort_set\n",
    "\n",
    "print(f\"Encounter_blocks in crrt_cohort not in crrt_initiation: {len(missing_initiation)}\")\n",
    "print(f\"Encounter_blocks in crrt_initiation not in crrt_cohort: {len(missing_cohort)}\")\n",
    "\n",
    "# Filter final_crrt to only include encounter_blocks present in crrt_initiation['encounter_block']\n",
    "final_crrt = clif.crrt_therapy.df[\n",
    "    clif.crrt_therapy.df['encounter_block'].isin(crrt_initiation['encounter_block'])\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Get CRRT Parameters at Initiation\n",
    "# ============================================================================\n",
    "print(\"\\n4. Getting CRRT parameters at initiation time...\")\n",
    "# Merge initiation times back to crrt_cohort\n",
    "crrt_cohort = crrt_cohort.merge(crrt_initiation, on='encounter_block', how='left')\n",
    "# Filter to records at exactly the initiation time\n",
    "crrt_at_initiation = crrt_cohort[\n",
    "    crrt_cohort['recorded_dttm'] == crrt_cohort['crrt_initiation_time']\n",
    "].copy()\n",
    "\n",
    "print(f\"   CRRT records at initiation: {len(crrt_at_initiation):,}\")\n",
    "print(f\"   Unique encounter blocks: {crrt_at_initiation['encounter_block'].nunique():,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2cad0",
   "metadata": {},
   "source": [
    "## Step3: Exclude ESRD encounters\n",
    "\n",
    "Prior to admission ICD codes for ESRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading Hospital dx table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'hospital_diagnosis',\n",
    "        filters={'hospitalization_id': list(crrt_hosp_ids)}\n",
    "    )\n",
    "    print(f\"   Hospital dx loaded: {len(clif.hospital_diagnosis.df):,} rows\")\n",
    "    print(f\"   Unique Hospital dx hospitalizations: {clif.hospital_diagnosis.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "    print(\"Merge encounter blocks with diagnosis\")\n",
    "    clif.hospital_diagnosis.df = clif.hospital_diagnosis.df.merge(\n",
    "                    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "                    on='hospitalization_id',\n",
    "                    how='left')\n",
    "\n",
    "    n_dx_hosp = clif.hospital_diagnosis.df['hospitalization_id'].nunique()\n",
    "    n_dx_blocks = clif.hospital_diagnosis.df['encounter_block'].nunique()\n",
    "    cohort_hosp_ids = set(clif.hospital_diagnosis.df['hospitalization_id'].unique())\n",
    "    cohort_blocks = set(clif.hospital_diagnosis.df['encounter_block'].unique())\n",
    "    print(f\"   Total Hospital dx records: {len(clif.hospital_diagnosis.df):,}\")\n",
    "    print(f\"   Records with encounter blocks: {clif.hospital_diagnosis.df['encounter_block'].notna().sum():,}\")\n",
    "    print(f\"   Unique encounter blocks in Hospital dx data: {n_dx_blocks}\")\n",
    "    print(f\"   Unique hospitalizations  in Hospital dx data: {n_dx_hosp}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Hospital dx not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c985b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_diagnosis_df = clif.hospital_diagnosis.df.copy()\n",
    "\n",
    "print(\"Hospital dx column names :\", hospital_diagnosis_df.columns)\n",
    "# Clean and standardize diagnosis codes\n",
    "hospital_diagnosis_df['diagnosis_code'] = hospital_diagnosis_df['diagnosis_code'].str.replace('.', '').str.lower()\n",
    "\n",
    "if 'present_on_admission' in hospital_diagnosis_df.columns:\n",
    "    hospital_diagnosis_df = hospital_diagnosis_df.rename(columns={'present_on_admission': 'poa_present'})\n",
    "\n",
    "# Check present_on_admission column type and standardize to int8\n",
    "if 'poa_present' in hospital_diagnosis_df.columns:\n",
    "    # Only allow 1 (present on admission) or 0 (not present on admission)\n",
    "    # Any other value (including Exempt, Unknown, Unspecified, NA) is set to 0\n",
    "    hospital_diagnosis_df['poa_present'] = hospital_diagnosis_df['poa_present'].astype(str).str.lower()\n",
    "    hospital_diagnosis_df['poa_present'] = hospital_diagnosis_df['poa_present'].map(\n",
    "        {'yes': 1, 'y': 1, 'true': 1, '1': 1, 'no': 0, 'n': 0, 'false': 0, '0': 0}\n",
    "    ).fillna(0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ESRD diagnosis codes\n",
    "# Let's debug why we're not finding ESRD codes\n",
    "esrd_codes = [\n",
    "    'z992',    # Dependence on renal dialysis\n",
    "    'z9115',   # Patient's noncompliance with renal dialysis\n",
    "    'i120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD\n",
    "    'n186',    # End stage renal disease\n",
    "    'i132',    # Hypertensive heart and chronic kidney disease with heart failure and ESRD\n",
    "    'z992',    # Dependence on renal dialysis (alternate code)\n",
    "    'i120',    # Hypertensive chronic kidney disease with stage 5 CKD or ESRD (alternate code)\n",
    "    'z91158',  # Patient's noncompliance with renal dialysis (alternate code)\n",
    "    'i1311',   # Hypertensive heart and chronic kidney disease with heart failure and stage 5 CKD\n",
    "    'i132',    # Hypertensive heart and chronic kidney disease with ESRD (alternate code)\n",
    "    '5856',     #ICD9 :End stage renal disease\n",
    "    '40391',    #ICD9: Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease\n",
    "    '40311',     #ICD9: Hypertensive chronic kidney disease, benign, with chronic kidney disease stage V or end stage renal disease\n",
    "    'v4511',     #ICD9: Renal dialysis status\n",
    "    'v4512'     #ICD9: Noncompliance with renal dialysis\n",
    "]\n",
    "\n",
    "# Get hospitalization IDs with ESRD diagnoses and print debug info\n",
    "print(\"\\nNumber of rows matching ESRD codes:\", hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes).sum())\n",
    "\n",
    "\n",
    "# Count how many ESRD codes have present_on_admission = 1, 0, or NA\n",
    "esrd_poa_counts = hospital_diagnosis_df[\n",
    "    hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes)\n",
    "]['poa_present'].value_counts(dropna=False)\n",
    "print(\"Present_on_admission values for ESRD codes:\")\n",
    "print(esrd_poa_counts)\n",
    "\n",
    "# Use a more inclusive approach for ESRD identification\n",
    "# Include cases where present_on_admission is 1 OR NA (assuming NA means unknown/possible)\n",
    "esrd_mask = (\n",
    "    hospital_diagnosis_df['diagnosis_code'].isin(esrd_codes) & \n",
    "    ((hospital_diagnosis_df['poa_present'] == 1) | \n",
    "        (hospital_diagnosis_df['poa_present'].isna()))\n",
    ")\n",
    "hosp_ids_with_esrd = hospital_diagnosis_df[esrd_mask]['hospitalization_id'].unique()\n",
    "blocks_with_esrd = hospital_diagnosis_df[esrd_mask]['encounter_block'].unique()\n",
    "\n",
    "print(f\"Hospitalizations with ESRD (including NA present_on_admission): {len(hosp_ids_with_esrd)}\")\n",
    "\n",
    "\n",
    "strobe_counts['3_hospitalizations_with_esrd'] = len(hosp_ids_with_esrd)\n",
    "strobe_counts['3_encounter_blocks_with_esrd'] = len(blocks_with_esrd)\n",
    "\n",
    "\n",
    "# Filter out hospitalizations with ESRD\n",
    "cohort_df = cohort_df[~cohort_df['hospitalization_id'].isin(hosp_ids_with_esrd)].copy()\n",
    "cohort_hosp_ids = set(cohort_df['hospitalization_id'].unique())\n",
    "cohort_blocks = set(cohort_df['encounter_block'].unique())\n",
    "# Create cohort subset excluding hospitalizations with ESRD\n",
    "strobe_counts['3_encounter_blocks_without_esrd'] = len(cohort_blocks)  # Count blocks without ESRD\n",
    "strobe_counts['3_hospitalizations_without_esrd'] = len(cohort_hosp_ids)  # Count hospitalizations without ESRD\n",
    "\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aec46e",
   "metadata": {},
   "source": [
    "## Step4: Data availability, and CRRT Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643037bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading labs table...\")\n",
    "clif.load_table(\n",
    "    'vitals',\n",
    "    columns=vitals_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': list(cohort_hosp_ids)\n",
    "    }\n",
    ")\n",
    "print(f\"   Vitals loaded: {len(clif.vitals.df):,} rows\")\n",
    "print(f\"   Unique vitals categories: {clif.vitals.df['vital_category'].nunique()}\")\n",
    "print(f\"   Unique vitals hospitalizations: {clif.vitals.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "clif.vitals.df = clif.vitals.df.merge(\n",
    "    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39738ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_range = clif.vitals.df.groupby('encounter_block').agg({\n",
    "    'recorded_dttm': ['min', 'max']\n",
    "}).reset_index()\n",
    "vitals_range.columns = ['encounter_block', 'first_vital_dttm', 'last_vital_dttm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd93f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where vital_category is 'weight_kg'\n",
    "weight_df = clif.vitals.df[clif.vitals.df['vital_category'] == 'weight_kg'].copy()\n",
    "# Identify the number of hospitalizations that do not have weight recorded\n",
    "hosp_with_weight = set(weight_df['hospitalization_id'].unique())\n",
    "hosp_without_weight = cohort_hosp_ids - hosp_with_weight\n",
    "print(f\"Number of hospitalizations without recorded weight: {len(hosp_without_weight)}\")\n",
    "\n",
    "clif.vitals.df = None ## clear from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6118388",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = cohort_df[~cohort_df['hospitalization_id'].isin(hosp_without_weight)].copy()\n",
    "cohort_hosp_ids = set(cohort_df['hospitalization_id'].unique())\n",
    "cohort_blocks = set(cohort_df['encounter_block'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed890818",
   "metadata": {},
   "outputs": [],
   "source": [
    "crrt_initiation = crrt_initiation[crrt_initiation['encounter_block'].isin(cohort_blocks)].copy()\n",
    "crrt_at_initiation = crrt_at_initiation[crrt_at_initiation['encounter_block'].isin(cohort_blocks)].copy()\n",
    "print(f\"   CRRT initiation blocks in cohort: {len(crrt_initiation):,}\")\n",
    "strobe_counts['4_encounter_blocks_with_weight'] = crrt_initiation['encounter_block'].nunique()\n",
    "strobe_counts['4_encounter_blocks_without_weight'] = len(cohort_blocks - set(crrt_initiation['encounter_block'].unique()))\n",
    "strobe_counts['4_hospitalizations_with_weight'] = crrt_at_initiation['hospitalization_id'].nunique()  # Count hospitalizations without weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1af127",
   "metadata": {},
   "source": [
    "## Add weights to crrt initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  Get Closest Weight to CRRT Initiation\n",
    "# ============================================================================\n",
    "print(\"\\nFinding closest weights to CRRT initiation time...\")\n",
    "if 'vital_value' in weight_df.columns:\n",
    "    weight_df = weight_df.rename(columns={'vital_value': 'weight_kg'})\n",
    "if 'vital_category' in weight_df.columns:\n",
    "    weight_df = weight_df.drop(columns=['vital_category'])\n",
    "print(f\"   Weight records available: {len(weight_df):,}\")\n",
    "\n",
    "combined = crrt_initiation.merge(weight_df, on='encounter_block', how='inner')\n",
    "\n",
    "before_mask = combined['recorded_dttm'] <= combined['crrt_initiation_time']\n",
    "combined_before = combined[before_mask].copy()\n",
    "combined_before_sorted = combined_before.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "closest_before = (combined_before_sorted\n",
    "                  .groupby('encounter_block')\n",
    "                  .last()\n",
    "                  .reset_index())\n",
    "\n",
    "all_blocks = set(combined['encounter_block'])\n",
    "blocks_with_before = set(closest_before['encounter_block'])\n",
    "blocks_missing = all_blocks - blocks_with_before\n",
    "print(\"Blocks without weight recorded before initiation time:\", len(blocks_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca480e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_mask = (\n",
    "    combined['encounter_block'].isin(blocks_missing) &\n",
    "    (combined['recorded_dttm'] > combined['crrt_initiation_time']) &\n",
    "    (combined['weight_kg'].notnull())\n",
    ")\n",
    "combined_after = combined[after_mask].copy()\n",
    "combined_after_sorted = combined_after.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "first_after = (combined_after_sorted\n",
    "               .groupby('encounter_block')\n",
    "               .first()\n",
    "               .reset_index())\n",
    "\n",
    "num_taken_after = len(first_after)\n",
    "print(f\"   Number of weights from after initiation: {num_taken_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_final = pd.concat([closest_before, first_after], axis=0, ignore_index=True)\n",
    "combined = combined_final\n",
    "\n",
    "closest_weights = (combined\n",
    "                .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "                .groupby('encounter_block')\n",
    "                .last()\n",
    "                .reset_index())\n",
    "\n",
    "closest_weights = closest_weights[['encounter_block', 'weight_kg']]\n",
    "\n",
    "print(f\"   Weights found for: {len(closest_weights):,} encounter_blocks\")\n",
    "\n",
    "# Calculate and print the number of encounter blocks for which weights were not found\n",
    "all_encounter_blocks_with_crrt = set(crrt_initiation['encounter_block'].unique())\n",
    "encounter_blocks_with_weights = set(closest_weights['encounter_block'].unique())\n",
    "encounter_blocks_without_weights = all_encounter_blocks_with_crrt - encounter_blocks_with_weights\n",
    "print(f\"   Weights NOT found for: {len(encounter_blocks_without_weights):,} encounter_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Combine CRRT Data with Weights\n",
    "# ============================================================================\n",
    "print(\"\\n6. Combining CRRT data with weights...\")\n",
    "\n",
    "index_crrt_df = crrt_at_initiation.merge(\n",
    "    closest_weights,\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"   Final dataset: {len(index_crrt_df):,} records\")\n",
    "print(f\"   Records with weights: {index_crrt_df['weight_kg'].notna().sum():,}\")\n",
    "print(f\"   Records with CRRT mode: {index_crrt_df['crrt_mode_category'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of encounters who do not have any CRRT settings documented: \n",
    "# pre_filter_replacement_fluid_rate, post_filter_replacement_fluid_rate, dialysate_flow_rate, ultrafiltration_out\n",
    "# Filter crrt_df to only include hospitalization_id present in cohort_df\n",
    "index_crrt_df = index_crrt_df[index_crrt_df['hospitalization_id'].isin(cohort_df['hospitalization_id'])]\n",
    "\n",
    "crrt_settings_cols = [\n",
    "    'pre_filter_replacement_fluid_rate',\n",
    "    'post_filter_replacement_fluid_rate',\n",
    "    'dialysate_flow_rate',\n",
    "    'ultrafiltration_out'\n",
    "]\n",
    "# Find encounter_blocks with ANY crrt settings recorded\n",
    "crrt_settings_present = index_crrt_df.groupby('encounter_block')[crrt_settings_cols].apply(\n",
    "    lambda df: df.notnull().any().any()\n",
    ")\n",
    "crrt_blocks_with_settings = set(crrt_settings_present[crrt_settings_present].index)\n",
    "crrt_blocks_without_settings = set(crrt_df['encounter_block'].unique()) - crrt_blocks_with_settings\n",
    "num_encounters_without_crrt_settings = len(crrt_blocks_without_settings)\n",
    "print(f\"Number of encounter blocks without any recorded CRRT settings: {num_encounters_without_crrt_settings}\")\n",
    "\n",
    "# Filter cohort_df to only include encounter_blocks with at least one CRRT setting recorded\n",
    "cohort_df = cohort_df[cohort_df['encounter_block'].isin(crrt_blocks_with_settings)].copy()\n",
    "cohort_hosp_ids = set(cohort_df['hospitalization_id'].unique())\n",
    "cohort_blocks = set(cohort_df['encounter_block'].unique())\n",
    "strobe_counts['5_encounter_blocks_with_crrt_settings'] = len(cohort_blocks)\n",
    "strobe_counts['5_hospitalizations_with_crrt_settings'] = len(cohort_hosp_ids)\n",
    "strobe_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f5519",
   "metadata": {},
   "source": [
    "## Step5: Labs availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labs\n",
    "labs_required_columns = [\n",
    "    'hospitalization_id',\n",
    "    'lab_result_dttm',\n",
    "    'lab_category',\n",
    "    'lab_value',\n",
    "    'lab_value_numeric'\n",
    "]\n",
    "# labs_of_interest = ['po2_arterial','pco2_arterial', 'ph_arterial','ph_venous', 'bicarbonate','so2_arterial',\n",
    "#                     'sodium', 'potassium', 'chloride', 'calcium_total', 'magnesium', 'creatinine', \n",
    "#                     'bun', 'glucose_serum', 'lactate', 'hemoglobin' ]\n",
    "\n",
    "# labs_of_interest = ['ph_arterial', 'lactate', 'bicarbonate', 'potassium']\n",
    "labs_of_interest = ['lactate', 'bicarbonate', 'potassium']\n",
    "\n",
    "print(f\"\\nLoading labs table...\")\n",
    "clif.load_table(\n",
    "    'labs',\n",
    "    columns=labs_required_columns,\n",
    "    filters={\n",
    "        'hospitalization_id': cohort_df['hospitalization_id'].unique().tolist(),\n",
    "        'lab_category': labs_of_interest\n",
    "    }\n",
    ")\n",
    "print(f\"   Labs loaded: {len(clif.labs.df):,} rows\")\n",
    "print(f\"   Unique lab categories: {clif.labs.df['lab_category'].nunique()}\")\n",
    "print(f\"   Unique lab hospitalizations: {clif.labs.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "clif.labs.df = clif.labs.df.merge(\n",
    "    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "# Get labs dataframe\n",
    "labs_df = clif.labs.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Get Most Recent Labs Within -12 hours to +3 hours of CRRT initiation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Processing Labs - Most Recent Within -12 hours to +3 hours of CRRT initiation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Count unique encounter blocks with initiation time before merging\n",
    "n_unique_encounter_blocks_before = labs_df['encounter_block'].nunique() if 'encounter_block' in labs_df.columns else 0\n",
    "print(f\"   Unique encounter blocks in labs_df before merging: {n_unique_encounter_blocks_before:,}\")\n",
    "\n",
    "# Count unique encounter blocks with initiation time available in crrt_initiation\n",
    "n_unique_encounter_blocks_with_init = index_crrt_df['encounter_block'].nunique()\n",
    "print(f\"   Unique encounter blocks with initiation time in index_crrt_df: {n_unique_encounter_blocks_with_init:,}\")\n",
    "\n",
    "# Merge with CRRT initiation times to get the reference time\n",
    "labs_with_crrt_time = labs_df.merge(\n",
    "    index_crrt_df[['encounter_block', 'crrt_initiation_time']],\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Count unique encounter blocks after merging\n",
    "n_unique_encounter_blocks_after = labs_with_crrt_time['encounter_block'].nunique()\n",
    "print(f\"   Labs after merging with CRRT cohort: {len(labs_with_crrt_time):,}\")\n",
    "print(f\"   Unique encounter blocks after merging: {n_unique_encounter_blocks_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad619857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter labs to window: -12 hours to +3 hours of CRRT initiation\n",
    "time_lower = labs_with_crrt_time['crrt_initiation_time'] -  pd.Timedelta(hours=24)\n",
    "time_upper = labs_with_crrt_time['crrt_initiation_time'] + pd.Timedelta(hours=3)\n",
    "\n",
    "labs_in_window = labs_with_crrt_time[\n",
    "    (labs_with_crrt_time['lab_result_dttm'] >= time_lower) &\n",
    "    (labs_with_crrt_time['lab_result_dttm'] <= time_upper)\n",
    "].copy()\n",
    "\n",
    "print(f\"Labs within -12h to +3h window: {len(labs_in_window)} lab records\")\n",
    "print(f\"Unique encounter_blocks with labs in window: {labs_in_window['encounter_block'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a648c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encounter_blocks with each required lab\n",
    "blocks_with_labs = {}\n",
    "for lab in labs_of_interest:\n",
    "    blocks = set(labs_in_window[labs_in_window['lab_category'] ==\n",
    "lab]['encounter_block'].unique())\n",
    "    blocks_with_labs[lab] = blocks\n",
    "    print(f\"Encounter blocks with {lab}: {len(blocks)}\")\n",
    "\n",
    "# Find encounter_blocks with ALL required labs\n",
    "blocks_with_all_labs = blocks_with_labs['lactate'].intersection(\n",
    "    blocks_with_labs['bicarbonate']).intersection(\n",
    "    blocks_with_labs['potassium'])\n",
    "\n",
    "print(f\"\\nEncounter blocks with ALL required labs:  {len(blocks_with_all_labs)}\")\n",
    "print(f\"Encounter blocks missing at least one required lab:  {len(cohort_blocks) - len(blocks_with_all_labs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cohort to only include encounter_blocks with all required labs\n",
    "\n",
    "cohort_blocks_before = len(cohort_blocks)\n",
    "cohort_blocks_after = cohort_blocks.intersection(blocks_with_all_labs)\n",
    "\n",
    "# Identify dropped encounter_blocks\n",
    "dropped_blocks = cohort_blocks - cohort_blocks_after\n",
    "print(f\"Dropped encounter_blocks for missing labs: {len(dropped_blocks)} encounter blocks\")\n",
    "\n",
    "# Save hospitalization_id and encounter_block for dropped cases\n",
    "if len(dropped_blocks) > 0:\n",
    "    dropped_encounters_df = cohort_df[cohort_df['encounter_block'].isin(dropped_blocks)][['hospitalization_id', 'encounter_block']]\n",
    "    dropped_encounters_df.to_parquet('../output/intermediate/dropped_missing_labs_blocks.parquet', index=False)\n",
    "    print(f\"Saved dropped hospitalization_id and encounter_block to ../output/intermediate/dropped_missing_labs_blocks.parquet\")\n",
    "else:\n",
    "    print(\"No encounter_blocks dropped for missing labs.\")\n",
    "\n",
    "# Update the cohort_blocks set\n",
    "cohort_blocks = cohort_blocks_after\n",
    "\n",
    "# Update cohort_df\n",
    "cohort_df = cohort_df[cohort_df['encounter_block'].isin(cohort_blocks)]\n",
    "\n",
    "print(f\"Cohort before lab filter: {cohort_blocks_before} encounter blocks\")\n",
    "print(f\"Cohort after lab filter: {len(cohort_blocks)} encounter blocks\")\n",
    "print(f\"Excluded for missing labs: {cohort_blocks_before -  len(cohort_blocks)} encounter blocks\")\n",
    "\n",
    "# Update STROBE counts\n",
    "strobe_counts['6_encounter_blocks_with_required_labs'] = len(cohort_blocks)\n",
    "\n",
    "crrt_at_initiation = crrt_at_initiation[crrt_at_initiation['encounter_block'].isin(cohort_blocks)]\n",
    "index_crrt_df = index_crrt_df[index_crrt_df['encounter_block'].isin(cohort_blocks)]\n",
    "crrt_initiation = crrt_initiation[crrt_initiation['encounter_block'].isin(cohort_blocks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5778d",
   "metadata": {},
   "source": [
    "# Cohort Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097c941",
   "metadata": {},
   "source": [
    "## AKI\n",
    "\n",
    "Majority of the cohort should have an ICD code for AKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c06241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AKI Codes Sanity check\n",
    "\n",
    "# Define AKI ICD-10 codes\n",
    "aki_codes = [\n",
    "    # ICD-10 codes for acute kidney injury\n",
    "    'n170', 'n171', 'n172', 'n178', 'n179',  # Acute kidney failure codes\n",
    "    'r34',   # Anuria and oliguria\n",
    "    'n990', # Post-procedural kidney failure\n",
    "    't795',  # Traumatic anuria\n",
    "    '5845',  # ICD9 Acute kidney failure with lesion of tubular necrosis\n",
    "    '5849',  # ICD9- Acute kidney failure, unspecified\n",
    "    \"5848\"    # ICD9 - Acute kidney failure with other specified pathological lesion in kidney\n",
    "]\n",
    "\n",
    "# Filter to non-ESRD encounters first\n",
    "non_esrd_encounters = hospital_diagnosis_df[hospital_diagnosis_df['encounter_block'].isin(cohort_df['encounter_block'])]\n",
    "\n",
    "# Create mask for AKI diagnoses on the filtered data\n",
    "aki_mask = non_esrd_encounters['diagnosis_code'].isin(aki_codes)\n",
    "\n",
    "# Get encounter blocks with AKI diagnoses\n",
    "blocks_with_aki = non_esrd_encounters[aki_mask]['encounter_block'].unique()\n",
    "total_non_esrd_blocks = cohort_df['encounter_block'].nunique()\n",
    "strobe_counts['6_encounter_blocks_with_AKI_no_esrd'] = len(blocks_with_aki) \n",
    "\n",
    "# Calculate percentage\n",
    "aki_percentage = (len(blocks_with_aki) / total_non_esrd_blocks) * 100\n",
    "\n",
    "print(f\"\\nPercentage of non-ESRD encounter blocks with AKI codes: {aki_percentage:.1f}%\")\n",
    "print(f\"({len(blocks_with_aki)} out of {total_non_esrd_blocks} blocks)\")\n",
    "strobe_counts['6_Percentage_non_ESRD_encounter_blocks_with_AKI_codes'] = aki_percentage\n",
    "# Show sample of AKI diagnoses\n",
    "aki_diagnoses = non_esrd_encounters[aki_mask][['hospitalization_id', 'diagnosis_code','poa_present']].drop_duplicates()\n",
    "print(\"\\nSample of AKI-related diagnoses found: \")\n",
    "aki_diagnoses['diagnosis_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6af78",
   "metadata": {},
   "source": [
    "## ICU\n",
    "\n",
    "Cohort should ideally be an ICU hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ADT data to only include hospitalizations in all_ids\n",
    "adt_final_stitched = adt_stitched[adt_stitched['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "adt_final_stitched = adt_final_stitched.sort_values(by=['encounter_block', 'in_dttm'])\n",
    "desired_order = ['hospitalization_id', 'encounter_block', 'hospital_id', 'in_dttm', 'out_dttm']\n",
    "remaining_cols = [col for col in adt_final_stitched.columns if col not in desired_order]\n",
    "adt_final_stitched = adt_final_stitched[desired_order + remaining_cols]\n",
    "\n",
    "print(\"\\n=== Validating ICU Administration ===\")\n",
    "\n",
    "adt_final_stitched['is_icu'] = adt_final_stitched['location_category'] == 'icu'\n",
    "\n",
    "# Check if each hospitalization had at least one ICU stay\n",
    "hosp_icu_status = adt_final_stitched.groupby('encounter_block')['is_icu'].any()\n",
    "non_icu_hosps = hosp_icu_status[~hosp_icu_status].index.tolist()\n",
    "strobe_counts[\"6_number_hosp_without_ICU_stay\"] = len(non_icu_hosps)\n",
    "print(f\"\\nNumber of CRRT hospitalizations without any ICU stay: {len(non_icu_hosps)}\")\n",
    "if len(non_icu_hosps) > 0:\n",
    "    print(\"WARNING: Found CRRT hospitalizations without ICU stays\")\n",
    "    print(\"Number of hospitalization IDs without ICU stays:\", len(non_icu_hosps), \"check crrt_non_icu_df df\")\n",
    "else:\n",
    "    print(\"All CRRT hospitalizations had at least one ICU stay\")\n",
    "\n",
    "crrt_non_icu_df = crrt_df[crrt_df['encounter_block'].isin(non_icu_hosps)]\n",
    "crrt_non_icu_df = crrt_non_icu_df.sort_values(by=['hospitalization_id', 'encounter_block', 'recorded_dttm'])\n",
    "desired_order = ['hospitalization_id', 'encounter_block', 'recorded_dttm', 'crrt_mode_category']\n",
    "remaining_cols = [col for col in crrt_non_icu_df.columns if col not in desired_order]\n",
    "crrt_non_icu_df = crrt_non_icu_df[desired_order + remaining_cols]\n",
    "adt_df_non_icu_hosps = adt_stitched[adt_stitched['encounter_block'].isin(non_icu_hosps)]\n",
    "adt_df_non_icu_hosps.to_csv('../output/intermediate/adt_df_non_icu_hosps.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc004349",
   "metadata": {},
   "source": [
    "# Strobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display strobe counts\n",
    "display(strobe_counts)\n",
    "\n",
    "# Save strobe counts to CSV in ../output/intermediate\n",
    "strobe_counts_df = pd.DataFrame(list(strobe_counts.items()), columns=['counter', 'value'])\n",
    "strobe_counts_df.to_csv('../output/final/strobe_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "def create_consort_diagram_straight_flow(\n",
    "    strobe_counts: Dict,\n",
    "    output_dir: Union[str, Path] = \"../output/final/graphs\"\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Creates a CONSORT flow diagram with a straight vertical main flow \n",
    "    and exclusions branching off to the right side, connecting from the \n",
    "    vertical line *segment between nodes* (center of vertical arrow connecting top and bottom box).\n",
    "    Exclusion arrows stop at the edge of exclusion box, not inside/overlap.\n",
    "    \"\"\"    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 0. Setup and Data Derivation (Dynamic counts)\n",
    "    # ------------------------------------------------------------------\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Fetch counts dynamically\n",
    "    get_count = lambda key, default: strobe_counts.get(key, default)\n",
    "    \n",
    "    start_n = get_count(\"1b_after_stitching\", get_count(\"1_adult_hospitalizations\", 0))\n",
    "    \n",
    "    n_crrt = get_count(\"2_crrt_blocks\", 0)\n",
    "    n_no_esrd = get_count(\"3_encounter_blocks_without_esrd\", n_crrt)\n",
    "    n_with_weight = get_count(\"4_encounter_blocks_with_weight\", n_no_esrd)\n",
    "    n_with_settings = get_count(\"5_encounter_blocks_with_crrt_settings\", n_with_weight)\n",
    "    n_with_labs = get_count(\"6_encounter_blocks_with_required_labs\", n_with_settings)\n",
    "\n",
    "    # Build step list\n",
    "    current_parent_n = start_n\n",
    "    \n",
    "    steps = []\n",
    "    \n",
    "    # 1. CRRT vs No CRRT\n",
    "    steps.append({\n",
    "        'name': 'CRRT', 'parent_n': current_parent_n, 'remaining_n': n_crrt,\n",
    "        'remaining_label': \"CRRT hospitalizations\",\n",
    "        'excluded_n': max(current_parent_n - n_crrt, 0), 'excluded_label': \"Excluded: No CRRT\"\n",
    "    })\n",
    "    current_parent_n = n_crrt\n",
    "\n",
    "    # 2. ESRD exclusion\n",
    "    steps.append({\n",
    "        'name': 'ESRD', 'parent_n': current_parent_n, 'remaining_n': n_no_esrd,\n",
    "        'remaining_label': \"After ESRD exclusion\",\n",
    "        'excluded_n': max(current_parent_n - n_no_esrd, 0), 'excluded_label': \"Excluded: ESRD diagnosis\"\n",
    "    })\n",
    "    current_parent_n = n_no_esrd\n",
    "\n",
    "    # 3. Missing weight\n",
    "    steps.append({\n",
    "        'name': 'Weight', 'parent_n': current_parent_n, 'remaining_n': n_with_weight,\n",
    "        'remaining_label': \"With documented weight\",\n",
    "        'excluded_n': max(current_parent_n - n_with_weight, 0), 'excluded_label': \"Excluded: Missing weight\"\n",
    "    })\n",
    "    current_parent_n = n_with_weight\n",
    "\n",
    "    # 4. Missing CRRT settings (Omitted if excluded_n is 0, as requested)\n",
    "    excluded_settings = max(current_parent_n - n_with_settings, 0)\n",
    "    if excluded_settings > 0:\n",
    "         steps.append({\n",
    "            'name': 'Settings', 'parent_n': current_parent_n, 'remaining_n': n_with_settings,\n",
    "            'remaining_label': \"With CRRT settings\",\n",
    "            'excluded_n': excluded_settings, 'excluded_label': \"Excluded: Missing CRRT settings\"\n",
    "        })\n",
    "         current_parent_n = n_with_settings\n",
    "    else:\n",
    "        # If skipped, the parent count for labs comes from n_with_weight\n",
    "        n_with_settings = n_with_weight\n",
    "\n",
    "    # 5. Missing required labs\n",
    "    steps.append({\n",
    "        'name': 'Labs', 'parent_n': n_with_settings, 'remaining_n': n_with_labs,\n",
    "        'remaining_label': \"With required labs\",\n",
    "        'excluded_n': max(n_with_settings - n_with_labs, 0), 'excluded_label': \"Excluded: Missing required labs\"\n",
    "    })\n",
    "    \n",
    "    # Filter out steps where the remaining count is zero, unless it's the starting count\n",
    "    steps = [step for step in steps if step['remaining_n'] > 0 or step['parent_n'] > 0]\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Figure setup and Geometry\n",
    "    # ------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Geometry (X coordinates are the critical part)\n",
    "    box_h = 0.08\n",
    "    box_w = 0.40\n",
    "    \n",
    "    # Main flow (left column) X-position\n",
    "    x_main_start = 0.05\n",
    "    x_main_center = x_main_start + box_w / 2 # 0.25\n",
    "    \n",
    "    # Exclusion flow (right column) X-position\n",
    "    x_excl_start = 0.55\n",
    "    x_excl_center = x_excl_start + box_w / 2 # 0.75\n",
    "\n",
    "    # Where to end the exclusion arrows (leave a gap before the exclusion box)\n",
    "    excl_arrow_gap = 0.015  # in axes units, tweak for clarity\n",
    "    \n",
    "    v_spacing = 0.14\n",
    "    \n",
    "    # Helper to draw rounded box with centered text\n",
    "    def draw_box(x, y, w, h, text, fontsize=11, weight=\"normal\"):\n",
    "        rect = FancyBboxPatch(\n",
    "            (x, y), w, h,\n",
    "            boxstyle=\"round,pad=0.01\",\n",
    "            linewidth=2,\n",
    "            edgecolor=\"black\",\n",
    "            facecolor=\"white\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x + w/2, y + h/2,\n",
    "            text,\n",
    "            ha=\"center\", va=\"center\",\n",
    "            fontsize=fontsize,\n",
    "            fontweight=weight,\n",
    "            wrap=True\n",
    "        )\n",
    "        return x + w/2, y # return box center x and bottom y\n",
    "\n",
    "    # Title\n",
    "    ax.text(0.5, 0.98, \"CRRT Cohort Selection\", ha=\"center\", va=\"center\",\n",
    "            fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    arrow_main = dict(arrowstyle=\"->\", lw=2, color=\"black\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Top box (Starting cohort)\n",
    "    # ------------------------------------------------------------------\n",
    "    top_y = 0.90 - box_h\n",
    "    # Top box is placed in the main column (left side)\n",
    "    top_center_x, top_bottom_y = draw_box(\n",
    "        x_main_start,\n",
    "        top_y,\n",
    "        box_w,\n",
    "        box_h,\n",
    "        \"All adult hospitalizations\\n\"\n",
    "        \"(2018-2024)\\n\"\n",
    "        f\"n = {start_n:,}\",\n",
    "        fontsize=11\n",
    "    )\n",
    "    \n",
    "    # For caching main column box vertical coordinates for exclusion elbows\n",
    "    main_box_ys = [top_y]\n",
    "    for i in range(len(steps)):\n",
    "        main_box_ys.append(top_y - ((i + 1) * v_spacing))\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Draw Rows (Remaining and Excluded)\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    # Collect the centers and box edges for exclusion elbows\n",
    "    for i, step in enumerate(steps):\n",
    "        \n",
    "        # --- A. Calculate Y coordinates ---\n",
    "        current_y = top_y - ((i + 1) * v_spacing)\n",
    "        \n",
    "        if i == 0:\n",
    "            y_parent = top_y\n",
    "        else:\n",
    "            y_parent = top_y - (i * v_spacing)\n",
    "        \n",
    "        # Y coordinates for bottom and top of the two main boxes\n",
    "        y_top_box = y_parent\n",
    "        y_bottom_box = current_y\n",
    "        # Vertical center (for arrow elbow): halfway between two box centers\n",
    "        box_center_y_top = y_top_box + (box_h / 2)\n",
    "        box_center_y_bottom = y_bottom_box + (box_h / 2)\n",
    "        arrow_vertical_center = (box_center_y_top + box_center_y_bottom) / 2\n",
    "\n",
    "        # --- B. Draw Remaining Box (Main Column) ---\n",
    "        remain_text = (\n",
    "            f\"Remaining hospitalizations\\n\"\n",
    "            f\"{step['remaining_label']}\\n\"\n",
    "            f\"n = {step['remaining_n']:,}\"\n",
    "        )\n",
    "        \n",
    "        remain_center_x, remain_bottom_y = draw_box(\n",
    "            x_main_start,\n",
    "            current_y,\n",
    "            box_w,\n",
    "            box_h,\n",
    "            remain_text,\n",
    "            fontsize=11\n",
    "        )\n",
    "        \n",
    "        # --- C. Draw Excluded Box (Right Column) ---\n",
    "        if step['excluded_n'] > 0:\n",
    "            excl_text = (\n",
    "                f\"{step['excluded_label']}\\n\"\n",
    "                f\"n = {step['excluded_n']:,}\"\n",
    "            )\n",
    "            draw_box(\n",
    "                x_excl_start,\n",
    "                arrow_vertical_center - box_h / 2, # Center box vertically at elbow\n",
    "                box_w,\n",
    "                box_h,\n",
    "                excl_text,\n",
    "                fontsize=11\n",
    "            )\n",
    "        \n",
    "        # --- D. Draw Arrows ---\n",
    "        \n",
    "        # 1. Main vertical flow line\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(remain_center_x, current_y + box_h),\n",
    "            xytext=(remain_center_x, y_parent),\n",
    "            arrowprops=arrow_main,\n",
    "        )\n",
    "        \n",
    "        # 2. Exclusion flow line (branching from vertical line to excluded box)\n",
    "        if step['excluded_n'] > 0:\n",
    "            # P1: Center of the vertical line segment (center between box centers)\n",
    "            p1 = (remain_center_x, arrow_vertical_center) \n",
    "            \n",
    "            # The arrow now ends just before the exclusion box starts (leaving a gap)\n",
    "            p2 = (x_excl_start - excl_arrow_gap, arrow_vertical_center)\n",
    "            \n",
    "            # Draw horizontal line segment from P1 to P2 with an arrowhead\n",
    "            ax.annotate(\n",
    "                \"\",\n",
    "                xy=p2,\n",
    "                xytext=p1,\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"black\"),\n",
    "                annotation_clip=False\n",
    "            )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Add analytical note box to the PNG itself\n",
    "    # ------------------------------------------------------------------\n",
    "    # Fetch AKI and ICU info from strobe_counts\n",
    "    n_non_esrd = strobe_counts.get('3_encounter_blocks_without_esrd', np.nan)\n",
    "    n_aki_no_esrd = strobe_counts.get('6_encounter_blocks_with_AKI_no_esrd', np.nan)\n",
    "    perc_aki_no_esrd = strobe_counts.get('6_Percentage_non_ESRD_encounter_blocks_with_AKI_codes', np.nan)\n",
    "    n_total_crrt_hosp = strobe_counts.get('2_crrt_hospitalizations', np.nan)\n",
    "    n_hosp_without_icu = strobe_counts.get('6_number_hosp_without_ICU_stay', np.nan)\n",
    "    n_with_labs = get_count(\"6_encounter_blocks_with_required_labs\", np.nan)\n",
    "\n",
    "    print(\"n_with_labs\", n_with_labs, n_hosp_without_icu)\n",
    "    print(\"np.isnan(n_with_labs)\",np.isnan(n_with_labs), \"np.isnan(n_hosp_without_icu)\", np.isnan(n_hosp_without_icu))\n",
    "    if not (np.isnan(n_with_labs) or np.isnan(n_hosp_without_icu)):\n",
    "        n_with_icu = int(n_with_labs) - int(n_hosp_without_icu)\n",
    "        perc_with_icu = 100.0 * n_with_icu / int(n_with_labs) if n_with_labs else np.nan\n",
    "        print(\"icu block\", n_with_icu, perc_with_icu)\n",
    "    else:\n",
    "        n_with_icu = None\n",
    "        perc_with_icu = None\n",
    "\n",
    "    # Compose note text\n",
    "    note_lines = []\n",
    "    if not (np.isnan(n_non_esrd) or np.isnan(n_aki_no_esrd) or np.isnan(perc_aki_no_esrd)):\n",
    "        note_lines.append(\n",
    "            f\"AKI codes present (non-ESRD): {int(n_aki_no_esrd):,} / {int(n_with_labs):,} ({perc_aki_no_esrd:.1f}%)\"\n",
    "        )\n",
    "    else:\n",
    "        note_lines.append(\"AKI data not available\")\n",
    "    if not (n_with_icu is None or perc_with_icu is None):\n",
    "        note_lines.append(\n",
    "            f\"CRRT hospitalizations with ICU admission: {n_with_icu:,} / {int(n_with_labs):,} ({perc_with_icu:.1f}%)\"\n",
    "        )\n",
    "    else:\n",
    "        note_lines.append(\"ICU admission data not available\")\n",
    "\n",
    "    note_text = '\\n'.join(note_lines)\n",
    "\n",
    "    # Place the note as a small text box at the bottom left, under the boxes, in the figure\n",
    "    # (y position: low enough to not collide; x position: left side)\n",
    "    fig_height = 1.0\n",
    "    y_note = 0.035  # Try bottom margin; adjust if needed\n",
    "    x_note = 0.05\n",
    "\n",
    "    # Add a subdued rectangle as background\n",
    "    bbox_width = 0.89\n",
    "    bbox_height = 0.10 if len(note_lines) > 2 else 0.085\n",
    "    fancy_rect = FancyBboxPatch(\n",
    "        (x_note, y_note - 0.03), bbox_width, bbox_height,\n",
    "        boxstyle=\"round,pad=0.01\",\n",
    "        linewidth=1,\n",
    "        edgecolor=\"#bbbbbb\",\n",
    "        facecolor=\"#f6f6ee\",\n",
    "        zorder=0\n",
    "    )\n",
    "    ax.add_patch(fancy_rect)\n",
    "\n",
    "    # Put note text on top of the box\n",
    "    ax.text(\n",
    "        x_note + bbox_width/2,\n",
    "        y_note + bbox_height/2 - 0.01,\n",
    "        note_text,\n",
    "        ha=\"center\", va=\"center\",\n",
    "        fontsize=11, fontweight=\"normal\",\n",
    "        color=\"black\",\n",
    "        wrap=True\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Save and return path\n",
    "    # ------------------------------------------------------------------\n",
    "    consort_file = output_dir / \"consort_diagram_straight_flow_right_excl.png\"\n",
    "    plt.savefig(consort_file, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    # plt.close(fig)\n",
    "\n",
    "    return consort_file\n",
    "\n",
    "create_consort_diagram_straight_flow(strobe_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ec4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d73b6f9",
   "metadata": {},
   "source": [
    "# Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OUTCOMES CALCULATION \n",
    "# ============================================================================\n",
    "\n",
    "# 1. ICU LENGTH OF STAY \n",
    "print(\"\\n1. Processing ICU segments...\")\n",
    "icu_segs = adt_final_stitched.copy()\n",
    "icu_segs = icu_segs[\n",
    "    (icu_segs['location_category'] == 'icu') &\n",
    "    (icu_segs['in_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'] > icu_segs['in_dttm'])\n",
    "]\n",
    "\n",
    "print(f\"   ICU segments identified: {len(icu_segs):,}\")\n",
    "\n",
    "# Calculate ICU LOS as sum of all ICU segment durations\n",
    "icu_los = icu_segs[icu_segs['encounter_block'].isin(cohort_df['encounter_block'])].copy()\n",
    "icu_los['seg_days'] = (icu_los['out_dttm'] - icu_los['in_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "icu_los_summary = icu_los.groupby('encounter_block').agg({\n",
    "    'seg_days': 'sum'\n",
    "}).reset_index()\n",
    "icu_los_summary.rename(columns={'seg_days': 'icu_los_days'}, inplace=True)\n",
    "\n",
    "print(f\"   Median ICU LOS: {icu_los_summary['icu_los_days'].median():.2f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HOSPITAL LENGTH OF STAY (difference between first and last vital)\n",
    "# ============================================================================\n",
    "print(\"\\n3. Calculating Hospital Length of Stay...\")\n",
    "hosp_los = cohort_df[['encounter_block']].merge(\n",
    "    vitals_range,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Hospital LOS = last_vital_dttm - first_vital_dttm\n",
    "hosp_los['hosp_los_days'] = (\n",
    "    hosp_los['last_vital_dttm'] - hosp_los['first_vital_dttm']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Ensure non-negative values\n",
    "hosp_los['hosp_los_days'] = hosp_los['hosp_los_days'].apply(\n",
    "    lambda x: max(x, 0) if pd.notna(x) and np.isfinite(x) else np.nan\n",
    ")\n",
    "\n",
    "print(f\"   Median Hospital LOS: {hosp_los['hosp_los_days'].median():.2f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DEATH STATUS AND FINAL OUTCOME DATETIME\n",
    "# ============================================================================\n",
    "print(\"\\n4. Determining death status and final outcome datetime...\")\n",
    "\n",
    "# Get discharge category and death_dttm from hospitalization and patient tables\n",
    "patient_df = clif.patient.df[['patient_id', 'death_dttm', 'race_category', 'sex_category', 'ethnicity_category']]\n",
    "\n",
    "death_info = cohort_df.merge(\n",
    "    hosp_df[['hospitalization_id', 'patient_id', 'discharge_category', 'age_at_admission', 'admission_type_category']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ").merge(\n",
    "    patient_df,\n",
    "    on='patient_id',\n",
    "    how='left'\n",
    ").merge(\n",
    "    vitals_range,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop 'hospitalization_id' from death_info\n",
    "if 'hospitalization_id' in death_info.columns:\n",
    "    death_info = death_info.drop(columns=['hospitalization_id'])\n",
    "\n",
    "# Collapse to unique encounter_block, aggregating required columns\n",
    "death_info = death_info.sort_values('encounter_block')  \n",
    "\n",
    "agg_dict = {\n",
    "    'admission_type_category': 'last',\n",
    "    'discharge_category': 'last',\n",
    "    'race_category': 'last',\n",
    "    'sex_category': 'last',\n",
    "    'ethnicity_category': 'last',\n",
    "    'death_dttm': 'last',\n",
    "    'first_vital_dttm': 'min',\n",
    "    'last_vital_dttm': 'max'\n",
    "}\n",
    "\n",
    "# Include all other columns not being aggregated with \"first\" to keep at least one value per group, unless they are non-aggregatable\n",
    "for col in death_info.columns:\n",
    "    if col not in agg_dict and col not in ['encounter_block']:\n",
    "        agg_dict[col] = 'first'\n",
    "\n",
    "death_info = death_info.groupby('encounter_block', as_index=False).agg(agg_dict)\n",
    "\n",
    "# Standardize discharge category\n",
    "death_info['discharge_category'] = death_info['discharge_category'].str.lower()\n",
    "\n",
    "# Step 1: Determine if patient died (based on discharge_category)\n",
    "death_info['died'] = death_info['discharge_category'].isin(['expired', 'hospice']).astype(int)\n",
    "\n",
    "# Step 2: Determine final_outcome_dttm\n",
    "# If died: use death_dttm if available, otherwise use last_vital_dttm\n",
    "# If not died: use last_vital_dttm\n",
    "death_info['final_outcome_dttm'] = (\n",
    "    death_info['death_dttm']\n",
    "    .fillna(death_info['last_vital_dttm'])  # Fallback to last_vital\n",
    "    .where(death_info['died'] == 1, pd.NaT)  # Only keep for died==1, else NaT\n",
    ")\n",
    "\n",
    "print(f\"   Patients identified as died (expired/hospice): {death_info['died'].sum():,}\")\n",
    "\n",
    "num_with_death_dttm = ((death_info['died'] == 1) & (death_info['death_dttm'].notna())).sum()\n",
    "num_using_last_vital = ((death_info['died'] == 1) & (death_info['death_dttm'].isna())).sum()\n",
    "\n",
    "print(f\"   - With death_dttm: {num_with_death_dttm:,}\")\n",
    "print(f\"   - Using last_vital_dttm: {num_using_last_vital:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MORTALITY CALCULATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n5. Calculating mortality outcomes...\")\n",
    "\n",
    "# In-hospital death: died AND final_outcome_dttm is between first and last vital\n",
    "death_info['in_hosp_death'] = (\n",
    "    (death_info['died'] == 1) &\n",
    "    (death_info['final_outcome_dttm'].notna()) &\n",
    "    (death_info['final_outcome_dttm'] >= death_info['first_vital_dttm']) &\n",
    "    (death_info['final_outcome_dttm'] <= death_info['last_vital_dttm'])\n",
    ").astype(int)\n",
    "\n",
    "# 30-day mortality: died AND final_outcome_dttm within 30 days of first vital\n",
    "death_info['death_30d'] = (\n",
    "    (death_info['died'] == 1) &\n",
    "    (death_info['final_outcome_dttm'].notna()) &\n",
    "    (death_info['final_outcome_dttm'] <= (death_info['first_vital_dttm'] + pd.Timedelta(days=30)))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "print(f\"   In-hospital deaths: {death_info['in_hosp_death'].sum():,} ({death_info['in_hosp_death'].mean()*100:.1f}%)\")\n",
    "print(f\"   30-day deaths: {death_info['death_30d'].sum():,} ({death_info['death_30d'].mean()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. COMBINE ALL OUTCOMES\n",
    "# ============================================================================\n",
    "print(\"\\n6. Combining all outcomes...\")\n",
    "outcomes_df = cohort_df[['hospitalization_id', 'encounter_block']].merge(\n",
    "    icu_los_summary, on='encounter_block', how='left'\n",
    ").merge(\n",
    "    hosp_los[['encounter_block', 'hosp_los_days']], on='encounter_block', how='left'\n",
    ").merge(\n",
    "    death_info, on='encounter_block', how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal outcomes dataset:\")\n",
    "print(f\"   Total records: {len(outcomes_df):,}\")\n",
    "print(f\"   Records with ICU LOS: {outcomes_df['icu_los_days'].notna().sum():,}\")\n",
    "print(f\"   Records with Hospital LOS: {outcomes_df['hosp_los_days'].notna().sum():,}\")\n",
    "print(f\"   In-hospital mortality rate: {outcomes_df['in_hosp_death'].mean()*100:.1f}%\")\n",
    "print(f\"   30-day mortality rate: {outcomes_df['death_30d'].mean()*100:.1f}%\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTCOMES SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ICU LOS (days):\")\n",
    "print(f\"  Median [IQR]: {outcomes_df['icu_los_days'].median():.1f} [{outcomes_df['icu_los_days'].quantile(0.25):.1f}-{outcomes_df['icu_los_days'].quantile(0.75):.1f}]\")\n",
    "print(f\"\\nHospital LOS (days):\")\n",
    "print(f\"  Median [IQR]: {outcomes_df['hosp_los_days'].median():.1f} [{outcomes_df['hosp_los_days'].quantile(0.25):.1f}-{outcomes_df['hosp_los_days'].quantile(0.75):.1f}]\")\n",
    "print(f\"\\nMortality:\")\n",
    "print(f\"  In-hospital: {outcomes_df['in_hosp_death'].sum():,}/{len(outcomes_df):,} ({outcomes_df['in_hosp_death'].mean()*100:.1f}%)\")\n",
    "print(f\"  30-day: {outcomes_df['death_30d'].sum():,}/{len(outcomes_df):,} ({outcomes_df['death_30d'].mean()*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert specified columns to lowercase (if they exist)\n",
    "category_cols = [\n",
    "    'admission_type_category', 'discharge_category',\n",
    "    'race_category', 'sex_category', 'ethnicity_category'\n",
    "]\n",
    "for col in category_cols:\n",
    "    if col in outcomes_df.columns:\n",
    "        outcomes_df[col] = outcomes_df[col].str.lower()\n",
    "\n",
    "# Arrange columns: patient_id, hospitalization_id, encounter_block, then everything else\n",
    "front_cols = [col for col in ['patient_id', 'hospitalization_id', 'encounter_block'] if col in outcomes_df.columns]\n",
    "other_cols = [col for col in outcomes_df.columns if col not in front_cols]\n",
    "outcomes_df = outcomes_df[front_cols + other_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0586c15",
   "metadata": {},
   "source": [
    "# CRRT Dose\n",
    "\n",
    "Calculate the dose for each time point , and then take the median of first 3 hours for the dose and the initiation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169aa760",
   "metadata": {},
   "outputs": [],
   "source": [
    "crrt_cohort = crrt_cohort[crrt_cohort['encounter_block'].isin(cohort_blocks)]\n",
    "\n",
    "# Define desired column order\n",
    "desired_order = [\n",
    "    'hospitalization_id', 'encounter_block', 'recorded_dttm', \n",
    "    'crrt_mode_category', 'dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', \n",
    "    'post_filter_replacement_fluid_rate', 'ultrafiltration_out', \n",
    "    'blood_flow_rate', 'crrt_initiation_time'\n",
    "]\n",
    "# Only keep columns that exist in crrt_cohort\n",
    "front_cols = [col for col in desired_order if col in crrt_cohort.columns]\n",
    "other_cols = [col for col in crrt_cohort.columns if col not in front_cols]\n",
    "crrt_cohort = crrt_cohort[front_cols + other_cols]\n",
    "\n",
    "# Sort as specified\n",
    "crrt_cohort = crrt_cohort.sort_values(['encounter_block', 'recorded_dttm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Calculating CRRT dose at initiation...\")\n",
    "\n",
    "# First, filter for valid CRRT modes only\n",
    "valid_modes = ['cvvh', 'cvvhd', 'cvvhdf']\n",
    "crrt_cohort['crrt_mode_category'] = crrt_cohort['crrt_mode_category'].str.lower()\n",
    "\n",
    "print(f\"   Total CRRT records before filtering: {len(crrt_df):,}\")\n",
    "crrt_df_filtered = crrt_cohort[crrt_cohort['crrt_mode_category'].isin(valid_modes)].copy()\n",
    "print(f\"   Records after filtering for valid modes (cvvh, cvvhd, cvvhdf): {len(crrt_df_filtered):,}\")\n",
    "print(f\"   Excluded records: {len(crrt_df) - len(crrt_df_filtered):,}\")\n",
    "\n",
    "# Fill NaN values with 0 for flow rate columns\n",
    "flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate',\n",
    "            'post_filter_replacement_fluid_rate']\n",
    "\n",
    "# Drop rows where all 3 variables are missing\n",
    "crrt_df_filtered = crrt_df_filtered.dropna(subset=flow_cols, how='all')\n",
    "\n",
    "# Then fill remaining NaNs in those columns with 0\n",
    "crrt_df_filtered[flow_cols] = crrt_df_filtered[flow_cols].fillna(0)\n",
    "\n",
    "print(\"\\n   Mode distribution across all time points:\")\n",
    "print(crrt_df_filtered['crrt_mode_category'].value_counts())\n",
    "\n",
    "# Calculate mode-specific dose at EACH time point\n",
    "conditions = [\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvhd',\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvh',\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvhdf'\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    crrt_df_filtered['dialysate_flow_rate'],\n",
    "    crrt_df_filtered['pre_filter_replacement_fluid_rate'] + crrt_df_filtered['post_filter_replacement_fluid_rate'],\n",
    "    crrt_df_filtered['dialysate_flow_rate'] + crrt_df_filtered['pre_filter_replacement_fluid_rate'] +\n",
    "    crrt_df_filtered['post_filter_replacement_fluid_rate']\n",
    "]\n",
    "\n",
    "# Mode-specific total flow rate at each time point\n",
    "crrt_df_filtered['total_flow_rate'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "# Also calculate full flow rate (all components) at each time point\n",
    "crrt_df_filtered['total_flow_rate_full'] = (\n",
    "    crrt_df_filtered['dialysate_flow_rate'] +\n",
    "    crrt_df_filtered['pre_filter_replacement_fluid_rate'] +\n",
    "    crrt_df_filtered['post_filter_replacement_fluid_rate']\n",
    ")\n",
    "\n",
    "# Merge weight data (assuming weight_df has encounter_block and weight_kg)\n",
    "crrt_df_filtered = crrt_df_filtered.merge(\n",
    "    closest_weights[['encounter_block', 'weight_kg']].drop_duplicates(),\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate dose at each time point\n",
    "crrt_df_filtered['crrt_dose_ml_kg_hr'] = np.where(\n",
    "    (crrt_df_filtered['weight_kg'] > 0) & (crrt_df_filtered['total_flow_rate'] > 0),\n",
    "    crrt_df_filtered['total_flow_rate'] / crrt_df_filtered['weight_kg'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "crrt_df_filtered['crrt_dose_ml_kg_hr_full'] = np.where(\n",
    "    (crrt_df_filtered['weight_kg'] > 0) & (crrt_df_filtered['total_flow_rate_full'] > 0),\n",
    "    crrt_df_filtered['total_flow_rate_full'] / crrt_df_filtered['weight_kg'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "print(f\"\\n   Dose calculations at individual time points:\")\n",
    "print(f\"     Mode-specific doses calculated: {crrt_df_filtered['crrt_dose_ml_kg_hr'].notna().sum():,}\")\n",
    "print(f\"     Full doses calculated: {crrt_df_filtered['crrt_dose_ml_kg_hr_full'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61306f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Calculate Median Dose for First 3 Hours + Get Initiation Values\n",
    "# ============================================================================\n",
    "\n",
    "# Filter to first 3 hours after CRRT initiation\n",
    "crrt_first_3hrs = crrt_df_filtered[\n",
    "    crrt_df_filtered['recorded_dttm'] <= (crrt_df_filtered['crrt_initiation_time'] + pd.Timedelta(hours=3))\n",
    "].copy()\n",
    "\n",
    "print(f\"   Records within first 3 hours: {len(crrt_first_3hrs):,}\")\n",
    "\n",
    "# Define columns to aggregate\n",
    "dose_columns = ['dialysate_flow_rate', 'blood_flow_rate', \n",
    "                'pre_filter_replacement_fluid_rate', \n",
    "                'post_filter_replacement_fluid_rate', 'ultrafiltration_out',\n",
    "                'total_flow_rate', 'total_flow_rate_full',\n",
    "                'crrt_dose_ml_kg_hr', 'crrt_dose_ml_kg_hr_full']\n",
    "\n",
    "# Calculate medians for first 3 hours\n",
    "median_3hr = crrt_first_3hrs.groupby('encounter_block').agg({\n",
    "    'hospitalization_id': 'first',\n",
    "    'crrt_initiation_time': 'first',\n",
    "    'weight_kg': 'first',\n",
    "    'crrt_mode_category': lambda x: x.mode()[0] if not x.empty else np.nan,  # Most frequent mode\n",
    "    **{col: 'median' for col in dose_columns}\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"   Encounters with median values calculated: {len(median_3hr):,}\")\n",
    "\n",
    "# Get values at initiation time (original values)\n",
    "print(\"\\n   Getting original values at initiation time...\")\n",
    "\n",
    "crrt_at_init = crrt_df_filtered[\n",
    "    crrt_df_filtered['recorded_dttm'] == crrt_df_filtered['crrt_initiation_time']\n",
    "].copy()\n",
    "\n",
    "# Group by encounter_block and take first (should be unique at initiation time)\n",
    "init_values = crrt_at_init.groupby('encounter_block').agg({\n",
    "    col: 'first' for col in dose_columns\n",
    "}).reset_index()\n",
    "\n",
    "# Rename init columns to add _not_avged suffix\n",
    "init_values = init_values.rename(columns={\n",
    "    col: f'{col}_not_avged' for col in dose_columns\n",
    "})\n",
    "\n",
    "print(f\"   Encounters with initiation values: {len(init_values):,}\")\n",
    "\n",
    "# Merge median and initiation values\n",
    "final_df = median_3hr.merge(init_values, on='encounter_block', how='left')\n",
    "\n",
    "# Now arrange columns in the requested order\n",
    "# First the main columns (with median values)\n",
    "main_columns = [\n",
    "    'encounter_block', 'hospitalization_id', 'crrt_initiation_time',\n",
    "    'weight_kg', 'crrt_mode_category',\n",
    "    'dialysate_flow_rate', 'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate',\n",
    "    'ultrafiltration_out', 'total_flow_rate', 'total_flow_rate_full',\n",
    "    'crrt_dose_ml_kg_hr', 'crrt_dose_ml_kg_hr_full'\n",
    "]\n",
    "\n",
    "# Then the initiation columns (with _not_avged suffix)\n",
    "not_avged_columns = [f'{col}_not_avged' for col in dose_columns]\n",
    "\n",
    "# Combine all columns\n",
    "all_columns = main_columns + not_avged_columns\n",
    "\n",
    "# Select and reorder columns\n",
    "final_df = final_df[all_columns]\n",
    "\n",
    "print(f\"\\n   Final dataframe created:\")\n",
    "print(f\"     Total rows: {len(final_df):,} (one per encounter)\")\n",
    "print(f\"     Total columns: {len(final_df.columns)}\")\n",
    "\n",
    "# Assign to your desired variable name\n",
    "index_crrt_df = final_df.copy()\n",
    "\n",
    "print(\"\\n✅ Final dataframe created with one row per encounter!\")\n",
    "print(f\"   Stored as 'index_crrt_df' with {len(index_crrt_df)} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization: Overlaid Histograms of Median vs Initiation Doses\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Creating Overlaid Histogram Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set style for better visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create a single figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Remove NaN values for cleaner plotting\n",
    "dose_full_median = final_df['crrt_dose_ml_kg_hr_full'].dropna()\n",
    "dose_full_init = final_df['crrt_dose_ml_kg_hr_full_not_avged'].dropna()\n",
    "\n",
    "# Create overlaid histograms\n",
    "ax.hist(dose_full_median, bins=30, alpha=0.5, label='Median (First 3hr)',\n",
    "        color='blue', edgecolor='darkblue', density=True)\n",
    "ax.hist(dose_full_init, bins=30, alpha=0.5, label='At Initiation',\n",
    "        color='red', edgecolor='darkred', density=True)\n",
    "\n",
    "# Add vertical lines for means\n",
    "ax.axvline(dose_full_median.mean(), color='blue', linestyle='--',\n",
    "            linewidth=2, label=f'Mean (3hr): {dose_full_median.mean():.1f}')\n",
    "ax.axvline(dose_full_init.mean(), color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Mean (Init): {dose_full_init.mean():.1f}')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('CRRT Dose (mL/kg/hr)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Full CRRT Dose: Median (3hr) vs Initiation', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Save figure\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"../output/final/dose_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot 2: Mode-Specific CRRT Dose Comparison by Mode Category\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get unique mode categories (excluding NaN)\n",
    "mode_categories = final_df['crrt_mode_category'].dropna().unique()\n",
    "mode_categories = sorted(mode_categories)  # Sort for consistent ordering\n",
    "\n",
    "# Create figure with subplots for each mode\n",
    "n_modes = len(mode_categories)\n",
    "fig, axes = plt.subplots(1, n_modes, figsize=(6*n_modes, 6))\n",
    "\n",
    "# If only one mode, make axes iterable\n",
    "if n_modes == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Color palette\n",
    "colors_median = ['steelblue', 'royalblue', 'dodgerblue']\n",
    "colors_init = ['coral', 'salmon', 'lightsalmon']\n",
    "\n",
    "# Process each mode\n",
    "for idx, mode in enumerate(mode_categories):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Filter data for this mode\n",
    "    mode_data = final_df[final_df['crrt_mode_category'] == mode]\n",
    "\n",
    "    # Get dose values for this mode\n",
    "    dose_median = mode_data['crrt_dose_ml_kg_hr'].dropna()\n",
    "    dose_init = mode_data['crrt_dose_ml_kg_hr_not_avged'].dropna()\n",
    "\n",
    "    # Skip if no data\n",
    "    if len(dose_median) == 0 and len(dose_init) == 0:\n",
    "        ax.text(0.5, 0.5, f'No data for {mode.upper()}',\n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f'{mode.upper()}', fontsize=14, fontweight='bold')\n",
    "        continue\n",
    "\n",
    "    # Create overlaid histograms\n",
    "    if len(dose_median) > 0:\n",
    "        ax.hist(dose_median, bins=20, alpha=0.5, label=f'Median 3hr (n={len(dose_median)})',\n",
    "                color=colors_median[idx % len(colors_median)], edgecolor='darkblue', density=True)\n",
    "        ax.axvline(dose_median.mean(), color=colors_median[idx % len(colors_median)],\n",
    "                    linestyle='--', linewidth=2, label=f'Mean 3hr: {dose_median.mean():.1f}')\n",
    "\n",
    "    if len(dose_init) > 0:\n",
    "        ax.hist(dose_init, bins=20, alpha=0.5, label=f'At Init (n={len(dose_init)})',\n",
    "                color=colors_init[idx % len(colors_init)], edgecolor='darkred', density=True)\n",
    "        ax.axvline(dose_init.mean(), color=colors_init[idx % len(colors_init)],\n",
    "                    linestyle='--', linewidth=2, label=f'Mean Init: {dose_init.mean():.1f}')\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('CRRT Dose (mL/kg/hr)', fontsize=11)\n",
    "    ax.set_ylabel('Density' if idx == 0 else '', fontsize=11)\n",
    "    ax.set_title(f'{mode.upper()}\\n(n={len(mode_data)} encounters)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Mode-Specific CRRT Dose Comparison by Mode Category\\nMedian (First 3hr) vs At Initiation',\n",
    "            fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_path = '../output/final/graphs/crrt_dose_comparison_by_mode.png'\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved mode-specific comparison to: {output_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8ab44",
   "metadata": {},
   "source": [
    "# CRRT duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46518d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_crrt_duration(crrt_cohort):\n",
    "    \"\"\"\n",
    "    Calculate CRRT duration for each encounter.\n",
    "    Duration is defined as the time from crrt_initiation_time to the last recorded setting,\n",
    "    considering CRRT ended when there's a 24-hour gap in recordings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crrt_cohort : pd.DataFrame\n",
    "        DataFrame with CRRT time series data including columns:\n",
    "        - encounter_block: patient identifier\n",
    "        - crrt_initiation_time: start of CRRT\n",
    "        - recorded_dttm: timestamp column for each setting recording\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with encounter_block and calculated duration metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = crrt_cohort.copy()\n",
    "\n",
    "    # Ensure datetime columns are properly formatted\n",
    "    time_column = 'recorded_dttm'\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    df['crrt_initiation_time'] = pd.to_datetime(df['crrt_initiation_time'])\n",
    "\n",
    "    # CRITICAL: Drop all rows where recorded_dttm is before crrt_initiation_time\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[time_column] >= df['crrt_initiation_time']]\n",
    "    rows_dropped = initial_rows - len(df)\n",
    "\n",
    "    if rows_dropped > 0:\n",
    "        print(f\"Dropped {rows_dropped} recordings that occurred before CRRT initiation time\")\n",
    "        print(f\"Remaining recordings: {len(df)}\")\n",
    "\n",
    "    # Sort by encounter and time\n",
    "    df = df.sort_values(['encounter_block', time_column])\n",
    "\n",
    "    # Function to calculate duration for each encounter\n",
    "    def get_encounter_duration(group):\n",
    "        \"\"\"Calculate CRRT duration for a single encounter with 24-hour gap detection\"\"\"\n",
    "\n",
    "        # Get initiation time\n",
    "        initiation_time = group['crrt_initiation_time'].iloc[0]\n",
    "\n",
    "        # Get all recorded times (already filtered to be >= initiation_time)\n",
    "        recorded_times = group[time_column].dropna().sort_values()\n",
    "\n",
    "        if len(recorded_times) == 0:\n",
    "            # No recordings after initiation\n",
    "            return pd.Series({\n",
    "                'crrt_initiation_time': initiation_time,\n",
    "                'crrt_end_time': initiation_time,\n",
    "                'duration_hours': 0,\n",
    "                'duration_days': 0,\n",
    "                'num_recordings': 0,\n",
    "                'had_24hr_gap': False\n",
    "            })\n",
    "\n",
    "        # Check for 24-hour gaps\n",
    "        time_diffs = recorded_times.diff()\n",
    "\n",
    "        # Find if there's any gap >= 24 hours\n",
    "        gaps_24hr = time_diffs > pd.Timedelta(hours=24)\n",
    "\n",
    "        if gaps_24hr.any():\n",
    "            # CRRT ended at the last recording before the first 24-hour gap\n",
    "            first_gap_idx = gaps_24hr.idxmax()\n",
    "            # Get the index position of the gap\n",
    "            gap_position = recorded_times.index.get_loc(first_gap_idx)\n",
    "            # The end time is the recording just before the gap\n",
    "            end_time = recorded_times.iloc[gap_position - 1]\n",
    "            had_gap = True\n",
    "        else:\n",
    "            # No 24-hour gap, use the last recording\n",
    "            end_time = recorded_times.iloc[-1]\n",
    "            had_gap = False\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = end_time - initiation_time\n",
    "        duration_hours = duration.total_seconds() / 3600\n",
    "        duration_days = duration_hours / 24\n",
    "\n",
    "        # Count recordings\n",
    "        num_recordings = len(recorded_times)\n",
    "\n",
    "        return pd.Series({\n",
    "            'crrt_initiation_time': initiation_time,\n",
    "            'crrt_end_time': end_time,\n",
    "            'duration_hours': duration_hours,\n",
    "            'duration_days': duration_days,\n",
    "            'num_recordings': num_recordings,\n",
    "            'had_24hr_gap': had_gap\n",
    "        })\n",
    "\n",
    "    # Apply to each encounter\n",
    "    duration_df = df.groupby('encounter_block').apply(get_encounter_duration).reset_index()\n",
    "\n",
    "    # Add summary statistics\n",
    "    print(\"\\n=== CRRT Duration Summary ===\")\n",
    "    print(f\"Total encounters: {len(duration_df)}\")\n",
    "    print(f\"Encounters with recordings: {len(duration_df[duration_df['num_recordings'] > 0])}\")\n",
    "    print(f\"Encounters without recordings: {len(duration_df[duration_df['num_recordings'] == 0])}\")\n",
    "\n",
    "    # Stats for encounters with recordings\n",
    "    valid_durations = duration_df[duration_df['duration_hours'] > 0]\n",
    "\n",
    "    if len(valid_durations) > 0:\n",
    "        print(f\"\\nDuration Statistics (for {len(valid_durations)} encounters with valid recordings):\")\n",
    "        print(f\"\\nDuration (hours):\")\n",
    "        print(f\"  Mean: {valid_durations['duration_hours'].mean():.1f}\")\n",
    "        print(f\"  Median: {valid_durations['duration_hours'].median():.1f}\")\n",
    "        print(f\"  Q25: {valid_durations['duration_hours'].quantile(0.25):.1f}\")\n",
    "        print(f\"  Q75: {valid_durations['duration_hours'].quantile(0.75):.1f}\")\n",
    "        print(f\"  Min: {valid_durations['duration_hours'].min():.1f}\")\n",
    "        print(f\"  Max: {valid_durations['duration_hours'].max():.1f}\")\n",
    "\n",
    "        print(f\"\\nDuration (days):\")\n",
    "        print(f\"  Mean: {valid_durations['duration_days'].mean():.1f}\")\n",
    "        print(f\"  Median: {valid_durations['duration_days'].median():.1f}\")\n",
    "        print(f\"  Q25: {valid_durations['duration_days'].quantile(0.25):.1f}\")\n",
    "        print(f\"  Q75: {valid_durations['duration_days'].quantile(0.75):.1f}\")\n",
    "\n",
    "    print(f\"\\nEncounters with 24-hour gap: {duration_df['had_24hr_gap'].sum()} ({duration_df['had_24hr_gap'].mean()*100:.1f}%)\")\n",
    "\n",
    "    # Add duration categories\n",
    "    duration_df['duration_category'] = pd.cut(\n",
    "        duration_df['duration_days'],\n",
    "        bins=[-0.001, 0, 1, 3, 7, 14, float('inf')],\n",
    "        labels=['No duration', '<1 day', '1-3 days', '3-7 days', '7-14 days', '>14 days'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDuration categories:\")\n",
    "    print(duration_df['duration_category'].value_counts().sort_index())\n",
    "\n",
    "    return duration_df\n",
    "\n",
    "# Pre-processing function to check for pre-initiation recordings\n",
    "def check_pre_initiation_recordings(crrt_cohort):\n",
    "    \"\"\"\n",
    "    Check how many recordings occur before CRRT initiation time\n",
    "    \"\"\"\n",
    "    df = crrt_cohort.copy()\n",
    "    df['recorded_dttm'] = pd.to_datetime(df['recorded_dttm'])\n",
    "    df['crrt_initiation_time'] = pd.to_datetime(df['crrt_initiation_time'])\n",
    "\n",
    "    # Find pre-initiation recordings\n",
    "    pre_init = df[df['recorded_dttm'] < df['crrt_initiation_time']]\n",
    "\n",
    "    if len(pre_init) > 0:\n",
    "        print(\"=== Pre-Initiation Recordings Found ===\")\n",
    "        print(f\"Total pre-initiation recordings: {len(pre_init)} ({len(pre_init)/len(df)*100:.1f}% of all recordings)\")\n",
    "        print(f\"Affected encounters: {pre_init['encounter_block'].nunique()}\")\n",
    "\n",
    "        # Calculate how early these recordings are\n",
    "        pre_init['hours_before'] = (pre_init['crrt_initiation_time'] - pre_init['recorded_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "        print(f\"\\nTiming statistics (hours before initiation):\")\n",
    "        print(f\"  Mean: {pre_init['hours_before'].mean():.1f} hours\")\n",
    "        print(f\"  Median: {pre_init['hours_before'].median():.1f} hours\")\n",
    "        print(f\"  Max: {pre_init['hours_before'].max():.1f} hours\")\n",
    "\n",
    "        # Show a few examples\n",
    "        print(\"\\nExample pre-initiation recordings:\")\n",
    "        sample = pre_init.nlargest(5, 'hours_before')[['encounter_block', 'recorded_dttm', 'crrt_initiation_time', 'hours_before']]\n",
    "        print(sample)\n",
    "    else:\n",
    "        print(\"No pre-initiation recordings found - data is clean!\")\n",
    "\n",
    "    return pre_init\n",
    "\n",
    "# First check for pre-initiation recordings (optional)\n",
    "pre_init_check = check_pre_initiation_recordings(crrt_cohort)\n",
    "\n",
    "# Calculate duration (automatically drops pre-initiation recordings)\n",
    "duration_df = calculate_crrt_duration(crrt_cohort)\n",
    "\n",
    "# Merge back with main cohort\n",
    "index_crrt_df = index_crrt_df.merge(\n",
    "    duration_df[['encounter_block', 'duration_hours', 'duration_days', 'duration_category', 'had_24hr_gap']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdfe96f",
   "metadata": {},
   "source": [
    "# Respiratory support\n",
    "\n",
    "For duration on IMV, start time as the first IMV row, and end time as not on IMV for 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading respiratory support table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'respiratory_support',\n",
    "        filters={'hospitalization_id': list(cohort_df[\"hospitalization_id\"].unique())}\n",
    "    )\n",
    "    print(f\"   respiratory_support loaded: {len(clif.respiratory_support.df):,} rows\")\n",
    "    print(f\"   Unique respiratory_support hospitalizations: {clif.respiratory_support.df['hospitalization_id'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   CRRT therapy not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clif.respiratory_support.df = clif.respiratory_support.df.merge(\n",
    "    clif.encounter_mapping[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "clif.respiratory_support = clif.respiratory_support.waterfall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20654793",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support_df = clif.respiratory_support.df\n",
    "# del clif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_support_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imv_duration(resp_support_df):\n",
    "    \"\"\"\n",
    "    Calculate IMV duration for each encounter block.\n",
    "    Duration is from first IMV recording to last IMV recording,\n",
    "    considering IMV ended when there's a 24-hour gap without IMV.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resp_support_df : pd.DataFrame\n",
    "        DataFrame with columns: encounter_block, recorded_dttm, device_category\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with encounter_block and IMV duration metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = resp_support_df.copy()\n",
    "\n",
    "    # Convert recorded_dttm to datetime\n",
    "    df['recorded_dttm'] = pd.to_datetime(df['recorded_dttm'])\n",
    "\n",
    "    # Filter for IMV records only (case-insensitive)\n",
    "    df['device_category_lower'] = df['device_category'].str.lower()\n",
    "    imv_df = df[df['device_category_lower'] == 'imv'].copy()\n",
    "\n",
    "    # Sort by encounter block and time\n",
    "    imv_df = imv_df.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "    # Function to calculate duration for each encounter block\n",
    "    def get_imv_duration(group):\n",
    "        \"\"\"Calculate IMV duration for a single encounter block\"\"\"\n",
    "\n",
    "        # Get all IMV recording times\n",
    "        recorded_times = group['recorded_dttm'].dropna().sort_values()\n",
    "\n",
    "        if len(recorded_times) == 0:\n",
    "            # No IMV recordings\n",
    "            return pd.Series({\n",
    "                'imv_start_time': pd.NaT,\n",
    "                'imv_end_time': pd.NaT,\n",
    "                'imv_duration_hours': 0,\n",
    "                'imv_duration_days': 0\n",
    "            })\n",
    "\n",
    "        # IMV start time is the first recording\n",
    "        imv_start_time = recorded_times.iloc[0]\n",
    "\n",
    "        # Check for 24-hour gaps\n",
    "        time_diffs = recorded_times.diff()\n",
    "\n",
    "        # Find if there's any gap >= 24 hours\n",
    "        gaps_24hr = time_diffs > pd.Timedelta(hours=24)\n",
    "\n",
    "        if gaps_24hr.any():\n",
    "            # IMV ended at the last recording before the first 24-hour gap\n",
    "            first_gap_idx = gaps_24hr.idxmax()\n",
    "            gap_position = recorded_times.index.get_loc(first_gap_idx)\n",
    "            imv_end_time = recorded_times.iloc[gap_position - 1]\n",
    "        else:\n",
    "            # No 24-hour gap, use the last recording\n",
    "            imv_end_time = recorded_times.iloc[-1]\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = imv_end_time - imv_start_time\n",
    "        duration_hours = duration.total_seconds() / 3600\n",
    "        duration_days = duration_hours / 24\n",
    "\n",
    "        return pd.Series({\n",
    "            'imv_start_time': imv_start_time,\n",
    "            'imv_end_time': imv_end_time,\n",
    "            'imv_duration_hours': duration_hours,\n",
    "            'imv_duration_days': duration_days\n",
    "        })\n",
    "\n",
    "    # Apply to each encounter block\n",
    "    imv_duration_df = imv_df.groupby('encounter_block').apply(get_imv_duration).reset_index()\n",
    "\n",
    "    return imv_duration_df\n",
    "\n",
    "# Usage:\n",
    "imv_duration_df = calculate_imv_duration(resp_support_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "imv_duration_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back with main cohort\n",
    "index_crrt_df = index_crrt_df.merge(\n",
    "    imv_duration_df[['encounter_block', 'imv_duration_hours', 'imv_duration_days']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958f23d",
   "metadata": {},
   "source": [
    "# Save Intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df.to_parquet(\"../output/intermediate/cohort_df.parquet\", index=False)\n",
    "outcomes_df.to_parquet(\"../output/intermediate/outcomes_df.parquet\", index=False)\n",
    "# Filter weight_df to hospitalization_ids present in cohort_df before saving\n",
    "weight_df_filtered = weight_df[weight_df[\"hospitalization_id\"].isin(cohort_df[\"hospitalization_id\"])]\n",
    "weight_df_filtered.to_parquet(\"../output/intermediate/weight_df.parquet\", index=False)\n",
    "# save or use crrt_initiation df \n",
    "crrt_initiation.to_parquet(\"../output/intermediate/crrt_initiation.parquet\", index=False)\n",
    "crrt_at_initiation.to_parquet(\"../output/intermediate/crrt_at_initiation.parquet\", index=False)\n",
    "index_crrt_df.to_parquet(\"../output/intermediate/index_crrt_df.parquet\", index=False)\n",
    "crrt_cohort.to_parquet(\"../output/intermediate/crrt_cohort.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b69ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

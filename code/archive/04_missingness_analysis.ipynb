{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missingness Analysis for CRRT Cohort\n",
    "\n",
    "This notebook analyzes the characteristics of encounters dropped due to missing labs compared to those retained in the final analysis.\n",
    "\n",
    "## Objectives:\n",
    "1. Compare demographics between dropped and retained encounters\n",
    "2. Compare mortality outcomes (30-day, 90-day, in-hospital)\n",
    "3. Compare CRRT parameters and clinical characteristics\n",
    "4. Assess potential selection bias introduced by lab requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "## import outlier json\n",
    "# with open('../config/outlier_config.json', 'r', encoding='utf-8') as f:\n",
    "#     outlier_cfg = json.load(f)\n",
    "\n",
    "print(f\"\\n=� Configuration:\")\n",
    "print(f\"   Data directory: {config['tables_path']}\")\n",
    "print(f\"   File type: {config['file_type']}\")\n",
    "print(f\"   Timezone: {config['timezone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dropped Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dropped encounters\n",
    "dropped_encounters = pd.read_parquet('../output/intermediate/dropped_missing_labs_blocks.parquet')\n",
    "print(f\"  Loaded {len(dropped_encounters):,} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_hosp_ids = dropped_encounters['hospitalization_id'].unique().tolist()\n",
    "adult_encounter_blocks= dropped_encounters['encounter_block'].unique().tolist()\n",
    "print(f\"Extracted {len(adult_hosp_ids):,} unique adult hospitalization IDs from dropped encounters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 0: Load Core Tables (Patient, Hospitalization, ADT)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loading CLIF Tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from clifpy.clif_orchestrator import ClifOrchestrator\n",
    "\n",
    "# Initialize ClifOrchestrator\n",
    "clif = ClifOrchestrator(\n",
    "    data_directory=config['tables_path'],\n",
    "    filetype=config['file_type'],\n",
    "    timezone=config['timezone']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Step 0: Load Core Tables (Patient, Hospitalization, ADT)\")\n",
    "print(\"=\" * 80)\n",
    "core_tables = ['patient', 'hospitalization', 'adt']\n",
    "\n",
    "print(f\"\\nLoading {len(core_tables)} core tables...\")\n",
    "for table_name in core_tables:\n",
    "    print(f\"   Loading {table_name}...\", end=\" \")\n",
    "    try:\n",
    "        clif.load_table(table_name, )\n",
    "        table = getattr(clif, table_name)\n",
    "        print(f\"✓ ({len(table.df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nCore tables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter adt table to only those hospitalization_ids in adult_hosp_ids\n",
    "adt_final_stitched = clif.adt.df[clif.adt.df['hospitalization_id'].isin(adult_hosp_ids)].copy()\n",
    "\n",
    "# Merge in encounter_block from dropped_encounters (using hospitalization_id and encounter_block keys)\n",
    "adt_final_stitched = adt_final_stitched.merge(\n",
    "    dropped_encounters[['hospitalization_id', 'encounter_block']].drop_duplicates(),\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = clif.hospitalization.df[\n",
    "    clif.hospitalization.df[\"hospitalization_id\"].isin(adult_hosp_ids)].copy()\n",
    "\n",
    "cohort_df = cohort_df.merge(\n",
    "    clif.patient.df, \n",
    "    on=\"patient_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cohort_df = cohort_df.merge(\n",
    "    dropped_encounters,\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading vitals table...\")\n",
    "clif.load_table(\n",
    "    'vitals',\n",
    "    filters={\n",
    "        'hospitalization_id': list(adult_hosp_ids)\n",
    "    }\n",
    ")\n",
    "print(f\"   Vitals loaded: {len(clif.vitals.df):,} rows\")\n",
    "print(f\"   Unique vitals categories: {clif.vitals.df['vital_category'].nunique()}\")\n",
    "print(f\"   Unique vitals hospitalizations: {clif.vitals.df['hospitalization_id'].nunique()}\")\n",
    "\n",
    "vitals_df = clif.vitals.df.merge(\n",
    "    dropped_encounters[['hospitalization_id', 'encounter_block']].drop_duplicates(),\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "vitals_range = vitals_df.groupby('encounter_block').agg({\n",
    "    'recorded_dttm': ['min', 'max']\n",
    "}).reset_index()\n",
    "vitals_range.columns = ['encounter_block', 'first_vital_dttm', 'last_vital_dttm']\n",
    "\n",
    "# Join vitals_range with cohort_df on encounter_block\n",
    "cohort_df = cohort_df.merge(\n",
    "    vitals_range,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OUTCOMES CALCULATION \n",
    "# ============================================================================\n",
    "\n",
    "# 1. ICU LENGTH OF STAY \n",
    "print(\"\\n1. Processing ICU segments...\")\n",
    "icu_segs = adt_final_stitched.copy()\n",
    "icu_segs = icu_segs[\n",
    "    (icu_segs['location_category'] == 'icu') &\n",
    "    (icu_segs['in_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'].notna()) &\n",
    "    (icu_segs['out_dttm'] > icu_segs['in_dttm'])\n",
    "]\n",
    "\n",
    "print(f\"   ICU segments identified: {len(icu_segs):,}\")\n",
    "\n",
    "# Calculate ICU LOS as sum of all ICU segment durations\n",
    "icu_los = icu_segs[icu_segs['encounter_block'].isin(cohort_df['encounter_block'])].copy()\n",
    "icu_los['seg_days'] = (icu_los['out_dttm'] - icu_los['in_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "icu_los_summary = icu_los.groupby('encounter_block').agg({\n",
    "    'seg_days': 'sum'\n",
    "}).reset_index()\n",
    "icu_los_summary.rename(columns={'seg_days': 'icu_los_days'}, inplace=True)\n",
    "\n",
    "print(f\"   Median ICU LOS: {icu_los_summary['icu_los_days'].median():.2f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HOSPITAL LENGTH OF STAY (difference between first and last vital)\n",
    "# ============================================================================\n",
    "print(\"\\n3. Calculating Hospital Length of Stay...\")\n",
    "hosp_los = cohort_df[['encounter_block']].merge(\n",
    "    vitals_range,\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Hospital LOS = last_vital_dttm - first_vital_dttm\n",
    "hosp_los['hosp_los_days'] = (\n",
    "    hosp_los['last_vital_dttm'] - hosp_los['first_vital_dttm']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Ensure non-negative values\n",
    "hosp_los['hosp_los_days'] = hosp_los['hosp_los_days'].apply(\n",
    "    lambda x: max(x, 0) if pd.notna(x) and np.isfinite(x) else np.nan\n",
    ")\n",
    "\n",
    "print(f\"   Median Hospital LOS: {hosp_los['hosp_los_days'].median():.2f} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DEATH STATUS AND FINAL OUTCOME DATETIME\n",
    "# ============================================================================\n",
    "print(\"\\n4. Determining death status and final outcome datetime...\")\n",
    "\n",
    "# Get discharge category and death_dttm from hospitalization and patient tables\n",
    "death_info = cohort_df.copy()\n",
    "\n",
    "# Drop 'hospitalization_id' from death_info\n",
    "if 'hospitalization_id' in death_info.columns:\n",
    "    death_info = death_info.drop(columns=['hospitalization_id'])\n",
    "\n",
    "# Collapse to unique encounter_block, aggregating required columns\n",
    "death_info = death_info.sort_values('encounter_block')  \n",
    "\n",
    "agg_dict = {\n",
    "    'admission_type_category': 'last',\n",
    "    'discharge_category': 'last',\n",
    "    'race_category': 'last',\n",
    "    'sex_category': 'last',\n",
    "    'ethnicity_category': 'last',\n",
    "    'death_dttm': 'last',\n",
    "    'first_vital_dttm': 'min',\n",
    "    'last_vital_dttm': 'max'\n",
    "}\n",
    "\n",
    "# Include all other columns not being aggregated with \"first\" to keep at least one value per group, unless they are non-aggregatable\n",
    "for col in death_info.columns:\n",
    "    if col not in agg_dict and col not in ['encounter_block']:\n",
    "        agg_dict[col] = 'first'\n",
    "\n",
    "death_info = death_info.groupby('encounter_block', as_index=False).agg(agg_dict)\n",
    "\n",
    "# Standardize discharge category\n",
    "death_info['discharge_category'] = death_info['discharge_category'].str.lower()\n",
    "\n",
    "# Step 1: Determine if patient died (based on discharge_category)\n",
    "death_info['died'] = death_info['discharge_category'].isin(['expired', 'hospice']).astype(int)\n",
    "\n",
    "# Step 2: Determine final_outcome_dttm\n",
    "# If died: use death_dttm if available, otherwise use last_vital_dttm\n",
    "# If not died: use last_vital_dttm\n",
    "death_info['final_outcome_dttm'] = (\n",
    "    death_info['death_dttm']\n",
    "    .fillna(death_info['last_vital_dttm'])  # Fallback to last_vital\n",
    "    .where(death_info['died'] == 1, pd.NaT)  # Only keep for died==1, else NaT\n",
    ")\n",
    "\n",
    "print(f\"   Patients identified as died (expired/hospice): {death_info['died'].sum():,}\")\n",
    "\n",
    "num_with_death_dttm = ((death_info['died'] == 1) & (death_info['death_dttm'].notna())).sum()\n",
    "num_using_last_vital = ((death_info['died'] == 1) & (death_info['death_dttm'].isna())).sum()\n",
    "\n",
    "print(f\"   - With death_dttm: {num_with_death_dttm:,}\")\n",
    "print(f\"   - Using last_vital_dttm: {num_using_last_vital:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MORTALITY CALCULATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n5. Calculating mortality outcomes...\")\n",
    "\n",
    "# In-hospital death: died AND final_outcome_dttm is between first and last vital\n",
    "death_info['in_hosp_death'] = (\n",
    "    (death_info['died'] == 1) &\n",
    "    (death_info['final_outcome_dttm'].notna()) &\n",
    "    (death_info['final_outcome_dttm'] >= death_info['first_vital_dttm']) &\n",
    "    (death_info['final_outcome_dttm'] <= death_info['last_vital_dttm'])\n",
    ").astype(int)\n",
    "\n",
    "# 30-day mortality: died AND final_outcome_dttm within 30 days of first vital\n",
    "death_info['death_30d'] = (\n",
    "    (death_info['died'] == 1) &\n",
    "    (death_info['final_outcome_dttm'].notna()) &\n",
    "    (death_info['final_outcome_dttm'] <= (death_info['first_vital_dttm'] + pd.Timedelta(days=30)))\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "print(f\"   In-hospital deaths: {death_info['in_hosp_death'].sum():,} ({death_info['in_hosp_death'].mean()*100:.1f}%)\")\n",
    "print(f\"   30-day deaths: {death_info['death_30d'].sum():,} ({death_info['death_30d'].mean()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. COMBINE ALL OUTCOMES\n",
    "# ============================================================================\n",
    "print(\"\\n6. Combining all outcomes...\")\n",
    "outcomes_df = cohort_df[['hospitalization_id', 'encounter_block']].merge(\n",
    "    icu_los_summary, on='encounter_block', how='left'\n",
    ").merge(\n",
    "    hosp_los[['encounter_block', 'hosp_los_days']], on='encounter_block', how='left'\n",
    ").merge(\n",
    "    death_info, on='encounter_block', how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal outcomes dataset:\")\n",
    "print(f\"   Total records: {len(outcomes_df):,}\")\n",
    "print(f\"   Records with ICU LOS: {outcomes_df['icu_los_days'].notna().sum():,}\")\n",
    "print(f\"   Records with Hospital LOS: {outcomes_df['hosp_los_days'].notna().sum():,}\")\n",
    "print(f\"   In-hospital mortality rate: {outcomes_df['in_hosp_death'].mean()*100:.1f}%\")\n",
    "print(f\"   30-day mortality rate: {outcomes_df['death_30d'].mean()*100:.1f}%\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTCOMES SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ICU LOS (days):\")\n",
    "print(f\"  Median [IQR]: {outcomes_df['icu_los_days'].median():.1f} [{outcomes_df['icu_los_days'].quantile(0.25):.1f}-{outcomes_df['icu_los_days'].quantile(0.75):.1f}]\")\n",
    "print(f\"\\nHospital LOS (days):\")\n",
    "print(f\"  Median [IQR]: {outcomes_df['hosp_los_days'].median():.1f} [{outcomes_df['hosp_los_days'].quantile(0.25):.1f}-{outcomes_df['hosp_los_days'].quantile(0.75):.1f}]\")\n",
    "print(f\"\\nMortality:\")\n",
    "print(f\"  In-hospital: {outcomes_df['in_hosp_death'].sum():,}/{len(outcomes_df):,} ({outcomes_df['in_hosp_death'].mean()*100:.1f}%)\")\n",
    "print(f\"  30-day: {outcomes_df['death_30d'].sum():,}/{len(outcomes_df):,} ({outcomes_df['death_30d'].mean()*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert specified columns to lowercase (if they exist)\n",
    "category_cols = [\n",
    "    'admission_type_category', 'discharge_category',\n",
    "    'race_category', 'sex_category', 'ethnicity_category'\n",
    "]\n",
    "for col in category_cols:\n",
    "    if col in outcomes_df.columns:\n",
    "        outcomes_df[col] = outcomes_df[col].str.lower()\n",
    "\n",
    "# Arrange columns: patient_id, hospitalization_id, encounter_block, then everything else\n",
    "front_cols = [col for col in ['patient_id', 'hospitalization_id', 'encounter_block'] if col in outcomes_df.columns]\n",
    "other_cols = [col for col in outcomes_df.columns if col not in front_cols]\n",
    "outcomes_df = outcomes_df[front_cols + other_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CRRT Dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading crrt_therapy table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'crrt_therapy',\n",
    "        filters={'hospitalization_id': list(adult_hosp_ids)}\n",
    "    )\n",
    "    print(f\"   CRRT therapy loaded: {len(clif.crrt_therapy.df):,} rows\")\n",
    "    print(f\"   Unique CRRT therapy hospitalizations: {clif.crrt_therapy.df['hospitalization_id'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   CRRT therapy not available or error: {e}\")\n",
    "\n",
    "crrt_df = clif.crrt_therapy.df\n",
    "# Join dropped encounters (those not present in crrt_df) with crrt_df on hospitalization_id\n",
    "# Now join these dropped encounters with crrt_df on hospitalization_id\n",
    "crrt_df = crrt_df.merge(\n",
    "    dropped_encounters, on='hospitalization_id', how='left', indicator=True\n",
    ")\n",
    "print(f\"   Dropped encounters joined: {len(crrt_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Processing CRRT Data\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   CRRT therapy loaded: {len(clif.crrt_therapy.df):,} rows\")\n",
    "print(f\"   Unique CRRT therapy hospitalizations: {clif.crrt_therapy.df['hospitalization_id'].nunique()}\")\n",
    "# ============================================================================\n",
    "#  Define CRRT Initiation Time\n",
    "# ============================================================================\n",
    "print(\"\\n1. Defining CRRT initiation time...\")\n",
    "\n",
    "# Filter crrt_df to only include hospitalization_ids present in the cohort\n",
    "crrt_cohort = crrt_df[crrt_df['hospitalization_id'].isin(cohort_df['hospitalization_id'])].copy()\n",
    "\n",
    "# Sort by encounter_block and time\n",
    "crrt_cohort = crrt_cohort.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "# Create indicator for any CRRT activity (any non-null flow rate)\n",
    "crrt_cohort['has_crrt_activity'] = (\n",
    "    (\n",
    "        crrt_cohort['dialysate_flow_rate'].notna() & (crrt_cohort['dialysate_flow_rate'] > 0)\n",
    "    ) |\n",
    "    (\n",
    "        crrt_cohort['pre_filter_replacement_fluid_rate'].notna() & (crrt_cohort['pre_filter_replacement_fluid_rate'] > 0)\n",
    "    ) |\n",
    "    (\n",
    "        crrt_cohort['post_filter_replacement_fluid_rate'].notna() & (crrt_cohort['post_filter_replacement_fluid_rate'] > 0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get CRRT initiation time (first non-null flow rate per encounter_block)\n",
    "crrt_initiation = (crrt_cohort[crrt_cohort['has_crrt_activity']]\n",
    "                    .groupby('encounter_block')\n",
    "                    .agg({'recorded_dttm': 'min'})\n",
    "                    .reset_index())\n",
    "crrt_initiation.rename(columns={'recorded_dttm': 'crrt_initiation_time'}, inplace=True)\n",
    "\n",
    "print(f\"   CRRT initiation times identified for: {len(crrt_initiation):,} encounter_blocks\")\n",
    "print(f\"   Date range: {crrt_initiation['crrt_initiation_time'].min()} to {crrt_initiation['crrt_initiation_time'].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Get CRRT Parameters at Initiation\n",
    "# ============================================================================\n",
    "print(\"\\n4. Getting CRRT parameters at initiation time...\")\n",
    "# Merge initiation times back to crrt_cohort\n",
    "crrt_cohort = crrt_cohort.merge(crrt_initiation, on='encounter_block', how='left')\n",
    "# Filter to records at exactly the initiation time\n",
    "crrt_at_initiation = crrt_cohort[\n",
    "    crrt_cohort['recorded_dttm'] == crrt_cohort['crrt_initiation_time']\n",
    "].copy()\n",
    "\n",
    "print(f\"   CRRT records at initiation: {len(crrt_at_initiation):,}\")\n",
    "print(f\"   Unique encounter blocks: {crrt_at_initiation['encounter_block'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where vital_category is 'weight_kg'\n",
    "weight_df = clif.vitals.df[clif.vitals.df['vital_category'] == 'weight_kg'].copy()\n",
    "weight_df = weight_df.merge(dropped_encounters[['hospitalization_id', 'encounter_block']], \n",
    "                            on='hospitalization_id', how='inner')\n",
    "\n",
    "# clif.vitals.df = None ## clear from memory\n",
    "\n",
    "# ============================================================================\n",
    "#  Get Closest Weight to CRRT Initiation\n",
    "# ============================================================================\n",
    "print(\"\\nFinding closest weights to CRRT initiation time...\")\n",
    "if 'vital_value' in weight_df.columns:\n",
    "    weight_df = weight_df.rename(columns={'vital_value': 'weight_kg'})\n",
    "if 'vital_category' in weight_df.columns:\n",
    "    weight_df = weight_df.drop(columns=['vital_category'])\n",
    "print(f\"   Weight records available: {len(weight_df):,}\")\n",
    "\n",
    "combined = crrt_initiation.merge(weight_df, on='encounter_block', how='inner')\n",
    "\n",
    "before_mask = combined['recorded_dttm'] <= combined['crrt_initiation_time']\n",
    "combined_before = combined[before_mask].copy()\n",
    "combined_before_sorted = combined_before.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "closest_before = (combined_before_sorted\n",
    "                  .groupby('encounter_block')\n",
    "                  .last()\n",
    "                  .reset_index())\n",
    "\n",
    "all_blocks = set(combined['encounter_block'])\n",
    "blocks_with_before = set(closest_before['encounter_block'])\n",
    "blocks_missing = all_blocks - blocks_with_before\n",
    "print(\"Blocks without weight recorded before initiation time:\", len(blocks_missing))\n",
    "\n",
    "after_mask = (\n",
    "    combined['encounter_block'].isin(blocks_missing) &\n",
    "    (combined['recorded_dttm'] > combined['crrt_initiation_time']) &\n",
    "    (combined['weight_kg'].notnull())\n",
    ")\n",
    "combined_after = combined[after_mask].copy()\n",
    "combined_after_sorted = combined_after.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "first_after = (combined_after_sorted\n",
    "               .groupby('encounter_block')\n",
    "               .first()\n",
    "               .reset_index())\n",
    "\n",
    "num_taken_after = len(first_after)\n",
    "print(f\"   Number of weights from after initiation: {num_taken_after}\")\n",
    "\n",
    "combined_final = pd.concat([closest_before, first_after], axis=0, ignore_index=True)\n",
    "combined = combined_final\n",
    "\n",
    "closest_weights = (combined\n",
    "                .sort_values(['encounter_block', 'recorded_dttm'])\n",
    "                .groupby('encounter_block')\n",
    "                .last()\n",
    "                .reset_index())\n",
    "\n",
    "closest_weights = closest_weights[['encounter_block', 'weight_kg']]\n",
    "\n",
    "print(f\"   Weights found for: {len(closest_weights):,} encounter_blocks\")\n",
    "\n",
    "# Calculate and print the number of encounter blocks for which weights were not found\n",
    "all_encounter_blocks_with_crrt = set(crrt_initiation['encounter_block'].unique())\n",
    "encounter_blocks_with_weights = set(closest_weights['encounter_block'].unique())\n",
    "encounter_blocks_without_weights = all_encounter_blocks_with_crrt - encounter_blocks_with_weights\n",
    "print(f\"   Weights NOT found for: {len(encounter_blocks_without_weights):,} encounter_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n6. Combining CRRT data with weights...\")\n",
    "\n",
    "index_crrt_df = crrt_at_initiation.merge(\n",
    "    closest_weights,\n",
    "    on='encounter_block',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"   Final dataset: {len(index_crrt_df):,} records\")\n",
    "print(f\"   Records with weights: {index_crrt_df['weight_kg'].notna().sum():,}\")\n",
    "print(f\"   Records with CRRT mode: {index_crrt_df['crrt_mode_category'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crrt_cohort = crrt_cohort[crrt_cohort['encounter_block'].isin(dropped_encounters)]\n",
    "\n",
    "# Define desired column order\n",
    "desired_order = [\n",
    "    'hospitalization_id', 'encounter_block', 'recorded_dttm', \n",
    "    'crrt_mode_category', 'dialysate_flow_rate', 'pre_filter_replacement_fluid_rate', \n",
    "    'post_filter_replacement_fluid_rate', 'ultrafiltration_out', \n",
    "    'blood_flow_rate', 'crrt_initiation_time'\n",
    "]\n",
    "# Only keep columns that exist in crrt_cohort\n",
    "front_cols = [col for col in desired_order if col in crrt_cohort.columns]\n",
    "other_cols = [col for col in crrt_cohort.columns if col not in front_cols]\n",
    "crrt_cohort = crrt_cohort[front_cols + other_cols]\n",
    "\n",
    "# Sort as specified\n",
    "crrt_cohort = crrt_cohort.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "print(\"\\n Calculating CRRT dose at initiation...\")\n",
    "\n",
    "# First, filter for valid CRRT modes only\n",
    "valid_modes = ['cvvh', 'cvvhd', 'cvvhdf']\n",
    "crrt_cohort['crrt_mode_category'] = crrt_cohort['crrt_mode_category'].str.lower()\n",
    "\n",
    "print(f\"   Total CRRT records before filtering: {len(crrt_df):,}\")\n",
    "crrt_df_filtered = crrt_cohort[crrt_cohort['crrt_mode_category'].isin(valid_modes)].copy()\n",
    "print(f\"   Records after filtering for valid modes (cvvh, cvvhd, cvvhdf): {len(crrt_df_filtered):,}\")\n",
    "print(f\"   Excluded records: {len(crrt_df) - len(crrt_df_filtered):,}\")\n",
    "\n",
    "# Fill NaN values with 0 for flow rate columns\n",
    "flow_cols = ['dialysate_flow_rate', 'pre_filter_replacement_fluid_rate',\n",
    "            'post_filter_replacement_fluid_rate']\n",
    "\n",
    "# Drop rows where all 3 variables are missing\n",
    "crrt_df_filtered = crrt_df_filtered.dropna(subset=flow_cols, how='all')\n",
    "\n",
    "# Then fill remaining NaNs in those columns with 0\n",
    "crrt_df_filtered[flow_cols] = crrt_df_filtered[flow_cols].fillna(0)\n",
    "\n",
    "print(\"\\n   Mode distribution across all time points:\")\n",
    "print(crrt_df_filtered['crrt_mode_category'].value_counts())\n",
    "\n",
    "# Calculate mode-specific dose at EACH time point\n",
    "conditions = [\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvhd',\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvh',\n",
    "    crrt_df_filtered['crrt_mode_category'] == 'cvvhdf'\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    crrt_df_filtered['dialysate_flow_rate'],\n",
    "    crrt_df_filtered['pre_filter_replacement_fluid_rate'] + crrt_df_filtered['post_filter_replacement_fluid_rate'],\n",
    "    crrt_df_filtered['dialysate_flow_rate'] + crrt_df_filtered['pre_filter_replacement_fluid_rate'] +\n",
    "    crrt_df_filtered['post_filter_replacement_fluid_rate']\n",
    "]\n",
    "\n",
    "# Mode-specific total flow rate at each time point\n",
    "crrt_df_filtered['total_flow_rate'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "# Also calculate full flow rate (all components) at each time point\n",
    "crrt_df_filtered['total_flow_rate_full'] = (\n",
    "    crrt_df_filtered['dialysate_flow_rate'] +\n",
    "    crrt_df_filtered['pre_filter_replacement_fluid_rate'] +\n",
    "    crrt_df_filtered['post_filter_replacement_fluid_rate']\n",
    ")\n",
    "\n",
    "# Merge weight data (assuming weight_df has encounter_block and weight_kg)\n",
    "crrt_df_filtered = crrt_df_filtered.merge(\n",
    "    closest_weights[['encounter_block', 'weight_kg']].drop_duplicates(),\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate dose at each time point\n",
    "crrt_df_filtered['crrt_dose_ml_kg_hr'] = np.where(\n",
    "    (crrt_df_filtered['weight_kg'] > 0) & (crrt_df_filtered['total_flow_rate'] > 0),\n",
    "    crrt_df_filtered['total_flow_rate'] / crrt_df_filtered['weight_kg'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "crrt_df_filtered['crrt_dose_ml_kg_hr_full'] = np.where(\n",
    "    (crrt_df_filtered['weight_kg'] > 0) & (crrt_df_filtered['total_flow_rate_full'] > 0),\n",
    "    crrt_df_filtered['total_flow_rate_full'] / crrt_df_filtered['weight_kg'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "print(f\"\\n   Dose calculations at individual time points:\")\n",
    "print(f\"     Mode-specific doses calculated: {crrt_df_filtered['crrt_dose_ml_kg_hr'].notna().sum():,}\")\n",
    "print(f\"     Full doses calculated: {crrt_df_filtered['crrt_dose_ml_kg_hr_full'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Calculate Median Dose for First 3 Hours + Get Initiation Values\n",
    "# ============================================================================\n",
    "\n",
    "# Filter to first 3 hours after CRRT initiation\n",
    "crrt_first_3hrs = crrt_df_filtered[\n",
    "    crrt_df_filtered['recorded_dttm'] <= (crrt_df_filtered['crrt_initiation_time'] + pd.Timedelta(hours=3))\n",
    "].copy()\n",
    "\n",
    "print(f\"   Records within first 3 hours: {len(crrt_first_3hrs):,}\")\n",
    "\n",
    "# Define columns to aggregate\n",
    "dose_columns = ['dialysate_flow_rate', 'blood_flow_rate', \n",
    "                'pre_filter_replacement_fluid_rate', \n",
    "                'post_filter_replacement_fluid_rate', 'ultrafiltration_out',\n",
    "                'total_flow_rate', 'total_flow_rate_full',\n",
    "                'crrt_dose_ml_kg_hr', 'crrt_dose_ml_kg_hr_full']\n",
    "\n",
    "# Calculate medians for first 3 hours\n",
    "median_3hr = crrt_first_3hrs.groupby('encounter_block').agg({\n",
    "    'hospitalization_id': 'first',\n",
    "    'crrt_initiation_time': 'first',\n",
    "    'weight_kg': 'first',\n",
    "    'crrt_mode_category': lambda x: x.mode()[0] if not x.empty else np.nan,  # Most frequent mode\n",
    "    **{col: 'median' for col in dose_columns}\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"   Encounters with median values calculated: {len(median_3hr):,}\")\n",
    "\n",
    "# Get values at initiation time (original values)\n",
    "print(\"\\n   Getting original values at initiation time...\")\n",
    "\n",
    "crrt_at_init = crrt_df_filtered[\n",
    "    crrt_df_filtered['recorded_dttm'] == crrt_df_filtered['crrt_initiation_time']\n",
    "].copy()\n",
    "\n",
    "# Group by encounter_block and take first (should be unique at initiation time)\n",
    "init_values = crrt_at_init.groupby('encounter_block').agg({\n",
    "    col: 'first' for col in dose_columns\n",
    "}).reset_index()\n",
    "\n",
    "# Rename init columns to add _not_avged suffix\n",
    "init_values = init_values.rename(columns={\n",
    "    col: f'{col}_not_avged' for col in dose_columns\n",
    "})\n",
    "\n",
    "print(f\"   Encounters with initiation values: {len(init_values):,}\")\n",
    "\n",
    "# Merge median and initiation values\n",
    "final_df = median_3hr.merge(init_values, on='encounter_block', how='left')\n",
    "\n",
    "# Now arrange columns in the requested order\n",
    "# First the main columns (with median values)\n",
    "main_columns = [\n",
    "    'encounter_block', 'hospitalization_id', 'crrt_initiation_time',\n",
    "    'weight_kg', 'crrt_mode_category',\n",
    "    'dialysate_flow_rate', 'blood_flow_rate',\n",
    "    'pre_filter_replacement_fluid_rate', 'post_filter_replacement_fluid_rate',\n",
    "    'ultrafiltration_out', 'total_flow_rate', 'total_flow_rate_full',\n",
    "    'crrt_dose_ml_kg_hr', 'crrt_dose_ml_kg_hr_full'\n",
    "]\n",
    "\n",
    "# Then the initiation columns (with _not_avged suffix)\n",
    "not_avged_columns = [f'{col}_not_avged' for col in dose_columns]\n",
    "\n",
    "# Combine all columns\n",
    "all_columns = main_columns + not_avged_columns\n",
    "\n",
    "# Select and reorder columns\n",
    "final_df = final_df[all_columns]\n",
    "\n",
    "print(f\"\\n   Final dataframe created:\")\n",
    "print(f\"     Total rows: {len(final_df):,} (one per encounter)\")\n",
    "print(f\"     Total columns: {len(final_df.columns)}\")\n",
    "\n",
    "# Assign to your desired variable name\n",
    "index_crrt_df = final_df.copy()\n",
    "\n",
    "print(\"\\n✅ Final dataframe created with one row per encounter!\")\n",
    "print(f\"   Stored as 'index_crrt_df' with {len(index_crrt_df)} encounters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Plot 2: Mode-Specific CRRT Dose Comparison by Mode Category\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get unique mode categories (excluding NaN)\n",
    "mode_categories = final_df['crrt_mode_category'].dropna().unique()\n",
    "mode_categories = sorted(mode_categories)  # Sort for consistent ordering\n",
    "\n",
    "# Create figure with subplots for each mode\n",
    "n_modes = len(mode_categories)\n",
    "fig, axes = plt.subplots(1, n_modes, figsize=(6*n_modes, 6))\n",
    "\n",
    "# If only one mode, make axes iterable\n",
    "if n_modes == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Color palette\n",
    "colors_median = ['steelblue', 'royalblue', 'dodgerblue']\n",
    "colors_init = ['coral', 'salmon', 'lightsalmon']\n",
    "\n",
    "# Process each mode\n",
    "for idx, mode in enumerate(mode_categories):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Filter data for this mode\n",
    "    mode_data = final_df[final_df['crrt_mode_category'] == mode]\n",
    "\n",
    "    # Get dose values for this mode\n",
    "    dose_median = mode_data['crrt_dose_ml_kg_hr'].dropna()\n",
    "    dose_init = mode_data['crrt_dose_ml_kg_hr_not_avged'].dropna()\n",
    "\n",
    "    # Skip if no data\n",
    "    if len(dose_median) == 0 and len(dose_init) == 0:\n",
    "        ax.text(0.5, 0.5, f'No data for {mode.upper()}',\n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f'{mode.upper()}', fontsize=14, fontweight='bold')\n",
    "        continue\n",
    "\n",
    "    # Create overlaid histograms\n",
    "    if len(dose_median) > 0:\n",
    "        ax.hist(dose_median, bins=20, alpha=0.5, label=f'Median 3hr (n={len(dose_median)})',\n",
    "                color=colors_median[idx % len(colors_median)], edgecolor='darkblue', density=True)\n",
    "        ax.axvline(dose_median.mean(), color=colors_median[idx % len(colors_median)],\n",
    "                    linestyle='--', linewidth=2, label=f'Mean 3hr: {dose_median.mean():.1f}')\n",
    "\n",
    "    if len(dose_init) > 0:\n",
    "        ax.hist(dose_init, bins=20, alpha=0.5, label=f'At Init (n={len(dose_init)})',\n",
    "                color=colors_init[idx % len(colors_init)], edgecolor='darkred', density=True)\n",
    "        ax.axvline(dose_init.mean(), color=colors_init[idx % len(colors_init)],\n",
    "                    linestyle='--', linewidth=2, label=f'Mean Init: {dose_init.mean():.1f}')\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('CRRT Dose (mL/kg/hr)', fontsize=11)\n",
    "    ax.set_ylabel('Density' if idx == 0 else '', fontsize=11)\n",
    "    ax.set_title(f'{mode.upper()}\\n(n={len(mode_data)} encounters)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Mode-Specific CRRT Dose Comparison by Mode Category\\nMedian (First 3hr) vs At Initiation',\n",
    "            fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_path = '../output/final/graphs/crrt_dose_comparison_by_mode.png'\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved mode-specific comparison to: {output_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_crrt_duration(crrt_cohort):\n",
    "    \"\"\"\n",
    "    Calculate CRRT duration for each encounter.\n",
    "    Duration is defined as the time from crrt_initiation_time to the last recorded setting,\n",
    "    considering CRRT ended when there's a 24-hour gap in recordings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crrt_cohort : pd.DataFrame\n",
    "        DataFrame with CRRT time series data including columns:\n",
    "        - encounter_block: patient identifier\n",
    "        - crrt_initiation_time: start of CRRT\n",
    "        - recorded_dttm: timestamp column for each setting recording\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with encounter_block and calculated duration metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = crrt_cohort.copy()\n",
    "\n",
    "    # Ensure datetime columns are properly formatted\n",
    "    time_column = 'recorded_dttm'\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    df['crrt_initiation_time'] = pd.to_datetime(df['crrt_initiation_time'])\n",
    "\n",
    "    # CRITICAL: Drop all rows where recorded_dttm is before crrt_initiation_time\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[time_column] >= df['crrt_initiation_time']]\n",
    "    rows_dropped = initial_rows - len(df)\n",
    "\n",
    "    if rows_dropped > 0:\n",
    "        print(f\"Dropped {rows_dropped} recordings that occurred before CRRT initiation time\")\n",
    "        print(f\"Remaining recordings: {len(df)}\")\n",
    "\n",
    "    # Sort by encounter and time\n",
    "    df = df.sort_values(['encounter_block', time_column])\n",
    "\n",
    "    # Function to calculate duration for each encounter\n",
    "    def get_encounter_duration(group):\n",
    "        \"\"\"Calculate CRRT duration for a single encounter with 24-hour gap detection\"\"\"\n",
    "\n",
    "        # Get initiation time\n",
    "        initiation_time = group['crrt_initiation_time'].iloc[0]\n",
    "\n",
    "        # Get all recorded times (already filtered to be >= initiation_time)\n",
    "        recorded_times = group[time_column].dropna().sort_values()\n",
    "\n",
    "        if len(recorded_times) == 0:\n",
    "            # No recordings after initiation\n",
    "            return pd.Series({\n",
    "                'crrt_initiation_time': initiation_time,\n",
    "                'crrt_end_time': initiation_time,\n",
    "                'duration_hours': 0,\n",
    "                'duration_days': 0,\n",
    "                'num_recordings': 0,\n",
    "                'had_24hr_gap': False\n",
    "            })\n",
    "\n",
    "        # Check for 24-hour gaps\n",
    "        time_diffs = recorded_times.diff()\n",
    "\n",
    "        # Find if there's any gap >= 24 hours\n",
    "        gaps_24hr = time_diffs > pd.Timedelta(hours=24)\n",
    "\n",
    "        if gaps_24hr.any():\n",
    "            # CRRT ended at the last recording before the first 24-hour gap\n",
    "            first_gap_idx = gaps_24hr.idxmax()\n",
    "            # Get the index position of the gap\n",
    "            gap_position = recorded_times.index.get_loc(first_gap_idx)\n",
    "            # The end time is the recording just before the gap\n",
    "            end_time = recorded_times.iloc[gap_position - 1]\n",
    "            had_gap = True\n",
    "        else:\n",
    "            # No 24-hour gap, use the last recording\n",
    "            end_time = recorded_times.iloc[-1]\n",
    "            had_gap = False\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = end_time - initiation_time\n",
    "        duration_hours = duration.total_seconds() / 3600\n",
    "        duration_days = duration_hours / 24\n",
    "\n",
    "        # Count recordings\n",
    "        num_recordings = len(recorded_times)\n",
    "\n",
    "        return pd.Series({\n",
    "            'crrt_initiation_time': initiation_time,\n",
    "            'crrt_end_time': end_time,\n",
    "            'duration_hours': duration_hours,\n",
    "            'duration_days': duration_days,\n",
    "            'num_recordings': num_recordings,\n",
    "            'had_24hr_gap': had_gap\n",
    "        })\n",
    "\n",
    "    # Apply to each encounter\n",
    "    duration_df = df.groupby('encounter_block').apply(get_encounter_duration).reset_index()\n",
    "\n",
    "    # Add summary statistics\n",
    "    print(\"\\n=== CRRT Duration Summary ===\")\n",
    "    print(f\"Total encounters: {len(duration_df)}\")\n",
    "    print(f\"Encounters with recordings: {len(duration_df[duration_df['num_recordings'] > 0])}\")\n",
    "    print(f\"Encounters without recordings: {len(duration_df[duration_df['num_recordings'] == 0])}\")\n",
    "\n",
    "    # Stats for encounters with recordings\n",
    "    valid_durations = duration_df[duration_df['duration_hours'] > 0]\n",
    "\n",
    "    if len(valid_durations) > 0:\n",
    "        print(f\"\\nDuration Statistics (for {len(valid_durations)} encounters with valid recordings):\")\n",
    "        print(f\"\\nDuration (hours):\")\n",
    "        print(f\"  Mean: {valid_durations['duration_hours'].mean():.1f}\")\n",
    "        print(f\"  Median: {valid_durations['duration_hours'].median():.1f}\")\n",
    "        print(f\"  Q25: {valid_durations['duration_hours'].quantile(0.25):.1f}\")\n",
    "        print(f\"  Q75: {valid_durations['duration_hours'].quantile(0.75):.1f}\")\n",
    "        print(f\"  Min: {valid_durations['duration_hours'].min():.1f}\")\n",
    "        print(f\"  Max: {valid_durations['duration_hours'].max():.1f}\")\n",
    "\n",
    "        print(f\"\\nDuration (days):\")\n",
    "        print(f\"  Mean: {valid_durations['duration_days'].mean():.1f}\")\n",
    "        print(f\"  Median: {valid_durations['duration_days'].median():.1f}\")\n",
    "        print(f\"  Q25: {valid_durations['duration_days'].quantile(0.25):.1f}\")\n",
    "        print(f\"  Q75: {valid_durations['duration_days'].quantile(0.75):.1f}\")\n",
    "\n",
    "    print(f\"\\nEncounters with 24-hour gap: {duration_df['had_24hr_gap'].sum()} ({duration_df['had_24hr_gap'].mean()*100:.1f}%)\")\n",
    "\n",
    "    # Add duration categories\n",
    "    duration_df['duration_category'] = pd.cut(\n",
    "        duration_df['duration_days'],\n",
    "        bins=[-0.001, 0, 1, 3, 7, 14, float('inf')],\n",
    "        labels=['No duration', '<1 day', '1-3 days', '3-7 days', '7-14 days', '>14 days'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDuration categories:\")\n",
    "    print(duration_df['duration_category'].value_counts().sort_index())\n",
    "\n",
    "    return duration_df\n",
    "\n",
    "# Pre-processing function to check for pre-initiation recordings\n",
    "def check_pre_initiation_recordings(crrt_cohort):\n",
    "    \"\"\"\n",
    "    Check how many recordings occur before CRRT initiation time\n",
    "    \"\"\"\n",
    "    df = crrt_cohort.copy()\n",
    "    df['recorded_dttm'] = pd.to_datetime(df['recorded_dttm'])\n",
    "    df['crrt_initiation_time'] = pd.to_datetime(df['crrt_initiation_time'])\n",
    "\n",
    "    # Find pre-initiation recordings\n",
    "    pre_init = df[df['recorded_dttm'] < df['crrt_initiation_time']]\n",
    "\n",
    "    if len(pre_init) > 0:\n",
    "        print(\"=== Pre-Initiation Recordings Found ===\")\n",
    "        print(f\"Total pre-initiation recordings: {len(pre_init)} ({len(pre_init)/len(df)*100:.1f}% of all recordings)\")\n",
    "        print(f\"Affected encounters: {pre_init['encounter_block'].nunique()}\")\n",
    "\n",
    "        # Calculate how early these recordings are\n",
    "        pre_init['hours_before'] = (pre_init['crrt_initiation_time'] - pre_init['recorded_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "        print(f\"\\nTiming statistics (hours before initiation):\")\n",
    "        print(f\"  Mean: {pre_init['hours_before'].mean():.1f} hours\")\n",
    "        print(f\"  Median: {pre_init['hours_before'].median():.1f} hours\")\n",
    "        print(f\"  Max: {pre_init['hours_before'].max():.1f} hours\")\n",
    "\n",
    "        # Show a few examples\n",
    "        print(\"\\nExample pre-initiation recordings:\")\n",
    "        sample = pre_init.nlargest(5, 'hours_before')[['encounter_block', 'recorded_dttm', 'crrt_initiation_time', 'hours_before']]\n",
    "        print(sample)\n",
    "    else:\n",
    "        print(\"No pre-initiation recordings found - data is clean!\")\n",
    "\n",
    "    return pre_init\n",
    "\n",
    "# First check for pre-initiation recordings (optional)\n",
    "pre_init_check = check_pre_initiation_recordings(crrt_cohort)\n",
    "\n",
    "# Calculate duration (automatically drops pre-initiation recordings)\n",
    "duration_df = calculate_crrt_duration(crrt_cohort)\n",
    "\n",
    "# Merge back with main cohort\n",
    "index_crrt_df = index_crrt_df.merge(\n",
    "    duration_df[['encounter_block', 'duration_hours', 'duration_days', 'duration_category', 'had_24hr_gap']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. IMV duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading respiratory support table...\")\n",
    "try:\n",
    "    clif.load_table(\n",
    "        'respiratory_support',\n",
    "        filters={'hospitalization_id': adult_hosp_ids}\n",
    "    )\n",
    "    print(f\"   respiratory_support loaded: {len(clif.respiratory_support.df):,} rows\")\n",
    "    print(f\"   Unique respiratory_support hospitalizations: {clif.respiratory_support.df['hospitalization_id'].nunique()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   CRRT therapy not available or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clif.respiratory_support.df = clif.respiratory_support.df.merge(\n",
    "    dropped_encounters[['hospitalization_id', 'encounter_block']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "clif.respiratory_support = clif.respiratory_support.waterfall()\n",
    "resp_support_df = clif.respiratory_support.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imv_duration(resp_support_df):\n",
    "    \"\"\"\n",
    "    Calculate IMV duration for each encounter block.\n",
    "    Duration is from first IMV recording to last IMV recording,\n",
    "    considering IMV ended when there's a 24-hour gap without IMV.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resp_support_df : pd.DataFrame\n",
    "        DataFrame with columns: encounter_block, recorded_dttm, device_category\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with encounter_block and IMV duration metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = resp_support_df.copy()\n",
    "\n",
    "    # Convert recorded_dttm to datetime\n",
    "    df['recorded_dttm'] = pd.to_datetime(df['recorded_dttm'])\n",
    "\n",
    "    # Filter for IMV records only (case-insensitive)\n",
    "    df['device_category_lower'] = df['device_category'].str.lower()\n",
    "    imv_df = df[df['device_category_lower'] == 'imv'].copy()\n",
    "\n",
    "    # Sort by encounter block and time\n",
    "    imv_df = imv_df.sort_values(['encounter_block', 'recorded_dttm'])\n",
    "\n",
    "    # Function to calculate duration for each encounter block\n",
    "    def get_imv_duration(group):\n",
    "        \"\"\"Calculate IMV duration for a single encounter block\"\"\"\n",
    "\n",
    "        # Get all IMV recording times\n",
    "        recorded_times = group['recorded_dttm'].dropna().sort_values()\n",
    "\n",
    "        if len(recorded_times) == 0:\n",
    "            # No IMV recordings\n",
    "            return pd.Series({\n",
    "                'imv_start_time': pd.NaT,\n",
    "                'imv_end_time': pd.NaT,\n",
    "                'imv_duration_hours': 0,\n",
    "                'imv_duration_days': 0\n",
    "            })\n",
    "\n",
    "        # IMV start time is the first recording\n",
    "        imv_start_time = recorded_times.iloc[0]\n",
    "\n",
    "        # Check for 24-hour gaps\n",
    "        time_diffs = recorded_times.diff()\n",
    "\n",
    "        # Find if there's any gap >= 24 hours\n",
    "        gaps_24hr = time_diffs > pd.Timedelta(hours=24)\n",
    "\n",
    "        if gaps_24hr.any():\n",
    "            # IMV ended at the last recording before the first 24-hour gap\n",
    "            first_gap_idx = gaps_24hr.idxmax()\n",
    "            gap_position = recorded_times.index.get_loc(first_gap_idx)\n",
    "            imv_end_time = recorded_times.iloc[gap_position - 1]\n",
    "        else:\n",
    "            # No 24-hour gap, use the last recording\n",
    "            imv_end_time = recorded_times.iloc[-1]\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = imv_end_time - imv_start_time\n",
    "        duration_hours = duration.total_seconds() / 3600\n",
    "        duration_days = duration_hours / 24\n",
    "\n",
    "        return pd.Series({\n",
    "            'imv_start_time': imv_start_time,\n",
    "            'imv_end_time': imv_end_time,\n",
    "            'imv_duration_hours': duration_hours,\n",
    "            'imv_duration_days': duration_days\n",
    "        })\n",
    "\n",
    "    # Apply to each encounter block\n",
    "    imv_duration_df = imv_df.groupby('encounter_block').apply(get_imv_duration).reset_index()\n",
    "\n",
    "    return imv_duration_df\n",
    "\n",
    "# Usage:\n",
    "imv_duration_df = calculate_imv_duration(resp_support_df)\n",
    "# Merge back with main cohort\n",
    "index_crrt_df = index_crrt_df.merge(\n",
    "    imv_duration_df[['encounter_block', 'imv_duration_hours', 'imv_duration_days']],\n",
    "    on='encounter_block',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_crrt_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Missingness comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MISSINGNESS COMPARISON: Table 1 - Dropped vs Retained Encounters\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON ANALYSIS: Dropped (Missing Labs) vs Retained Encounters\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load Full Cohort Data from Intermediate Files\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n1. Loading full cohort data from intermediate files...\")\n",
    "\n",
    "# Load the retained cohort data\n",
    "full_outcomes_df = pd.read_parquet('../output/intermediate/outcomes_df.parquet')\n",
    "full_index_crrt_df = pd.read_parquet('../output/intermediate/index_crrt_df.parquet')\n",
    "\n",
    "print(f\"   Full cohort outcomes: {len(full_outcomes_df):,} encounters\")\n",
    "print(f\"   Full cohort CRRT data: {len(full_index_crrt_df):,} encounters\")\n",
    "\n",
    "# Get dropped encounter blocks\n",
    "dropped_blocks = set(outcomes_df['encounter_block'].unique())\n",
    "print(f\"   Dropped encounters (missing labs): {len(dropped_blocks):,}\")\n",
    "\n",
    "# Rename local dfs to be clear they are the dropped group\n",
    "dropped_outcomes = outcomes_df.copy()\n",
    "dropped_crrt = index_crrt_df.copy()\n",
    "\n",
    "# The \"retained\" group is the full cohort (those who passed lab requirements)\n",
    "retained_outcomes = full_outcomes_df.copy()\n",
    "retained_crrt = full_index_crrt_df.copy()\n",
    "\n",
    "print(f\"   Retained encounters: {len(retained_outcomes):,}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. Helper Functions for Table 1\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def format_continuous(series, decimals=1):\n",
    "    \"\"\"Format continuous variable as median [IQR]\"\"\"\n",
    "    if series.dropna().empty:\n",
    "        return \"N/A\"\n",
    "    median = series.median()\n",
    "    q25 = series.quantile(0.25)\n",
    "    q75 = series.quantile(0.75)\n",
    "    return f\"{median:.{decimals}f} [{q25:.{decimals}f}-{q75:.{decimals}f}]\"\n",
    "\n",
    "def format_categorical(series):\n",
    "    \"\"\"Format categorical variable as n (%)\"\"\"\n",
    "    counts = series.value_counts()\n",
    "    total = len(series.dropna())\n",
    "    result = {}\n",
    "    for cat, count in counts.items():\n",
    "        pct = (count / total) * 100 if total > 0 else 0\n",
    "        result[cat] = f\"{count:,} ({pct:.1f}%)\"\n",
    "    return result\n",
    "\n",
    "def format_binary(series):\n",
    "    \"\"\"Format binary variable as n (%)\"\"\"\n",
    "    n = series.sum()\n",
    "    total = len(series.dropna())\n",
    "    pct = (n / total) * 100 if total > 0 else 0\n",
    "    return f\"{n:,} ({pct:.1f}%)\"\n",
    "\n",
    "def compare_continuous(dropped_series, retained_series):\n",
    "    \"\"\"Compare continuous variables using Mann-Whitney U test\"\"\"\n",
    "    dropped_clean = dropped_series.dropna()\n",
    "    retained_clean = retained_series.dropna()\n",
    "    if len(dropped_clean) < 2 or len(retained_clean) < 2:\n",
    "        return np.nan\n",
    "    stat, p = mannwhitneyu(dropped_clean, retained_clean, alternative='two-sided')\n",
    "    return p\n",
    "\n",
    "def compare_categorical(dropped_series, retained_series):\n",
    "    \"\"\"Compare categorical variables using Chi-square test\"\"\"\n",
    "    # Create contingency table\n",
    "    dropped_counts = dropped_series.value_counts()\n",
    "    retained_counts = retained_series.value_counts()\n",
    "    \n",
    "    # Align categories\n",
    "    all_cats = set(dropped_counts.index) | set(retained_counts.index)\n",
    "    contingency = []\n",
    "    for cat in all_cats:\n",
    "        dropped_n = dropped_counts.get(cat, 0)\n",
    "        retained_n = retained_counts.get(cat, 0)\n",
    "        contingency.append([dropped_n, retained_n])\n",
    "    \n",
    "    contingency = np.array(contingency)\n",
    "    \n",
    "    # Check if valid for chi-square\n",
    "    if contingency.shape[0] < 2 or np.any(contingency.sum(axis=1) == 0):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        return p\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compare_binary(dropped_series, retained_series):\n",
    "    \"\"\"Compare binary variables using Chi-square test\"\"\"\n",
    "    contingency = np.array([\n",
    "        [dropped_series.sum(), len(dropped_series) - dropped_series.sum()],\n",
    "        [retained_series.sum(), len(retained_series) - retained_series.sum()]\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        return p\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def format_pvalue(p):\n",
    "    \"\"\"Format p-value for display\"\"\"\n",
    "    if pd.isna(p):\n",
    "        return \"N/A\"\n",
    "    elif p < 0.001:\n",
    "        return \"<0.001\"\n",
    "    else:\n",
    "        return f\"{p:.3f}\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. Build Table 1 - Demographics & Outcomes\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n2. Building Table 1...\")\n",
    "\n",
    "table1_rows = []\n",
    "\n",
    "# Sample size\n",
    "table1_rows.append({\n",
    "    'Variable': 'N',\n",
    "    'Dropped (Missing Labs)': f\"{len(dropped_outcomes):,}\",\n",
    "    'Retained (Final Cohort)': f\"{len(retained_outcomes):,}\",\n",
    "    'p-value': ''\n",
    "})\n",
    "\n",
    "# --- DEMOGRAPHICS ---\n",
    "table1_rows.append({'Variable': '--- Demographics ---', 'Dropped (Missing Labs)': '', 'Retained (Final Cohort)': '', 'p-value': ''})\n",
    "\n",
    "# Age\n",
    "if 'age_at_admission' in dropped_outcomes.columns and 'age_at_admission' in retained_outcomes.columns:\n",
    "    p = compare_continuous(dropped_outcomes['age_at_admission'], retained_outcomes['age_at_admission'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'Age, years, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_outcomes['age_at_admission']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_outcomes['age_at_admission']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# Sex\n",
    "if 'sex_category' in dropped_outcomes.columns and 'sex_category' in retained_outcomes.columns:\n",
    "    p = compare_categorical(dropped_outcomes['sex_category'], retained_outcomes['sex_category'])\n",
    "    dropped_sex = format_categorical(dropped_outcomes['sex_category'])\n",
    "    retained_sex = format_categorical(retained_outcomes['sex_category'])\n",
    "    \n",
    "    table1_rows.append({\n",
    "        'Variable': 'Sex, n (%)',\n",
    "        'Dropped (Missing Labs)': '',\n",
    "        'Retained (Final Cohort)': '',\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "    for cat in sorted(set(dropped_sex.keys()) | set(retained_sex.keys())):\n",
    "        table1_rows.append({\n",
    "            'Variable': f'  {cat}',\n",
    "            'Dropped (Missing Labs)': dropped_sex.get(cat, '0 (0.0%)'),\n",
    "            'Retained (Final Cohort)': retained_sex.get(cat, '0 (0.0%)'),\n",
    "            'p-value': ''\n",
    "        })\n",
    "\n",
    "# Race\n",
    "if 'race_category' in dropped_outcomes.columns and 'race_category' in retained_outcomes.columns:\n",
    "    p = compare_categorical(dropped_outcomes['race_category'], retained_outcomes['race_category'])\n",
    "    dropped_race = format_categorical(dropped_outcomes['race_category'])\n",
    "    retained_race = format_categorical(retained_outcomes['race_category'])\n",
    "    \n",
    "    table1_rows.append({\n",
    "        'Variable': 'Race, n (%)',\n",
    "        'Dropped (Missing Labs)': '',\n",
    "        'Retained (Final Cohort)': '',\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "    for cat in sorted(set(dropped_race.keys()) | set(retained_race.keys())):\n",
    "        table1_rows.append({\n",
    "            'Variable': f'  {cat}',\n",
    "            'Dropped (Missing Labs)': dropped_race.get(cat, '0 (0.0%)'),\n",
    "            'Retained (Final Cohort)': retained_race.get(cat, '0 (0.0%)'),\n",
    "            'p-value': ''\n",
    "        })\n",
    "\n",
    "# Ethnicity\n",
    "if 'ethnicity_category' in dropped_outcomes.columns and 'ethnicity_category' in retained_outcomes.columns:\n",
    "    p = compare_categorical(dropped_outcomes['ethnicity_category'], retained_outcomes['ethnicity_category'])\n",
    "    dropped_eth = format_categorical(dropped_outcomes['ethnicity_category'])\n",
    "    retained_eth = format_categorical(retained_outcomes['ethnicity_category'])\n",
    "    \n",
    "    table1_rows.append({\n",
    "        'Variable': 'Ethnicity, n (%)',\n",
    "        'Dropped (Missing Labs)': '',\n",
    "        'Retained (Final Cohort)': '',\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "    for cat in sorted(set(dropped_eth.keys()) | set(retained_eth.keys())):\n",
    "        table1_rows.append({\n",
    "            'Variable': f'  {cat}',\n",
    "            'Dropped (Missing Labs)': dropped_eth.get(cat, '0 (0.0%)'),\n",
    "            'Retained (Final Cohort)': retained_eth.get(cat, '0 (0.0%)'),\n",
    "            'p-value': ''\n",
    "        })\n",
    "\n",
    "# --- OUTCOMES ---\n",
    "table1_rows.append({'Variable': '--- Outcomes ---', 'Dropped (Missing Labs)': '', 'Retained (Final Cohort)': '', 'p-value': ''})\n",
    "\n",
    "# ICU LOS\n",
    "if 'icu_los_days' in dropped_outcomes.columns and 'icu_los_days' in retained_outcomes.columns:\n",
    "    p = compare_continuous(dropped_outcomes['icu_los_days'], retained_outcomes['icu_los_days'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'ICU LOS, days, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_outcomes['icu_los_days']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_outcomes['icu_los_days']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# Hospital LOS\n",
    "if 'hosp_los_days' in dropped_outcomes.columns and 'hosp_los_days' in retained_outcomes.columns:\n",
    "    p = compare_continuous(dropped_outcomes['hosp_los_days'], retained_outcomes['hosp_los_days'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'Hospital LOS, days, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_outcomes['hosp_los_days']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_outcomes['hosp_los_days']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# In-hospital mortality\n",
    "if 'in_hosp_death' in dropped_outcomes.columns and 'in_hosp_death' in retained_outcomes.columns:\n",
    "    p = compare_binary(dropped_outcomes['in_hosp_death'], retained_outcomes['in_hosp_death'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'In-hospital mortality, n (%)',\n",
    "        'Dropped (Missing Labs)': format_binary(dropped_outcomes['in_hosp_death']),\n",
    "        'Retained (Final Cohort)': format_binary(retained_outcomes['in_hosp_death']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# 30-day mortality\n",
    "if 'death_30d' in dropped_outcomes.columns and 'death_30d' in retained_outcomes.columns:\n",
    "    p = compare_binary(dropped_outcomes['death_30d'], retained_outcomes['death_30d'])\n",
    "    table1_rows.append({\n",
    "        'Variable': '30-day mortality, n (%)',\n",
    "        'Dropped (Missing Labs)': format_binary(dropped_outcomes['death_30d']),\n",
    "        'Retained (Final Cohort)': format_binary(retained_outcomes['death_30d']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# --- CRRT PARAMETERS ---\n",
    "table1_rows.append({'Variable': '--- CRRT Parameters ---', 'Dropped (Missing Labs)': '', 'Retained (Final Cohort)': '', 'p-value': ''})\n",
    "\n",
    "# Weight\n",
    "if 'weight_kg' in dropped_crrt.columns and 'weight_kg' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['weight_kg'], retained_crrt['weight_kg'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'Weight, kg, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['weight_kg']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['weight_kg']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# CRRT Mode\n",
    "if 'crrt_mode_category' in dropped_crrt.columns and 'crrt_mode_category' in retained_crrt.columns:\n",
    "    p = compare_categorical(dropped_crrt['crrt_mode_category'], retained_crrt['crrt_mode_category'])\n",
    "    dropped_mode = format_categorical(dropped_crrt['crrt_mode_category'])\n",
    "    retained_mode = format_categorical(retained_crrt['crrt_mode_category'])\n",
    "    \n",
    "    table1_rows.append({\n",
    "        'Variable': 'CRRT Mode, n (%)',\n",
    "        'Dropped (Missing Labs)': '',\n",
    "        'Retained (Final Cohort)': '',\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "    for cat in sorted(set(dropped_mode.keys()) | set(retained_mode.keys())):\n",
    "        table1_rows.append({\n",
    "            'Variable': f'  {cat.upper()}',\n",
    "            'Dropped (Missing Labs)': dropped_mode.get(cat, '0 (0.0%)'),\n",
    "            'Retained (Final Cohort)': retained_mode.get(cat, '0 (0.0%)'),\n",
    "            'p-value': ''\n",
    "        })\n",
    "\n",
    "# CRRT Dose (mode-specific)\n",
    "if 'crrt_dose_ml_kg_hr' in dropped_crrt.columns and 'crrt_dose_ml_kg_hr' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['crrt_dose_ml_kg_hr'], retained_crrt['crrt_dose_ml_kg_hr'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'CRRT Dose (mode-specific), mL/kg/hr, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['crrt_dose_ml_kg_hr']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['crrt_dose_ml_kg_hr']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# CRRT Dose (full)\n",
    "if 'crrt_dose_ml_kg_hr_full' in dropped_crrt.columns and 'crrt_dose_ml_kg_hr_full' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['crrt_dose_ml_kg_hr_full'], retained_crrt['crrt_dose_ml_kg_hr_full'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'CRRT Dose (full), mL/kg/hr, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['crrt_dose_ml_kg_hr_full']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['crrt_dose_ml_kg_hr_full']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# Blood flow rate\n",
    "if 'blood_flow_rate' in dropped_crrt.columns and 'blood_flow_rate' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['blood_flow_rate'], retained_crrt['blood_flow_rate'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'Blood flow rate, mL/min, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['blood_flow_rate']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['blood_flow_rate']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# Dialysate flow rate\n",
    "if 'dialysate_flow_rate' in dropped_crrt.columns and 'dialysate_flow_rate' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['dialysate_flow_rate'], retained_crrt['dialysate_flow_rate'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'Dialysate flow rate, mL/hr, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['dialysate_flow_rate']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['dialysate_flow_rate']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# Duration (if available)\n",
    "if 'duration_hours' in dropped_crrt.columns and 'duration_hours' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['duration_hours'], retained_crrt['duration_hours'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'CRRT Duration, hours, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['duration_hours']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['duration_hours']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# IMV Duration (if available)\n",
    "if 'imv_duration_hours' in dropped_crrt.columns and 'imv_duration_hours' in retained_crrt.columns:\n",
    "    p = compare_continuous(dropped_crrt['imv_duration_hours'], retained_crrt['imv_duration_hours'])\n",
    "    table1_rows.append({\n",
    "        'Variable': 'IMV Duration, hours, median [IQR]',\n",
    "        'Dropped (Missing Labs)': format_continuous(dropped_crrt['imv_duration_hours']),\n",
    "        'Retained (Final Cohort)': format_continuous(retained_crrt['imv_duration_hours']),\n",
    "        'p-value': format_pvalue(p)\n",
    "    })\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. Create and Display Table 1\n",
    "# ----------------------------------------------------------------------------\n",
    "table1_df = pd.DataFrame(table1_rows)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TABLE 1: Comparison of Dropped vs Retained Encounters\")\n",
    "print(\"=\" * 100)\n",
    "print(table1_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "table1_df.to_csv('../output/final/table1_missingness_comparison.csv', index=False)\n",
    "print(f\"\\n✅ Table 1 saved to: ../output/final/table1_missingness_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
